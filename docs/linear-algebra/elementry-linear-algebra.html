<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Elementry Linear Algebra</title>
    <style>
        :root {
            --text-color: #2c3e50;
            --background-color: #ffffff;
            --accent-color: #3498db;
            --border-color: #ecf0f1;
        }

        @media (prefers-color-scheme: dark) {
            :root {
                --text-color: #ecf0f1;
                --background-color: #2c3e50;
                --accent-color: #3498db;
                --border-color: #34495e;
            }
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            max-width: 50rem;
            margin: 0 auto;
            padding: 2rem;
            color: var(--text-color);
            background: var(--background-color);
        }

        nav {
            position: sticky;
            top: 0;
            background: var(--background-color);
            border-bottom: 1px solid var(--border-color);
            padding: 1rem 0;
            margin-bottom: 2rem;
            display: flex;
            flex-wrap: wrap;
            gap: 1rem;
        }

        nav a {
            color: var(--accent-color);
            text-decoration: none;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            transition: background-color 0.2s;
        }

        nav a:hover {
            background-color: var(--border-color);
        }

        .content {
            margin-top: 2rem;
        }

        h1, h2, h3, h4, h5, h6 {
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        code {
            background: var(--border-color);
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-size: 0.9em;
        }

        pre code {
            display: block;
            padding: 1rem;
            overflow-x: auto;
        }

        img {
            max-width: 100%;
            height: auto;
        }

        .meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 2rem;
        }
    </style>
</head>
<body>
    <nav>
        <a href="/index.html">Home</a>
<a href="/operating-systems/v1-kernels-and-processes/1-introductions.html">1 Introductions</a>
<a href="/networks/0-foundation/1-network-components-and-protocols.html">1 Network Components And Protocols</a>
<a href="/operating-systems/v4-persistent-storage/11-file-systems-overview.html">11 File Systems Overview</a>
<a href="/operating-systems/v4-persistent-storage/13-files-and-directories.html">13 Files And Directories</a>
<a href="/networks/0-foundation/2-physical-layer.html">2 Physical Layer</a>
<a href="/operating-systems/v1-kernels-and-processes/2-the-kernel-abstraction.html">2 The Kernel Abstraction</a>
<a href="/networks/0-foundation/3-performance.html">3 Performance</a>
<a href="/operating-systems/v1-kernels-and-processes/3-the-programming-interface.html">3 The Programming Interface</a>
<a href="/algorithms/practice/4.html">4</a>
<a href="/operating-systems/v2-concurrency/4-concurrency-and-threads.html">4 Concurrency And Threads</a>
<a href="/operating-systems/v2-concurrency/5-synchronizing-access-to-shared-objects.html">5 Synchronizing Access To Shared Objects</a>
<a href="/operating-systems/v2-concurrency/7-multiprocessor-scheduling.html">7 Multiprocessor Scheduling</a>
<a href="/operating-systems/v2-concurrency/7-queueing-theory.html">7 Queueing Theory</a>
<a href="/operating-systems/v2-concurrency/7-uniprocessor-scheduling.html">7 Uniprocessor Scheduling</a>
<a href="/networks/4-transport/ACK-clocking.html">Ack Clocking</a>
<a href="/algorithms/approximation-algorithms.html">Approximation Algorithms</a>
<a href="/networks/3-network/ARP.html">Arp</a>
<a href="/algorithms/BFS.html">Bfs</a>
<a href="/algorithms/patterns/BFS.html">Bfs</a>
<a href="/networks/3-network/BGP.html">Bgp</a>
<a href="/distributed-systems/bigtable.html">Bigtable</a>
<a href="/algorithms/bipartite-graphs.html">Bipartite Graphs</a>
<a href="/networks/5-application/CDNs.html">Cdns</a>
<a href="/designing-data-intensive-applications/part-1-foundations-of-data-systems/ch1-reliable-scalable-and-maintainable-applications.html">Ch1 Reliable Scalable And Maintainable Applications</a>
<a href="/designing-data-intensive-applications/part-3-derived-data/ch10-batch-processing.html">Ch10 Batch Processing</a>
<a href="/designing-data-intensive-applications/part-1-foundations-of-data-systems/ch2-data-models-and-query-languages.html">Ch2 Data Models And Query Languages</a>
<a href="/designing-data-intensive-applications/part-1-foundations-of-data-systems/ch3-storage-and-retrieval.html">Ch3 Storage And Retrieval</a>
<a href="/designing-data-intensive-applications/part-1-foundations-of-data-systems/ch4-encoding-and-evolution.html">Ch4 Encoding And Evolution</a>
<a href="/designing-data-intensive-applications/part-2-distributed-data/ch5-replication.html">Ch5 Replication</a>
<a href="/linear-algebra/cheatsheet.html">Cheatsheet</a>
<a href="/distributed-systems/clocks.html">Clocks</a>
<a href="/ds-backup/clocks.html">Clocks</a>
<a href="/networks/1-physical/coding-and-modulation.html">Coding And Modulation</a>
<a href="/teaching/modern-java/collections-and-records.html">Collections And Records</a>
<a href="/digital-design/combinational-logic.html">Combinational Logic</a>
<a href="/operating-systems/lecture-notes/components.html">Components</a>
<a href="/cheatsheets/circuits/components.html">Components</a>
<a href="/algorithms/connected-components.html">Connected Components</a>
<a href="/distributed-systems/consistency.html">Consistency</a>
<a href="/distributed-systems/consistent-global-state.html">Consistent Global State</a>
<a href="/ds-backup/consistent-global-state.html">Consistent Global State</a>
<a href="/algorithms/DAGs.html">Dags</a>
<a href="/algorithms/DFS.html">Dfs</a>
<a href="/networks/3-network/DHCP.html">Dhcp</a>
<a href="/distributed-systems/disconnected-operation.html">Disconnected Operation</a>
<a href="/distributed-systems/distributed-cache-coherence.html">Distributed Cache Coherence</a>
<a href="/algorithms/divide-and-conquer.html">Divide And Conquer</a>
<a href="/cheatsheets/algorithms/divide-and-conquer.html">Divide And Conquer</a>
<a href="/networks/5-application/DNS.html">Dns</a>
<a href="/algorithms/dynamic-programming.html">Dynamic Programming</a>
<a href="/distributed-systems/dynamo-db.html">Dynamo Db</a>
<a href="/performance-engineering/efficiently-implementing-state-pattern-JVM.html">Efficiently Implementing State Pattern Jvm</a>
<a href="/cheatsheets/circuits/electricity.html">Electricity</a>
<a href="/linear-algebra/elementry-linear-algebra.html">Elementry Linear Algebra</a>
<a href="/networks/2-direct-links/errors.html">Errors</a>
<a href="/operating-systems/lecture-notes/file-systems.html">File Systems</a>
<a href="/networks/4-transport/flow-control.html">Flow Control</a>
<a href="/networks/2-direct-links/framing.html">Framing</a>
<a href="/networks/3-network/global-internet.html">Global Internet</a>
<a href="/distributed-systems/google-file-system.html">Google File System</a>
<a href="/cheatsheets/algorithms/graphs.html">Graphs</a>
<a href="/algorithms/problems/graphs-and-trees.html">Graphs And Trees</a>
<a href="/algorithms/graphs-intro.html">Graphs Intro</a>
<a href="/algorithms/greedy-algorithms.html">Greedy Algorithms</a>
<a href="/operating-systems/lecture-notes/handle-tables.html">Handle Tables</a>
<a href="/networks/5-application/HTTP.html">Http</a>
<a href="/networks/3-network/ICMP.html">Icmp</a>
<a href="/algorithms/induction.html">Induction</a>
<a href="/networks/0-foundation/information-theory.html">Information Theory</a>
<a href="/networks/3-network/internetworking.html">Internetworking</a>
<a href="/cheatsheets/algorithms/intervals.html">Intervals</a>
<a href="/machine-learning-for-big-data/intro-mapreduce-spark.html">Intro Mapreduce Spark</a>
<a href="/operating-systems/lecture-notes/io-systems-secondary-storage.html">Io Systems Secondary Storage</a>
<a href="/digital-design/karnaugh-maps.html">Karnaugh Maps</a>
<a href="/operating-systems/lecture-notes/kernel-abstraction.html">Kernel Abstraction</a>
<a href="/operating-systems/section-notes/lab-3-questions.html">Lab 3 Questions</a>
<a href="/teaching/modern-java/lambdas-and-streams.html">Lambdas And Streams</a>
<a href="/signal-conditioning/lecture-notes/lecture-1.html">Lecture 1</a>
<a href="/signal-conditioning/lecture-notes/lecture-2.html">Lecture 2</a>
<a href="/signal-conditioning/lecture-notes/lecture-3.html">Lecture 3</a>
<a href="/signal-conditioning/lecture-notes/lecture-4.html">Lecture 4</a>
<a href="/signal-conditioning/lecture-notes/lecture-5.html">Lecture 5</a>
<a href="/signal-conditioning/lecture-notes/lecture-6.html">Lecture 6</a>
<a href="/venv/lib/python3.13/site-packages/Markdown-3.7.dist-info/LICENSE.html">License</a>
<a href="/algorithms/linear-programming.html">Linear Programming</a>
<a href="/distributed-systems/load-balancing.html">Load Balancing</a>
<a href="/distributed-systems/managing-critical-state.html">Managing Critical State</a>
<a href="/ds-backup/managing-critical-state.html">Managing Critical State</a>
<a href="/networks/1-physical/media.html">Media</a>
<a href="/networks/3-network/motivation.html">Motivation</a>
<a href="/networks/2-direct-links/multiple-access.html">Multiple Access</a>
<a href="/distributed-systems/mutual-exclusion.html">Mutual Exclusion</a>
<a href="/ds-backup/mutual-exclusion.html">Mutual Exclusion</a>
<a href="/algorithms/network-flows.html">Network Flows</a>
<a href="/networks/3-network/networking-services.html">Networking Services</a>
<a href="/distributed-systems/non-blocking-two-phase-commit.html">Non Blocking Two Phase Commit</a>
<a href="/distributed-systems/ordering-events-in-distributed-systems.html">Ordering Events In Distributed Systems</a>
<a href="/ds-backup/ordering-events-in-distributed-systems.html">Ordering Events In Distributed Systems</a>
<a href="/networks/5-application/overview.html">Overview</a>
<a href="/operating-systems/lecture-notes/page-faults.html">Page Faults</a>
<a href="/operating-systems/lecture-notes/paging.html">Paging</a>
<a href="/tmp/pairing-algorithm.html">Pairing Algorithm</a>
<a href="/distributed-systems/paxos-architecture.html">Paxos Architecture</a>
<a href="/distributed-systems/paxos-intro.html">Paxos Intro</a>
<a href="/ds-backup/paxos-intro.html">Paxos Intro</a>
<a href="/distributed-systems/paxos-made-simple.html">Paxos Made Simple</a>
<a href="/ds-backup/paxos-made-simple.html">Paxos Made Simple</a>
<a href="/designing-data-intensive-applications/part-2-distributed-data/preface.html">Preface</a>
<a href="/distributed-systems/primary-backup.html">Primary Backup</a>
<a href="/ds-backup/primary-backup.html">Primary Backup</a>
<a href="/operating-systems/lecture-notes/processes.html">Processes</a>
<a href="/linear-algebra/python-cheatsheet.html">Python Cheatsheet</a>
<a href="/digital-design/quartus-workflow.html">Quartus Workflow</a>
<a href="/README.html">Readme</a>
<a href="/networks/reference.html">Reference</a>
<a href="/operating-systems/reference.html">Reference</a>
<a href="/cheatsheets/java-spring-boot/reference.html">Reference</a>
<a href="/networks/2-direct-links/retransmission.html">Retransmission</a>
<a href="/networks/3-network/routing.html">Routing</a>
<a href="/distributed-systems/RPC.html">Rpc</a>
<a href="/ds-backup/RPC.html">Rpc</a>
<a href="/cheatsheets/java-spring-boot/running.html">Running</a>
<a href="/algorithms/runtime.html">Runtime</a>
<a href="/distributed-systems/scaling-web-services.html">Scaling Web Services</a>
<a href="/ds-backup/scaling-web-services.html">Scaling Web Services</a>
<a href="/operating-systems/section-notes/section-1.html">Section 1</a>
<a href="/digital-design/sequential-logic.html">Sequential Logic</a>
<a href="/distributed-systems/sharding.html">Sharding</a>
<a href="/scripts/simple.html">Simple</a>
<a href="/algorithms/patterns/sliding-window.html">Sliding Window</a>
<a href="/networks/sockets.html">Sockets</a>
<a href="/tmp/bench/spec.html">Spec</a>
<a href="/algorithms/stable-matching.html">Stable Matching</a>
<a href="/networks/2-direct-links/switching.html">Switching</a>
<a href="/digital-design/system-verilog.html">System Verilog</a>
<a href="/networks/4-transport/TCP.html">Tcp</a>
<a href="/operating-systems/lecture-notes/tlb.html">Tlb</a>
<a href="/algorithms/tmp.html">Tmp</a>
<a href="/networks/4-transport/transport-overview.html">Transport Overview</a>
<a href="/algorithms/tree-intro.html">Tree Intro</a>
<a href="/distributed-systems/two-phase-commit.html">Two Phase Commit</a>
<a href="/networks/4-transport/UDP.html">Udp</a>
<a href="/digital-design/waveform-diagram.html">Waveform Diagram</a>
<a href="/operating-systems/lecture-notes/windows-memory-management.html">Windows Memory Management</a>
<a href="/operating-systems/lecture-notes/windows-objects-handles-refcounts.html">Windows Objects Handles Refcounts</a>
<a href="/operating-systems/lecture-notes/windows-rtz.html">Windows Rtz</a>
<a href="/networks/2-direct-links/wireless.html">Wireless</a>
    </nav>
    <main>
        <h1>Elementry Linear Algebra</h1>
        <div class="meta">
            Last modified: 2024-11-14
        </div>
        <div class="content">
            <h1 id="elementary-linear-algebra">Elementary Linear Algebra</h1>
<h2 id="systems-of-equations">Systems of Equations</h2>
<p>Systems of equations are both fundamental and important to actually understanding linear algebra. With that being said, the two primary introductory courses at the University of Washington, Math 208 and (to a much lesser extent) Applied Math 352, spend a significant amount of time on methods for <em>solving</em> systems of equations, something that I have almost no interest in. For the sake of completeness, I will briefly touch on notation and algorithms, but will try to both confine it to this document, and to focus on takeaways that become more useful later on.</p>
<h3 id="notation">Notation</h3>
<p>$$
\begin{align<em>}
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n &amp;= b_1 \
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n &amp;= b_2 \
&amp;\vdots \
a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n &amp;= b_m
\end{align</em>}
$$</p>
<p>Or equivalently, in matrix form $Ax = b$.</p>
<p>$$
\begin{align<em>}
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \
x_2 \
\vdots \
x_n
\end{bmatrix}
&amp;=
\begin{bmatrix}
b_1 \
b_2 \
\vdots \
b_m
\end{bmatrix}
\end{align</em>}
$$</p>
<h3 id="gaussian-elimination">Gaussian Elimination</h3>
<p>Perform any of the following <strong>elementary row operations</strong> to the augmented matrix $[A|b]$:</p>
<ul>
<li>Swap two rows</li>
<li>Multiply a row by a nonzero scalar</li>
<li>Add a multiple of one row to another</li>
</ul>
<p>The aim of the algorithm is to get the matrix into either <strong>row echelon form</strong> or <strong>reduced row echelon form</strong>. The former is a matrix where the first nonzero element in each row is 1, and the first nonzero element in each row is to the right of the first nonzero element in the row above it. The latter is a matrix where the first nonzero element in each row is 1, and the first nonzero element in each row is the only nonzero element in its column. For example, below $A$ is in row echelon form, and $B$ is in reduced row echelon form.</p>
<p>$$
\begin{align<em>}
A &amp;=
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{14} \
0 &amp; a_{22} &amp; a_{23} &amp; a_{24} \
0 &amp; 0 &amp; a_{33} &amp; a_{34} \
0 &amp; 0 &amp; 0 &amp; a_{44}
\end{bmatrix}
\
B &amp;=
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \
0 &amp; 1 &amp; 0 &amp; 0 \
0 &amp; 0 &amp; 1 &amp; 0 \
0 &amp; 0 &amp; 0 &amp; 1
\end{bmatrix}
\end{align</em>}
$$</p>
<p>As you perform row operations, you also act on $b$ to keep the system equivalent. Once you have the matrix in row echelon form, you can solve the system by back substitution, or by continuing to row reduce to reduced row echelon form, where the solution is immediately apparent.</p>
<h3 id="takeaways">Takeaways</h3>
<h4 id="row-operations">Row Operations</h4>
<p>Solving systems of equations is pretty boring, but the emergent structure of a system on the verge of being solved cements a few important ideas:</p>
<ul>
<li><strong>Row operations</strong> need to <em>somehow</em> be legal, in particular reversible and preservative of the solution set. This is a good example of a <strong>group</strong> in action.</li>
<li>The result of running Gaussian elimination is a <strong>basis</strong> for the solution set. This gives the <strong>span</strong> of the solution set, and the <strong>rank</strong> of the matrix.</li>
<li>Additional rows of zero left in the matrix after row reduction indicate <strong>redundancy</strong>, or in other words, <strong>linear dependence</strong>.</li>
</ul>
<h4 id="span">Span</h4>
<p>The span of a set of vectors is the set of all possible linear combinations of those vectors. The span of a set of vectors is a <strong>subspace</strong> of the vector space. The span of a set of vectors is the <strong>null space</strong> of the matrix whose columns are those vectors.</p>
<p>$$
\begin{align<em>}
\text{span}\left{ \begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix} \right} &amp;= \mathbb{R}^2 \
\text{span}\left{ \begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix}, \begin{bmatrix} 1 \ 1 \end{bmatrix} \right} &amp;= \mathbb{R}^2 \
\text{span}\left{ \begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix}, \begin{bmatrix} 1 \ 1 \end{bmatrix} \right} &amp;= \text{span}\left{ \begin{bmatrix} 1 \ 0 \end{bmatrix}, \begin{bmatrix} 0 \ 1 \end{bmatrix} \right}
\end{align</em>}
$$</p>
<p>For a set of $n$ vectors in $\mathbb{R}^n$ to span $\mathbb{R}^n$, the vectors must be linearly independent. This is a necessary and sufficient condition for a set of vectors to be a <strong>basis</strong> for $\mathbb{R}^n$. You can perform additional reasoning to determine the rules for spanning sets, and implications on linear independence. For instance, a set of less than $n$ vectors in $\mathbb{R}^n$ cannot span $\mathbb{R}^n$, and a set of more than $n$ vectors in $\mathbb{R}^n$ must be linearly dependent.</p>
<h4 id="linear-transformations">Linear Transformations</h4>
<p>For a function to be linear, it must satisfy two properties:</p>
<ul>
<li><strong>Additivity</strong>: $f(x + y) = f(x) + f(y)$</li>
<li><strong>Homogeneity</strong>: $f(cx) = cf(x)$</li>
</ul>
<p>A linear transformation is a function $T: \mathbb{R}^n \to \mathbb{R}^m$ that satisfies these properties. The <strong>kernel</strong> of a linear transformation is the set of vectors that are mapped to the zero vector, e.g. $T(x) = 0$, or $Ax = 0$ for a matrix $A$ that represents the transformation. The kernel is a subspace of the domain. The <strong>range</strong> of a linear transformation is the set of all possible outputs, and is a subspace of the codomain.</p>
<p>$$
\begin{align<em>}
T: \mathbb{R}^2 &amp;\to \mathbb{R}^2 \
T\left( \begin{bmatrix} x \ y \end{bmatrix} \right) &amp;= \begin{bmatrix} x \ 0 \end{bmatrix}
\end{align</em>}
$$</p>
<p>$$
\begin{align<em>}
\text{ker}(T) &amp;= \text{span}\left{ \begin{bmatrix} 0 \ 1 \end{bmatrix} \right} \
\text{range}(T) &amp;= \text{span}\left{ \begin{bmatrix} 1 \ 0 \end{bmatrix} \right}
\end{align</em>}
$$</p>
<h4 id="matrix-vector-multiplication">Matrix-Vector Multiplication</h4>
<p>$$
\begin{align<em>}
A\begin{bmatrix} x \ y \end{bmatrix} &amp;= x\begin{bmatrix} a_{11} \ a_{21} \end{bmatrix} + y\begin{bmatrix} a_{12} \ a_{22} \end{bmatrix} \
&amp;= \begin{bmatrix} a_{11}x + a_{12}y \ a_{21}x + a_{22}y \end{bmatrix}
\end{align</em>}
$$</p>
<p>Matrix-vector multiplication is a linear transformation. The columns of the matrix are the images of the basis vectors, and the result is the image of the input vector. The kernel of the transformation is the null space of the matrix, and the range is the column space of the matrix.</p>
<p>Visually, you can picture transforming the basis vectors/unit square of the domain into the basis vectors/unit square of the codomain. The matrix is the transformation matrix, and the columns are the images of the basis vectors. The result is the image of the input vector.</p>
<h4 id="matrix-matrix-multiplication">Matrix-Matrix Multiplication</h4>
<p>$$
\begin{align<em>}
AB &amp;= A\begin{bmatrix} b_1 &amp; b_2 &amp; \cdots &amp; b_n \end{bmatrix} \
&amp;= \begin{bmatrix} Ab_1 &amp; Ab_2 &amp; \cdots &amp; Ab_n \end{bmatrix}
\end{align</em>}
$$</p>
<p>The algorithm here is to multiply the matrix on the right by each column of the matrix on the left. The result is a matrix whose columns are the images of the columns of the matrix on the right. This is a linear transformation, and the kernel of the transformation is the null space of the matrix on the right, and the range is the column space of the matrix on the left.q</p>
<pre><code class="language-python">import numpy as np

# inefficient
def multiply_bad(A, B):
    C = np.zeros((A.shape[0], B.shape[1]))
    for i in range(A.shape[0]):
        for j in range(B.shape[1]):
            for k in range(A.shape[1]):
                C[i, j] += A[i, k] * B[k, j]
    return C

# efficient
def multiply_good(A, B):
    return np.dot(A, B)

M, N = 10000, 10000
A = np.random.rand(M, N)
B = np.random.rand(N, M)

%timeit multiply_bad(A, B)
%timeit multiply_good(A, B)
</code></pre>
<pre><code class="language-plaintext">
</code></pre>
<p>Matrix multiplication is fundamentally a costly operation, taking $O(n^3)$ time. That being said, libraries like numpy are heavily optimized and can perform matrix multiplication orders of magnitude faster than naive implementations using vectorized operations. In practice you should <strong>never</strong> write your own matrix multiplication.</p>
        </div>
    </main>
</body>
</html>
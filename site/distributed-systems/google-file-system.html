
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google File System (GFS) Overview | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="An overview of the Google File System (GFS), a highly scalable and fault-tolerant distributed storage system designed to handle large amounts of data across many commodity hardware nodes.">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/distributed-systems/google-file-system.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Google File System (GFS) Overview">
    <meta property="og:description" content="An overview of the Google File System (GFS), a highly scalable and fault-tolerant distributed storage system designed to handle large amounts of data across many commodity hardware nodes.">
    <meta property="og:url" content="https://notes.elimelt.com/distributed-systems/google-file-system.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Google File System (GFS) Overview">
    <meta name="twitter:description" content="An overview of the Google File System (GFS), a highly scalable and fault-tolerant distributed storage system designed to handle large amounts of data across many commodity hardware nodes.">

    <meta name="keywords" content="cloud storage,distributed systems,google cloud">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "Google File System (GFS) Overview", "dateModified": "2025-02-11T16:42:07.809631", "description": "An overview of the Google File System (GFS), a highly scalable and fault-tolerant distributed storage system designed to handle large amounts of data across many commodity hardware nodes.", "articleSection": "Distributed Systems", "keywords": "cloud storage,distributed systems,google cloud"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$", right: "$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                    {left: "\\(", right: "\\)", display: false}
                ],
                throwOnError: false
            });
        });
    </script>

<style>
    :root {
        --text-color: #1a1a1a;
        --background-color: #ffffff;
        --accent-color: #2563eb;
        --border-color: #e5e7eb;
        --nav-background: rgba(255, 255, 255, 0.95);
    }

    @media (prefers-color-scheme: dark) {
        :root {
            --text-color: #f3f4f6;
            --background-color: #1a1a1a;
            --accent-color: #60a5fa;
            --border-color: #374151;
            --nav-background: rgba(26, 26, 26, 0.95);
        }
    }

    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        line-height: 1.6;
        max-width: 50rem;
        margin: 0 auto;
        padding: 2rem;
        color: var(--text-color);
        background: var(--background-color);
    }

    nav {
        position: sticky;
        top: 0;
        background: var(--nav-background);
        backdrop-filter: blur(10px);
        border-bottom: 1px solid var(--border-color);
        padding: 1rem 0;
        margin-bottom: 2rem;
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
        z-index: 1000;
    }

    nav a {
        color: var(--accent-color);
        text-decoration: none;
        padding: 0.5rem 1rem;
        border-radius: 4px;
        transition: background-color 0.2s;
    }

    nav a:hover {
        background-color: var(--border-color);
    }

    .breadcrumbs {
        margin-bottom: 2rem;
        color: var(--text-color);
        opacity: 0.8;
    }

    .breadcrumbs a {
        color: var(--accent-color);
        text-decoration: none;
    }

    .content {
        margin-top: 2rem;
    }

    h1, h2, h3, h4, h5, h6 {
        margin-top: 2rem;
        margin-bottom: 1rem;
        line-height: 1.3;
    }

    code {
        background: var(--border-color);
        padding: 0.2rem 0.4rem;
        border-radius: 3px;
        font-size: 0.9em;
        font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
    }

    pre {
        background: var(--border-color);
        padding: 1rem;
        border-radius: 4px;
        overflow-x: auto;
        margin: 1.5rem 0;
    }

    pre code {
        background: none;
        padding: 0;
        border-radius: 0;
    }

    img {
        max-width: 100%;
        height: auto;
        border-radius: 4px;
        margin: 1.5rem 0;
    }

    .meta {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 0.9em;
        margin-bottom: 2rem;
        display: flex;
        flex-wrap: wrap;
        gap: 1rem;
    }

    .tags {
        margin-top: 2rem;
        padding-top: 1rem;
        border-top: 1px solid var(--border-color);
    }

    .tags a {
        display: inline-block;
        background: var(--border-color);
        color: var(--text-color);
        padding: 0.2rem 0.6rem;
        border-radius: 3px;
        text-decoration: none;
        font-size: 0.9em;
        margin-right: 0.5rem;
        margin-bottom: 0.5rem;
    }

    .tags a:hover {
        background: var(--accent-color);
        color: white;
    }

    a {
        color: #3391ff;
    }

    table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
    }

    th, td {
        padding: 0.75rem;
        border: 1px solid var(--border-color);
    }

    th {
        background: var(--border-color);
    }

    .md-content table td, .md-content table th {
        background: black;
    }

    blockquote {
        margin: 1.5rem 0;
        padding-left: 1rem;
        border-left: 4px solid var(--accent-color);
        color: var(--text-color);
        opacity: 0.8;
    }

    .katex-display {
        overflow: auto hidden;
        padding: 1em 0;
        margin: 0.5em 0;
    }

    .katex-display > .katex {
        white-space: normal;
    }

    .katex {
        font-size: 1.1em;
        display: inline;
        line-height: 1.2;
    }

    .katex-html {
        display: inline-block;
        vertical-align: middle;
    }

    .katex .strut {
        display: none;
    }

    .katex-display .katex {
        display: block;
        text-align: center;
    }

    .katex-display > .katex > .katex-html {
        display: block;
        max-width: 100%;
        overflow-x: auto;
        padding: 0.5em 0;
        min-height: 40px;
    }
</style></head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/distributed%20systems.html">Distributed Systems</a> » Google File System (GFS) Overview
        </div>
    </header>
    <main role="main">
        <article>
            <h1>Google File System (GFS) Overview</h1>
            <div class="meta">
                <time datetime="2025-02-11T16:42:07.809631">
                    Last modified: 2025-02-11
                </time>
                <span>Category: <a href="/categories/distributed%20systems.html">Distributed Systems</a></span>
            </div>
            <div class="content">
                <h1 id="google-file-system-gfs">Google File System (GFS)</h1>
<p><a href="https://static.googleusercontent.com/media/research.google.com/en//archive/gfs-sosp2003.pdf">reading</a></p>
<h2 id="introduction">Introduction</h2>
<p>GFS differs from more traditional file systems in a lot of ways. Particularly...</p>
<ul>
<li>Fault tolerance/expecting failures as the common case, since it runs on commodity hardware</li>
<li>Huge (multi-GB) files are the norm</li>
<li>Read/Append access patterns are common, while updates and random writes are rare. Particularly, the system is optimized for large streaming (sequential) reads and appends.</li>
</ul>
<h2 id="design-overview">Design Overview</h2>
<h3 id="assumptions">Assumptions</h3>
<ul>
<li>Built with commodity hardware, and is thus failure prone and needs to constantly monitor, detect, and tolerate failures</li>
<li>Typically storing a modest number of large files. Anywhere from a few million ~100 MB files, to multi-GB files. The system should be optimized for working with these large files.</li>
<li>Two types of workloads:</li>
<li>large streaming reads spanning &gt; 1 MB</li>
<li>smaller random reads (which are often batched)</li>
<li>Well defined semantics and easy concurrency control for concurrent appends are essential. A common application of GFS is to be used as a producer-consumer queue</li>
<li>High sustained bandwidth takes precedence over latency</li>
</ul>
<h3 id="interface">Interface</h3>
<p>Although familiar, GFS is not POSIX compliant. Files are organized hierarchically in directories and identified by path names, supporting <em>create, delete, open, close, read,</em> and <em>write</em>. Additionally, supports <em>snapshot</em> and <em>record append</em>. Snapshot creates a copy of a file or directory tree. Record append allows multiple clients to append data to the same file concurrently while guaranteeing the atomicity of each individual record.</p>
<h3 id="architecture">Architecture</h3>
<p>A GFS cluster is composed of a single <em>master</em>, and multiple <em>chunkservers</em>, and is accessed by multiple <em>clients</em>. Each component is typically run on a commodity Linux machine running a user-level server process, and it isn't uncommon to run both a client and chunkserver on the same machine (although this comes at a reliability cost).</p>
<p>Files are made of fixed-size <em>chunks</em>, identified by an immutable and globally unique 64 bit <em>chunk handle</em> which is assigned by the master at the time of creation. Chunkservers store chunks as Linux files on local disk, and read/write to specified chunk handle and byte ranges. Each chunk is replicated on multiple chunkservers (by default with three replicas, but is configurable).</p>
<p>The master maintains all filesystem metadata, including the namespace, access control information, file to chunk and chunk to server mappings, etc. The master communicates with chunkservers through a <em>HeartBeat</em>, which is a two-way message that allows the master to monitor and instruct the chunkservers and chunkservers to report their status.</p>
<p>GFS client libraries are linked in each application. All metadata updates go through the master, but simple data-bearing communication goes directly to the respective chunkserver.There is no need to go through something like the Linux vnode layer, which is a major benefit of not supporting POSIX.</p>
<p>Neither clients nor chunkservers cache file data, but clients will typically cache metadata. Technically, chunkservers do cache file data in that they use Linux files, and thus have a buffer cache for frequently accessed data in memory, but this is transparent to GFS.</p>
<h3 id="single-master">Single Master</h3>
<p>Using a single master architecture vastly simplifies things, since we can use global knowledge of the filesystem at the master for replication and chunk placement. To help this scale, the master's level of involvement in reads/writes needs to be minimized.</p>
<h4 id="common-case-read">Common Case Read</h4>
<ol>
<li>Client translates file name and byte offset to chunk index locally</li>
<li>Requests chunk handle and replica locations from master</li>
<li>Client caches the result with the file name and byte offset</li>
<li>Client chooses replica (often the closest) and reads/writes while their cache entry is still valid</li>
</ol>
<p>Note that for almost no extra cost, steps 1 and 2 can be batched for many files/chunks.</p>
<h3 id="chunk-size">Chunk Size</h3>
<p>GFS uses 64 MB lazily allocated (to mitigate internal fragmentation) chunks, each of which are stored as a plain Linux file. Using such large chunks has many benefits, including allowing clients to cache plenty of metadata, reducing network overhead/load of metadata acquisition, and allows for in-memory metadata at the master.</p>
<p>Large chunk sizes also have their disadvantages, including leading to hotspots for chunkservers with many smaller files. One such example is when you have an executable stored in GFS that you want to concurrently read and execute across many hundreds of machines, this leads to a temporary overload of those chunk servers. This was fixed by using a higher replication factor, and staggering starting times. <strong>Extension</strong>: peer to peer sharing (between clients) is a possible solution to hotspots.</p>
<h3 id="metadata">Metadata</h3>
<p>There are three main types of metadata, all of which is stored in-memory in the master.</p>
<ul>
<li>File and chunk namespaces</li>
<li>File to chunk mappings</li>
<li>Location of each chunk's replicas</li>
</ul>
<p>The first two are also persisted in an <em>operation log</em> stored on the master's local disk. The last however, is not persisted, and instead on startup/when a new chunkserver joins the cluster, the master requests metadata about the chunks stored locally.</p>
<h4 id="in-memory-data-structures">In-Memory Data Structures</h4>
<p>The master stores metadata in memory and goes fast! Additionally, this allows it to quickly scan through its entire state, making things like garbage collection, re-replication on failures, and migration for load balancing possible.</p>
<p>"b-b-bbbut that's nOt sCaLabLE" - someone right now probably</p>
<p>In reality, if the metadata were a limiting factor for your system, you would have a MASSIVE amount of data, and would need to be using a very under-specced machine as the master. Each 64 MB chunk only requires ~64 bytes of metadata, and namespace metadata is compressed using prefix compression (which is very effective for filesystems). Therefore, adding even an extra few GB of memory to your master would allow you to store a huge amount of additional metadata.</p>
<h4 id="chunk-locations">Chunk Locations</h4>
<p>As stated previously, the master doesn't persist the location of chunks, instead opting to poll chunkservers on startup. This design choice simplifies things greatly, and is a general approach to fault tolerance, in that it would have been an uphill battle trying to maintain a globally consistent and persistent view of the system, since chunkservers can partially fail, and they ultimately know best which chunks they have.</p>
<h4 id="operation-log">Operation Log</h4>
<p>The operation log is not only the sole persistent metadata in the system, but also defines the order in which concurrent operations are executed. The log is replicated remotely, and changes are batched and flushed to disk. The log is replayed on startup, but is also kept small by being checkpointed with a memory-serializable compact B-tree structure, making recovery faster. The master can create a checkpoint in a separate background thread, allowing concurrent operations to be executed. Once created, the checkpoint is written to disk both locally and remotely.</p>
<h3 id="consistency-model">Consistency Model</h3>
<h4 id="guarantees-by-gfs">Guarantees by GFS</h4>
<p>File namespace mutations are atomic through being done exclusively at the master with locking. File mutations have looser guarantees, particularly at the level of defined, and undefined regions</p>
<ul>
<li>Defined:</li>
<li>After a data mutation, the file is in a consistent state and the mutation was universally applied without being interrupted</li>
<li>Undefined:</li>
<li>All clients will see the same data, but the data may not be consistent with any single mutation (i.e. interleaved)</li>
<li>Successful concurrent mutations leave a region consistent but undefined, whereas failed concurrent mutations leave it undefined and inconsistent.</li>
</ul>
<p>After a sequence of successful mutations, the file is guaranteed to be defined and to contain the data written by the last mutation. GFS achieves this by...</p>
<ul>
<li>Applying mutations to chunks in a consistent order across replicas</li>
<li>Using chunk version numbers to detect stale replicas (due to missed mutations)</li>
</ul>
<p>Once a chunk becomes stale, it is no longer returned to the client, and is garbage collected ASAP. Since clients cache chunk metadata, there is a window of time in which a client will read from stale chunks, but this typically presents as reading a premature end of chunk (in the case of append workloads).</p>
<p>Component failures can lead to corrupted or destroyed data, but this is mitigated by checksumming files. Once a problem is detected, the data is restored from a valid replica if possible. In the (uncommon) case where all replicas are lost before the master can react, the data is unavailable, but corrupted data is never returned.</p>
<h4 id="implications-for-applications">Implications for Applications</h4>
<p>Long story short, you should always prefer appends over random writes. Note that GFS has "append at least once" semantics, and can also insert arbitrary padding between appends. It is thus important to use techniques like structuring your data and using unique identifiers for non-idempotent log entries.</p>
<h2 id="system-interactions">System Interactions</h2>
<h3 id="lease-and-mutation-order">Lease and Mutation Order</h3>
<p>A mutation is any change to the contents of a chunk. Each mutation is performed at all replicas. Mutations are carried out as follows:</p>
<ol>
<li>Master grants a <em>lease</em> (~60 sec) to one of the replicas, making it the <em>primary</em></li>
<li>The primary chooses a serial order for all mutations to the chunk</li>
<li>All replicas execute the mutations in the order defined by the primary</li>
</ol>
<p>Leases can be extended, and the extensions are piggybacked through HeartBeat messages. The master can also revoke a lease, although if it loses communication with the primary it only needs to wait for the lease to expire.</p>
<p>A write can be carried out through the following:</p>
<ol>
<li>The client asks the master which chunkserver holds the current lease, and the locations of other replicas. If no lease is held, the master grants one to a replica of its choice</li>
<li>The master replies with the identity of the primary and location of replicas (secondaries), which is cached by the client</li>
<li>The client pushes data to all replicas, which is stored by an LRU buffer cache until the data is used or expires</li>
<li>Once all replicas have acknowledged receiving the data, the client sends a write request to the primary, which identifies the data that the client just pushed. The primary assigns consecutive sequence numbers to all mutations it receives (possibly from other clients as well), and then applies the mutations locally</li>
<li>The primary forwards the write request to all secondary replica, and the replicas apply the mutations in the same order defined by the primary.</li>
<li>The secondaries ack that they completed the operation</li>
<li>The primary replies to the client, and any errors are reported to the client. Errors leave the region in an inconsistent state, but the failed mutation is usually retried multiple times, until eventually falling back to redoing the entire write.</li>
</ol>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/cloud%20storage.html">cloud storage</a>
                <a href="/tags/distributed%20systems.html">distributed systems</a>
                <a href="/tags/google%20cloud.html">google cloud</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
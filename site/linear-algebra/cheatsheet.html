
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Matrix Theory | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Covers fundamental concepts in linear algebra and matrix theory, including vector spaces, basic operations, systems of linear equations, matrices, eigenvalues a">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/linear-algebra/cheatsheet.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Matrix Theory">
    <meta property="og:description" content="Covers fundamental concepts in linear algebra and matrix theory, including vector spaces, basic operations, systems of linear equations, matrices, eigenvalues a">
    <meta property="og:url" content="https://notes.elimelt.com/linear-algebra/cheatsheet.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Matrix Theory">
    <meta name="twitter:description" content="Covers fundamental concepts in linear algebra and matrix theory, including vector spaces, basic operations, systems of linear equations, matrices, eigenvalues a">

    <meta name="keywords" content="linear algebra,vector spaces,operators,dual spaces,tensor products">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "Matrix Theory", "dateModified": "2025-08-09T18:05:18.292561", "description": "Covers fundamental concepts in linear algebra and matrix theory, including vector spaces, basic operations, systems of linear equations, matrices, eigenvalues a", "url": "https://notes.elimelt.com/linear-algebra/cheatsheet.html", "articleSection": "Linear Algebra", "keywords": "linear algebra,vector spaces,operators,dual spaces,tensor products"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/linear%20algebra.html">Linear Algebra</a> » Matrix Theory
        </div>
    </header>
    <main role="main">
        <article>
            <h1>Matrix Theory</h1>
            <div class="meta">
                <time datetime="2025-08-09T18:05:18.292561">
                    Last modified: 2025-08-09
                </time>
                <span>Category: <a href="/categories/linear%20algebra.html">Linear Algebra</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="fundamentals-of-vectors"><a class="toclink" href="#fundamentals-of-vectors">Fundamentals of Vectors</a></h1>
<h2 id="geometric-basics"><a class="toclink" href="#geometric-basics">Geometric Basics</a></h2>
<h3 id="definition-and-representation"><a class="toclink" href="#definition-and-representation">Definition and Representation</a></h3>
<ul>
<li>A vector is a quantity with both magnitude and direction</li>
<li>Notation: $\vec{v}$ or $\mathbf{v}$ or $\begin{pmatrix} x \ y \ z \end{pmatrix}$</li>
<li>Components: $\vec{v} = \langle v_1, v_2, v_3 \rangle$ in 3D space</li>
</ul>
<h3 id="geometric-interpretation"><a class="toclink" href="#geometric-interpretation">Geometric Interpretation</a></h3>
<ul>
<li>Directed line segment from initial point to terminal point</li>
<li>Length (magnitude): $|\vec{v}| = \sqrt{v_1^2 + v_2^2 + v_3^2}$</li>
<li>Two vectors are equal if they have same magnitude and direction</li>
</ul>
<h3 id="position-vectors"><a class="toclink" href="#position-vectors">Position Vectors</a></h3>
<ul>
<li>Vector from origin to a point $P(x,y,z)$</li>
<li>Written as: $\vec{r} = x\mathbf{i} + y\mathbf{j} + z\mathbf{k}$</li>
</ul>
<h3 id="direction-vectors"><a class="toclink" href="#direction-vectors">Direction Vectors</a></h3>
<ul>
<li>Unit vector in direction of $\vec{v}$: $\hat{v} = \frac{\vec{v}}{|\vec{v}|}$</li>
<li>Represents pure direction (magnitude = 1)</li>
</ul>
<h3 id="unit-vectors"><a class="toclink" href="#unit-vectors">Unit Vectors</a></h3>
<ul>
<li>Vectors with magnitude 1: $|\vec{u}| = 1$</li>
<li>Direction cosines: $\cos \alpha = \frac{v_1}{|\vec{v}|}, \cos \beta = \frac{v_2}{|\vec{v}|}, \cos \gamma = \frac{v_3}{|\vec{v}|}$</li>
</ul>
<h4 id="standard-basis-vectors"><a class="toclink" href="#standard-basis-vectors">Standard Basis Vectors</a></h4>
<table>
<thead>
<tr>
<th>Vector</th>
<th>Components</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathbf{i}$</td>
<td>$\langle 1,0,0 \rangle$</td>
<td>Unit vector in x-direction</td>
</tr>
<tr>
<td>$\mathbf{j}$</td>
<td>$\langle 0,1,0 \rangle$</td>
<td>Unit vector in y-direction</td>
</tr>
<tr>
<td>$\mathbf{k}$</td>
<td>$\langle 0,0,1 \rangle$</td>
<td>Unit vector in z-direction</td>
</tr>
</tbody>
</table>
<h2 id="basic-vector-operations"><a class="toclink" href="#basic-vector-operations">Basic Vector Operations</a></h2>
<h3 id="addition-and-subtraction"><a class="toclink" href="#addition-and-subtraction">Addition and Subtraction</a></h3>
<ul>
<li>Addition: $\vec{a} + \vec{b} = \langle a_1+b_1, a_2+b_2, a_3+b_3 \rangle$</li>
<li>Subtraction: $\vec{a} - \vec{b} = \langle a_1-b_1, a_2-b_2, a_3-b_3 \rangle$</li>
</ul>
<h4 id="parallelogram-law"><a class="toclink" href="#parallelogram-law">Parallelogram Law</a></h4>
<ul>
<li>Sum of vectors forms diagonal of parallelogram</li>
<li>$\vec{a} + \vec{b} = \vec{d}$ where $\vec{d}$ is diagonal</li>
</ul>
<h4 id="triangle-inequality"><a class="toclink" href="#triangle-inequality">Triangle Inequality</a></h4>
<ul>
<li>$|\vec{a} + \vec{b}| \leq |\vec{a}| + |\vec{b}|$</li>
<li>Equality holds if and only if vectors are parallel</li>
</ul>
<h3 id="scalar-multiplication"><a class="toclink" href="#scalar-multiplication">Scalar Multiplication</a></h3>
<ul>
<li>$c\vec{v} = \langle cv_1, cv_2, cv_3 \rangle$</li>
<li>Properties:</li>
<li>$c(\vec{a} + \vec{b}) = c\vec{a} + c\vec{b}$</li>
<li>$(c_1 + c_2)\vec{a} = c_1\vec{a} + c_2\vec{a}$</li>
<li>$|c\vec{v}| = |c||\vec{v}|$</li>
</ul>
<h1 id="systems-of-linear-equations"><a class="toclink" href="#systems-of-linear-equations">Systems of Linear Equations</a></h1>
<h2 id="matrix-form-ax-b"><a class="toclink" href="#matrix-form-ax-b">Matrix Form (Ax = b)</a></h2>
<p>$$
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \ x_2 \ \vdots \ x_n
\end{bmatrix} =
\begin{bmatrix}
b_1 \ b_2 \ \vdots \ b_m
\end{bmatrix}
$$</p>
<h2 id="solution-types"><a class="toclink" href="#solution-types">Solution Types</a></h2>
<table>
<thead>
<tr>
<th>Type</th>
<th>Condition</th>
<th>Geometric Interpretation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unique Solution</td>
<td>$\text{rank}(A) = \text{rank}([A|b]) = n$</td>
<td>Lines/planes intersect at one point</td>
</tr>
<tr>
<td>Infinite Solutions</td>
<td>$\text{rank}(A) = \text{rank}([A|b]) &lt; n$</td>
<td>Lines/planes overlap</td>
</tr>
<tr>
<td>No Solution</td>
<td>$\text{rank}(A) &lt; \text{rank}([A|b])$</td>
<td>Lines/planes are parallel</td>
</tr>
</tbody>
</table>
<h3 id="geometric-interpretation_1"><a class="toclink" href="#geometric-interpretation_1">Geometric Interpretation</a></h3>
<ul>
<li>2D: Intersection of lines</li>
<li>3D: Intersection of planes</li>
<li>Higher dimensions: Intersection of hyperplanes</li>
</ul>
<h2 id="solution-methods"><a class="toclink" href="#solution-methods">Solution Methods</a></h2>
<h3 id="gaussian-elimination"><a class="toclink" href="#gaussian-elimination">Gaussian Elimination</a></h3>
<ol>
<li>
<p>Forward Elimination</p>
</li>
<li>
<p>Convert matrix to row echelon form (REF)</p>
</li>
<li>Create zeros below diagonal</li>
</ol>
<p><code>[1 * * *]
   [0 1 * *]
   [0 0 1 *]
   [0 0 0 1]</code></p>
<ol>
<li>Back Substitution</li>
<li>Solve for variables from bottom up</li>
<li>$x_n \rightarrow x_{n-1} \rightarrow \cdots \rightarrow x_1$</li>
</ol>
<h3 id="gauss-jordan-elimination"><a class="toclink" href="#gauss-jordan-elimination">Gauss-Jordan Elimination</a></h3>
<ul>
<li>Convert to reduced row echelon form (RREF)</li>
<li>Create zeros above and below diagonal</li>
</ul>
<pre><code>[1 0 0 *]
[0 1 0 *]
[0 0 1 *]
</code></pre>
<h4 id="key-operations"><a class="toclink" href="#key-operations">Key Operations</a></h4>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Description</th>
<th>Notation</th>
</tr>
</thead>
<tbody>
<tr>
<td>Row Swap</td>
<td>Swap rows $i$ and $j$</td>
<td>$R_i \leftrightarrow R_j$</td>
</tr>
<tr>
<td>Scalar Multiplication</td>
<td>Multiply row $i$ by $c$</td>
<td>$cR_i$</td>
</tr>
<tr>
<td>Row Addition</td>
<td>Add multiple of row $i$ to row $j$</td>
<td>$R_j + cR_i$</td>
</tr>
</tbody>
</table>
<h1 id="matrices"><a class="toclink" href="#matrices">Matrices</a></h1>
<h2 id="types-and-properties"><a class="toclink" href="#types-and-properties">Types and Properties</a></h2>
<table>
<thead>
<tr>
<th>Type</th>
<th>Definition</th>
<th>Properties</th>
</tr>
</thead>
<tbody>
<tr>
<td>Square</td>
<td>$n \times n$ matrix</td>
<td>- Same number of rows and columns<br>- Can have determinant<br>- May be invertible</td>
</tr>
<tr>
<td>Rectangular</td>
<td>$m \times n$ matrix</td>
<td>- Different number of rows and columns<br>- No determinant</td>
</tr>
<tr>
<td>Identity ($I_n$)</td>
<td>$a_{ij} = \begin{cases} 1 &amp; \text{if } i=j \ 0 &amp; \text{if } i\neq j \end{cases}$</td>
<td>- Square matrix<br>- 1's on diagonal, 0's elsewhere<br>- $AI = IA = A$</td>
</tr>
<tr>
<td>Zero ($0$)</td>
<td>All entries are 0</td>
<td>- Can be any dimension<br>- $A + 0 = A$</td>
</tr>
<tr>
<td>Diagonal</td>
<td>$a_{ij} = 0$ for $i \neq j$</td>
<td>- Non-zero elements only on main diagonal</td>
</tr>
<tr>
<td>Triangular</td>
<td>Upper: $a_{ij} = 0$ for $i &gt; j$<br>Lower: $a_{ij} = 0$ for $i &lt; j$</td>
<td>- Square matrix<br>- Determinant = product of diagonal entries</td>
</tr>
</tbody>
</table>
<h2 id="basic-matrix-operations"><a class="toclink" href="#basic-matrix-operations">Basic Matrix Operations</a></h2>
<h3 id="addition-and-subtraction_1"><a class="toclink" href="#addition-and-subtraction_1">Addition and Subtraction</a></h3>
<ul>
<li>Only defined for matrices of same dimensions</li>
<li>$(A \pm B)<em ij="ij">{ij} = a</em>$} \pm b_{ij</li>
<li>Commutative: $A + B = B + A$</li>
<li>Associative: $(A + B) + C = A + (B + C)$</li>
</ul>
<h3 id="scalar-multiplication_1"><a class="toclink" href="#scalar-multiplication_1">Scalar Multiplication</a></h3>
<ul>
<li>$(cA)<em ij="ij">{ij} = c(a</em>)$</li>
<li>Distributive: $c(A + B) = cA + cB$</li>
<li>$(cd)A = c(dA)$</li>
</ul>
<h3 id="transpose"><a class="toclink" href="#transpose">Transpose</a></h3>
<ul>
<li>$(A^T)<em ji="ji">{ij} = a</em>$</li>
<li>$(A^T)^T = A$</li>
<li>$(A + B)^T = A^T + B^T$</li>
<li>$(cA)^T = cA^T$</li>
<li>$(AB)^T = B^T A^T$</li>
</ul>
<h2 id="row-echelon-form"><a class="toclink" href="#row-echelon-form">Row Echelon Form</a></h2>
<p>A matrix is in row echelon form if:</p>
<ol>
<li>All zero rows are at the bottom</li>
<li>Leading coefficient (pivot) of each nonzero row is to the right of pivots above</li>
<li>All entries below pivots are zero</li>
</ol>
<h3 id="reduced-row-echelon-form"><a class="toclink" href="#reduced-row-echelon-form">Reduced Row Echelon Form</a></h3>
<p>Additional conditions for RREF:</p>
<ol>
<li>Leading coefficient of each nonzero row is 1</li>
<li>Each leading 1 is the only nonzero entry in its column</li>
</ol>
<h3 id="pivot-positions"><a class="toclink" href="#pivot-positions">Pivot Positions</a></h3>
<ul>
<li>First nonzero element in each row</li>
<li>Determines rank of matrix</li>
<li>Maximum number of linearly independent columns</li>
</ul>
<h3 id="leading-entries"><a class="toclink" href="#leading-entries">Leading Entries</a></h3>
<p>Properties:</p>
<ul>
<li>Always nonzero</li>
<li>Leftmost nonzero entry in row</li>
<li>Each leading entry is right of leading entries above it</li>
</ul>
<h1 id="vector-spaces"><a class="toclink" href="#vector-spaces">Vector Spaces</a></h1>
<h2 id="axioms-of-vector-spaces"><a class="toclink" href="#axioms-of-vector-spaces">Axioms of Vector Spaces</a></h2>
<p>Let $V$ be a vector space over field $F$ with vectors $\mathbf{u}, \mathbf{v}, \mathbf{w} \in V$ and scalars $c, d \in F$:</p>
<ol>
<li>Closure under addition: $\mathbf{u} + \mathbf{v} \in V$</li>
<li>Commutativity: $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$</li>
<li>Associativity: $(\mathbf{u} + \mathbf{v}) + \mathbf{w} = \mathbf{u} + (\mathbf{v} + \mathbf{w})$</li>
<li>Additive identity: $\exists \mathbf{0} \in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$</li>
<li>Additive inverse: $\exists -\mathbf{v} \in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$</li>
<li>Scalar multiplication closure: $c\mathbf{v} \in V$</li>
<li>Scalar multiplication distributivity: $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$</li>
<li>Vector distributivity: $(c + d)\mathbf{v} = c\mathbf{v} + d\mathbf{v}$</li>
<li>Scalar multiplication associativity: $c(d\mathbf{v}) = (cd)\mathbf{v}$</li>
<li>Scalar multiplication identity: $1\mathbf{v} = \mathbf{v}$</li>
</ol>
<h2 id="linear-independence"><a class="toclink" href="#linear-independence">Linear Independence</a></h2>
<h3 id="definition"><a class="toclink" href="#definition">Definition</a></h3>
<p>Vectors ${\mathbf{v}_1, \mathbf{v}_2, ..., \mathbf{v}_n}$ are linearly independent if:
$$c_1\mathbf{v}_1 + c_2\mathbf{v}_2 + ... + c_n\mathbf{v}_n = \mathbf{0}$$
implies $c_1 = c_2 = ... = c_n = 0$</p>
<h3 id="tests-and-algorithms"><a class="toclink" href="#tests-and-algorithms">Tests and Algorithms</a></h3>
<table>
<thead>
<tr>
<th>Test</th>
<th>Description</th>
<th>Result</th>
</tr>
</thead>
<tbody>
<tr>
<td>Matrix Test</td>
<td>Form matrix $A$ with vectors as columns and solve $A\mathbf{x}=\mathbf{0}$</td>
<td>Independent if only solution is trivial</td>
</tr>
<tr>
<td>Determinant</td>
<td>For square matrix of vectors</td>
<td>Independent if det$(A) \neq 0$</td>
</tr>
<tr>
<td>Rank</td>
<td>Compute rank of matrix $A$</td>
<td>Independent if rank = number of vectors</td>
</tr>
</tbody>
</table>
<h2 id="span"><a class="toclink" href="#span">Span</a></h2>
<h3 id="definition_1"><a class="toclink" href="#definition_1">Definition</a></h3>
<p>The span of vectors ${\mathbf{v}_1, ..., \mathbf{v}_n}$ is:
$$\text{span}{\mathbf{v}_1, ..., \mathbf{v}_n} = {c_1\mathbf{v}_1 + ... + c_n\mathbf{v}_n : c_i \in F}$$</p>
<h3 id="geometric-interpretation_2"><a class="toclink" href="#geometric-interpretation_2">Geometric Interpretation</a></h3>
<ul>
<li>1 vector: line through origin</li>
<li>2 vectors: plane through origin (if independent)</li>
<li>3 vectors: 3D space (if independent)</li>
</ul>
<h2 id="basis"><a class="toclink" href="#basis">Basis</a></h2>
<p>A basis is a linearly independent set of vectors that spans the vector space.</p>
<h3 id="standard-basis"><a class="toclink" href="#standard-basis">Standard Basis</a></h3>
<p>For $\mathbb{R}^n$: ${\mathbf{e}_1, \mathbf{e}_2, ..., \mathbf{e}_n}$ where:
$\mathbf{e}_i = [0, ..., 1, ..., 0]^T$ (1 in $i$th position)</p>
<h3 id="dimension"><a class="toclink" href="#dimension">Dimension</a></h3>
<p>The dimension of a vector space is the number of vectors in any basis.</p>
<h4 id="basis-theorem"><a class="toclink" href="#basis-theorem">Basis Theorem</a></h4>
<p>Every basis of a vector space has the same number of vectors.</p>
<h4 id="dimension-theorem"><a class="toclink" href="#dimension-theorem">Dimension Theorem</a></h4>
<p>For finite-dimensional vector space $V$:</p>
<ul>
<li>Any linearly independent set can be extended to a basis</li>
<li>Any spanning set can be reduced to a basis</li>
<li>$\dim(V_1 + V_2) = \dim(V_1) + \dim(V_2) - \dim(V_1 \cap V_2)$</li>
</ul>
<h2 id="subspaces"><a class="toclink" href="#subspaces">Subspaces</a></h2>
<h3 id="tests-for-subspaces"><a class="toclink" href="#tests-for-subspaces">Tests for Subspaces</a></h3>
<p>A subset $W$ of vector space $V$ is a subspace if:</p>
<ol>
<li>$\mathbf{0} \in W$</li>
<li>Closed under addition: $\mathbf{u}, \mathbf{v} \in W \implies \mathbf{u} + \mathbf{v} \in W$</li>
<li>Closed under scalar multiplication: $c \in F, \mathbf{v} \in W \implies c\mathbf{v} \in W$</li>
</ol>
<h3 id="common-subspaces"><a class="toclink" href="#common-subspaces">Common Subspaces</a></h3>
<table>
<thead>
<tr>
<th>Subspace</th>
<th>Definition</th>
<th>Dimension</th>
</tr>
</thead>
<tbody>
<tr>
<td>Null Space</td>
<td>$N(A) = {\mathbf{x}: A\mathbf{x}=\mathbf{0}}$</td>
<td>$n - \text{rank}(A)$</td>
</tr>
<tr>
<td>Column Space</td>
<td>$C(A) = {\mathbf{y}: \mathbf{y}=A\mathbf{x}}$</td>
<td>$\text{rank}(A)$</td>
</tr>
<tr>
<td>Row Space</td>
<td>$R(A) = C(A^T)$</td>
<td>$\text{rank}(A)$</td>
</tr>
</tbody>
</table>
<h1 id="advanced-vector-operations-cont-1"><a class="toclink" href="#advanced-vector-operations-cont-1">Advanced Vector Operations (cont. 1)</a></h1>
<h2 id="dot-product"><a class="toclink" href="#dot-product">Dot Product</a></h2>
<p>The scalar product of two vectors.</p>
<h3 id="geometric-definition"><a class="toclink" href="#geometric-definition">Geometric Definition</a></h3>
<p>$$\vec{a} \cdot \vec{b} = |\vec{a}||\vec{b}|\cos(\theta)$$
where $\theta$ is the angle between vectors</p>
<h3 id="algebraic-definition"><a class="toclink" href="#algebraic-definition">Algebraic Definition</a></h3>
<p>For vectors in $\mathbb{R}^n$:
$$\vec{a} \cdot \vec{b} = \sum_{i=1}^n a_ib_i = a_1b_1 + a_2b_2 + ... + a_nb_n$$</p>
<h3 id="properties"><a class="toclink" href="#properties">Properties</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Commutative</td>
<td>$\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$</td>
<td>Order doesn't matter</td>
</tr>
<tr>
<td>Distributive</td>
<td>$\vec{a} \cdot (\vec{b} + \vec{c}) = \vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c}$</td>
<td>Distributes over addition</td>
</tr>
<tr>
<td>Scalar Multiplication</td>
<td>$(k\vec{a}) \cdot \vec{b} = k(\vec{a} \cdot \vec{b})$</td>
<td>Scalars can be factored out</td>
</tr>
<tr>
<td>Self-Dot Product</td>
<td>$\vec{a} \cdot \vec{a} = |\vec{a}|^2$</td>
<td>Dot product with itself equals magnitude squared</td>
</tr>
</tbody>
</table>
<h3 id="angle-formula"><a class="toclink" href="#angle-formula">Angle Formula</a></h3>
<p>$$\cos(\theta) = \frac{\vec{a} \cdot \vec{b}}{|\vec{a}||\vec{b}|}$$</p>
<h3 id="projection-formula"><a class="toclink" href="#projection-formula">Projection Formula</a></h3>
<p>Vector projection of $\vec{a}$ onto $\vec{b}$:
$$\text{proj}_{\vec{b}}\vec{a} = \frac{\vec{a} \cdot \vec{b}}{|\vec{b}|^2}\vec{b}$$</p>
<h2 id="cross-product"><a class="toclink" href="#cross-product">Cross Product</a></h2>
<p>Vector product resulting in a vector perpendicular to both input vectors (3D only).</p>
<h3 id="right-hand-rule"><a class="toclink" href="#right-hand-rule">Right-Hand Rule</a></h3>
<ol>
<li>Point index finger in direction of first vector</li>
<li>Point middle finger in direction of second vector</li>
<li>Thumb points in direction of cross product</li>
</ol>
<h3 id="properties_1"><a class="toclink" href="#properties_1">Properties</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Formula</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anti-commutative</td>
<td>$\vec{a} \times \vec{b} = -(\vec{b} \times \vec{a})$</td>
<td>Order matters</td>
</tr>
<tr>
<td>Distributive</td>
<td>$\vec{a} \times (\vec{b} + \vec{c}) = \vec{a} \times \vec{b} + \vec{a} \times \vec{c}$</td>
<td>Distributes over addition</td>
</tr>
<tr>
<td>Magnitude</td>
<td>$|\vec{a} \times \vec{b}| = |\vec{a}||\vec{b}|\sin(\theta)$</td>
<td>Area of parallelogram</td>
</tr>
<tr>
<td>Perpendicular</td>
<td>$\vec{a} \times \vec{b} \perp \vec{a}$ and $\vec{a} \times \vec{b} \perp \vec{b}$</td>
<td>Result is perpendicular to both vectors</td>
</tr>
</tbody>
</table>
<h3 id="triple-product"><a class="toclink" href="#triple-product">Triple Product</a></h3>
<p>Scalar triple product:
$$\vec{a} \cdot (\vec{b} \times \vec{c}) = \det[\vec{a} \; \vec{b} \; \vec{c}]$$
Represents volume of parallelepiped</p>
<h2 id="linear-combinations"><a class="toclink" href="#linear-combinations">Linear Combinations</a></h2>
<p>Sum of vectors with scalar coefficients:
$$c_1\vec{v_1} + c_2\vec{v_2} + ... + c_n\vec{v_n}$$</p>
<h3 id="geometric-interpretation_3"><a class="toclink" href="#geometric-interpretation_3">Geometric Interpretation</a></h3>
<ul>
<li>For two vectors: Points on plane formed by vectors</li>
<li>For three vectors: Points in space formed by vectors</li>
<li>Span: Set of all possible linear combinations</li>
</ul>
<h1 id="matrix-properties-cont-1"><a class="toclink" href="#matrix-properties-cont-1">Matrix Properties (cont. 1)</a></h1>
<h2 id="rank"><a class="toclink" href="#rank">Rank</a></h2>
<p>The rank of a matrix is the dimension of the vector space spanned by its columns (or rows).</p>
<h3 id="full-rank"><a class="toclink" href="#full-rank">Full Rank</a></h3>
<p>A matrix has full rank when:</p>
<ul>
<li>For m×n matrix: rank = min(m,n)</li>
<li>Column rank = Row rank</li>
<li>For square matrix: rank = n ⟺ matrix is invertible</li>
</ul>
<h3 id="rank-theorems"><a class="toclink" href="#rank-theorems">Rank Theorems</a></h3>
<table>
<thead>
<tr>
<th>Theorem</th>
<th>Statement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rank-Nullity</td>
<td>For m×n matrix A: rank(A) + nullity(A) = n</td>
</tr>
<tr>
<td>Product Rank</td>
<td>rank(AB) ≤ min(rank(A), rank(B))</td>
</tr>
<tr>
<td>Addition Rank</td>
<td>rank(A + B) ≤ rank(A) + rank(B)</td>
</tr>
</tbody>
</table>
<h2 id="trace"><a class="toclink" href="#trace">Trace</a></h2>
<p>The trace of a square matrix is the sum of elements on the main diagonal.
$$tr(A) = \sum_{i=1}^n a_{ii}$$</p>
<h3 id="properties_2"><a class="toclink" href="#properties_2">Properties</a></h3>
<ul>
<li>tr(A + B) = tr(A) + tr(B)</li>
<li>tr(cA) = c⋅tr(A)</li>
<li>tr(AB) = tr(BA)</li>
<li>tr(A^T) = tr(A)</li>
<li>For eigenvalues λᵢ: tr(A) = ∑λᵢ</li>
</ul>
<h2 id="determinant"><a class="toclink" href="#determinant">Determinant</a></h2>
<p>For square matrix A, det(A) or |A| measures the scaling factor of the linear transformation.</p>
<h3 id="properties_3"><a class="toclink" href="#properties_3">Properties</a></h3>
<ol>
<li>det(AB) = det(A)⋅det(B)</li>
<li>det(A^T) = det(A)</li>
<li>det(A^{-1}) = \frac{1}{det(A)}</li>
<li>For triangular matrices: det = product of diagonal entries</li>
<li>det(cA) = c^n det(A) for n×n matrix</li>
</ol>
<h3 id="calculation-methods"><a class="toclink" href="#calculation-methods">Calculation Methods</a></h3>
<h4 id="cofactor-expansion"><a class="toclink" href="#cofactor-expansion">Cofactor Expansion</a></h4>
<p>For n×n matrix:
$$det(A) = \sum_{j=1}^n a_{ij}C_{ij}$$
where Cᵢⱼ is the (i,j) cofactor</p>
<h4 id="rowcolumn-expansion"><a class="toclink" href="#rowcolumn-expansion">Row/Column Expansion</a></h4>
<p>Choose any row/column i:
$$det(A) = \sum_{j=1}^n (-1)^{i+j} a_{ij}M_{ij}$$
where Mᵢⱼ is the minor</p>
<h4 id="triangular-method"><a class="toclink" href="#triangular-method">Triangular Method</a></h4>
<ol>
<li>Convert to upper triangular using row operations</li>
<li>Multiply diagonal elements</li>
<li>Account for row operation signs</li>
</ol>
<h3 id="cramers-rule"><a class="toclink" href="#cramers-rule">Cramer's Rule</a></h3>
<p>For system Ax = b with det(A) ≠ 0:
$$x_i = \frac{det(A_i)}{det(A)}$$
where Aᵢ is A with column i replaced by b</p>
<h1 id="matrix-operations-cont-1"><a class="toclink" href="#matrix-operations-cont-1">Matrix Operations (cont. 1)</a></h1>
<h2 id="matrix-multiplication"><a class="toclink" href="#matrix-multiplication">Matrix Multiplication</a></h2>
<p>For matrices $A_{m×n}$ and $B_{n×p}$:
$$(AB)<em k="1">{ij} = \sum</em>$$}^n a_{ik}b_{kj</p>
<h3 id="properties_4"><a class="toclink" href="#properties_4">Properties</a></h3>
<ul>
<li>Distributive: $A(B + C) = AB + AC$</li>
<li>Associative: $(AB)C = A(BC)$</li>
<li>Scalar multiplication: $c(AB) = (cA)B = A(cB)$</li>
<li>Identity: $AI = IA = A$</li>
<li>Zero matrix: $A0 = 0A = 0$</li>
</ul>
<h3 id="non-commutativity"><a class="toclink" href="#non-commutativity">Non-commutativity</a></h3>
<ul>
<li>Generally, $AB \neq BA$</li>
<li>Exception: If $A$ and $B$ commute, they are called "commuting matrices"</li>
<li>Special cases where $AB = BA$:</li>
<li>When $A$ or $B$ is identity matrix</li>
<li>When $A$ or $B$ is scalar multiple of identity</li>
<li>When $A$ and $B$ are diagonal matrices</li>
</ul>
<h3 id="associativity"><a class="toclink" href="#associativity">Associativity</a></h3>
<p>$(AB)C = A(BC)$ always holds for conformable matrices</p>
<h2 id="inverse"><a class="toclink" href="#inverse">Inverse</a></h2>
<p>For square matrix $A$, if $AA^{-1} = A^{-1}A = I$, then $A^{-1}$ is the inverse of $A$</p>
<h3 id="existence-conditions"><a class="toclink" href="#existence-conditions">Existence Conditions</a></h3>
<ul>
<li>Matrix must be square</li>
<li>Matrix must be non-singular (determinant ≠ 0)</li>
<li>rank(A) = n for n×n matrix</li>
</ul>
<h3 id="properties_5"><a class="toclink" href="#properties_5">Properties</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Formula</th>
</tr>
</thead>
<tbody>
<tr>
<td>Double Inverse</td>
<td>$(A^{-1})^{-1} = A$</td>
</tr>
<tr>
<td>Product Inverse</td>
<td>$(AB)^{-1} = B^{-1}A^{-1}$</td>
</tr>
<tr>
<td>Scalar Inverse</td>
<td>$(cA)^{-1} = \frac{1}{c}A^{-1}$</td>
</tr>
<tr>
<td>Transpose Inverse</td>
<td>$(A^{-1})^T = (A^T)^{-1}$</td>
</tr>
</tbody>
</table>
<h3 id="calculation-methods_1"><a class="toclink" href="#calculation-methods_1">Calculation Methods</a></h3>
<h4 id="adjugate-method"><a class="toclink" href="#adjugate-method">Adjugate Method</a></h4>
<p>For an n×n matrix:
$$A^{-1} = \frac{1}{\det(A)}\text{adj}(A)$$
where adj(A) is the adjugate matrix</p>
<h4 id="gaussian-elimination_1"><a class="toclink" href="#gaussian-elimination_1">Gaussian Elimination</a></h4>
<ol>
<li>Form augmented matrix $\lbrack  A|I  \rbrack$</li>
<li>Convert left side to identity matrix</li>
<li>Right side becomes $A^{-1}$</li>
</ol>
<h3 id="elementary-matrices"><a class="toclink" href="#elementary-matrices">Elementary Matrices</a></h3>
<ul>
<li>Result from applying elementary row operations to identity matrix</li>
<li>Types:</li>
<li>Row swap: $E_{ij}$</li>
<li>Row multiplication: $E_i(c)$</li>
<li>Row addition: $E_{ij}(c)$</li>
<li>Properties:</li>
<li>Always invertible</li>
<li>$(E_1E_2...E_k)A = A'$ where $A'$ is row reduced form</li>
</ul>
<h2 id="conjugate-transpose"><a class="toclink" href="#conjugate-transpose">Conjugate Transpose</a></h2>
<p>For matrix $A$:</p>
<ul>
<li>Denoted as $A^H$ or $A^*$</li>
<li>$(a_{ij})^H = \overline{a_{ji}}$</li>
<li>Properties:</li>
<li>$(A^H)^H = A$</li>
<li>$(AB)^H = B^HA^H$</li>
<li>$(A + B)^H = A^H + B^H$</li>
<li>For real matrices, $A^H = A^T$</li>
</ul>
<h1 id="systems-of-linear-equations-cont-1"><a class="toclink" href="#systems-of-linear-equations-cont-1">Systems of Linear Equations (cont. 1)</a></h1>
<h2 id="homogeneous-systems-amathbfx-mathbf0"><a class="toclink" href="#homogeneous-systems-amathbfx-mathbf0">Homogeneous Systems ($A\mathbf{x} = \mathbf{0}$)</a></h2>
<p>A system where all constants are zero: $A\mathbf{x} = \mathbf{0}$</p>
<h3 id="trivial-solution"><a class="toclink" href="#trivial-solution">Trivial Solution</a></h3>
<ul>
<li>Always has solution $\mathbf{x} = \mathbf{0}$ (zero vector)</li>
<li>Exists for any coefficient matrix $A$</li>
</ul>
<h3 id="nontrivial-solutions"><a class="toclink" href="#nontrivial-solutions">Nontrivial Solutions</a></h3>
<ul>
<li>Exist if and only if $\text{rank}(A) &lt; n$ where $n$ is number of variables</li>
<li>Equivalent to $\text{det}(A) = 0$ for square matrices</li>
<li>Number of free variables = $n - \text{rank}(A)$</li>
</ul>
<h2 id="consistency-theorems"><a class="toclink" href="#consistency-theorems">Consistency Theorems</a></h2>
<h3 id="consistent-system-conditions"><a class="toclink" href="#consistent-system-conditions">Consistent System Conditions</a></h3>
<table>
<thead>
<tr>
<th>Condition</th>
<th>System is Consistent When</th>
</tr>
</thead>
<tbody>
<tr>
<td>Rank Test</td>
<td>$\text{rank}(A) = \text{rank}([A|\mathbf{b}])$</td>
</tr>
<tr>
<td>Square Matrix</td>
<td>$\text{det}(A) \neq 0$</td>
</tr>
<tr>
<td>General</td>
<td>Solutions exist if $\mathbf{b}$ is in column space of $A$</td>
</tr>
</tbody>
</table>
<h3 id="fredholm-alternative"><a class="toclink" href="#fredholm-alternative">Fredholm Alternative</a></h3>
<p>For system $A\mathbf{x} = \mathbf{b}$, exactly one of these is true:</p>
<ol>
<li>System has a solution</li>
<li>$\mathbf{y}^T A = \mathbf{0}$ has a solution with $\mathbf{y}^T\mathbf{b} \neq 0$</li>
</ol>
<h2 id="cramers-rule_1"><a class="toclink" href="#cramers-rule_1">Cramer's Rule</a></h2>
<p>For system $A\mathbf{x} = \mathbf{b}$ where $A$ is $n \times n$ with $\text{det}(A) \neq 0$:
$$x_i = \frac{\text{det}(A_i)}{\text{det}(A)}$$
Where $A_i$ is matrix $A$ with column $i$ replaced by $\mathbf{b}$</p>
<h2 id="matrix-inverse-method"><a class="toclink" href="#matrix-inverse-method">Matrix Inverse Method</a></h2>
<p>For square system $A\mathbf{x} = \mathbf{b}$ where $A$ is invertible:
$$\mathbf{x} = A^{-1}\mathbf{b}$$</p>
<p>Requirements:</p>
<ul>
<li>$A$ must be square</li>
<li>$\text{det}(A) \neq 0$</li>
<li>Computationally expensive for large systems</li>
</ul>
<h1 id="linear-transformations"><a class="toclink" href="#linear-transformations">Linear Transformations</a></h1>
<h2 id="definition_2"><a class="toclink" href="#definition_2">Definition</a></h2>
<p>A linear transformation $T: V \to W$ is a function between vector spaces that preserves:</p>
<ol>
<li>Addition: $T(u + v) = T(u) + T(v)$</li>
<li>Scalar multiplication: $T(cv) = cT(v)$</li>
</ol>
<h2 id="matrix-representation"><a class="toclink" href="#matrix-representation">Matrix Representation</a></h2>
<p>Every linear transformation $T: \mathbb{R}^n \to \mathbb{R}^m$ can be represented by a unique $m \times n$ matrix $A$</p>
<h3 id="standard-matrix"><a class="toclink" href="#standard-matrix">Standard Matrix</a></h3>
<p>For transformation $T$, the standard matrix $A$ is formed by:
$$A = [T(e_1) \; T(e_2) \; \cdots \; T(e_n)]$$
where $e_i$ are standard basis vectors</p>
<h2 id="properties_6"><a class="toclink" href="#properties_6">Properties</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>Definition</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kernel (Null Space)</td>
<td>$\text{ker}(T) = {v \in V : T(v) = 0}$</td>
<td>Solve $Ax = 0$</td>
</tr>
<tr>
<td>Range (Image)</td>
<td>$\text{range}(T) = {T(v) : v \in V}$</td>
<td>Span of columns of $A$</td>
</tr>
<tr>
<td>One-to-One (Injective)</td>
<td>$T(v_1) = T(v_2) \implies v_1 = v_2$</td>
<td>$\text{ker}(T) = {0}$</td>
</tr>
<tr>
<td>Onto (Surjective)</td>
<td>$\text{range}(T) = W$</td>
<td>Columns span $W$</td>
</tr>
<tr>
<td>Isomorphism</td>
<td>Bijective linear transformation</td>
<td>One-to-one and onto</td>
</tr>
</tbody>
</table>
<h2 id="special-transformations"><a class="toclink" href="#special-transformations">Special Transformations</a></h2>
<h3 id="rotation"><a class="toclink" href="#rotation">Rotation</a></h3>
<ul>
<li>2D rotation by angle $\theta$:
  $$R_\theta = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}$$</li>
</ul>
<h3 id="reflection"><a class="toclink" href="#reflection">Reflection</a></h3>
<ul>
<li>Across x-axis: $\begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix}$</li>
<li>Across y-axis: $\begin{bmatrix} -1 &amp; 0 \ 0 &amp; 1 \end{bmatrix}$</li>
</ul>
<h3 id="projection"><a class="toclink" href="#projection">Projection</a></h3>
<ul>
<li>Onto x-axis: $\begin{bmatrix} 1 &amp; 0 \ 0 &amp; 0 \end{bmatrix}$</li>
<li>Onto y-axis: $\begin{bmatrix} 0 &amp; 0 \ 0 &amp; 1 \end{bmatrix}$</li>
</ul>
<h3 id="scaling"><a class="toclink" href="#scaling">Scaling</a></h3>
<ul>
<li>Scale by factors $a$ and $b$:
  $$\begin{bmatrix} a &amp; 0 \ 0 &amp; b \end{bmatrix}$$</li>
</ul>
<h3 id="shearing"><a class="toclink" href="#shearing">Shearing</a></h3>
<ul>
<li>Horizontal shear by $k$:
  $$\begin{bmatrix} 1 &amp; k \ 0 &amp; 1 \end{bmatrix}$$</li>
<li>Vertical shear by $k$:
  $$\begin{bmatrix} 1 &amp; 0 \ k &amp; 1 \end{bmatrix}$$</li>
</ul>
<h1 id="inner-product-spaces"><a class="toclink" href="#inner-product-spaces">Inner Product Spaces</a></h1>
<h2 id="definition_3"><a class="toclink" href="#definition_3">Definition</a></h2>
<p>An inner product on a vector space $V$ is a function $\langle \cdot,\cdot \rangle: V \times V \to \mathbb{R}$ (or $\mathbb{C}$)</p>
<h2 id="properties_7"><a class="toclink" href="#properties_7">Properties</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>Mathematical Form</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive Definiteness</td>
<td>$\langle x,x \rangle \geq 0$ and $\langle x,x \rangle = 0 \iff x = 0$</td>
<td>Always non-negative, zero only for zero vector</td>
</tr>
<tr>
<td>Symmetry</td>
<td>$\langle x,y \rangle = \overline{\langle y,x \rangle}$</td>
<td>Complex conjugate for complex spaces</td>
</tr>
<tr>
<td>Linearity</td>
<td>$\langle ax+by,z \rangle = a\langle x,z \rangle + b\langle y,z \rangle$</td>
<td>Linear in first argument</td>
</tr>
</tbody>
</table>
<h2 id="norm"><a class="toclink" href="#norm">Norm</a></h2>
<p>The norm induced by inner product: $|x| = \sqrt{\langle x,x \rangle}$</p>
<h3 id="properties_8"><a class="toclink" href="#properties_8">Properties</a></h3>
<ul>
<li>Non-negative: $|x| \geq 0$</li>
<li>Positive definite: $|x| = 0 \iff x = 0$</li>
<li>Homogeneous: $|cx| = |c||x|$</li>
<li>Triangle inequality: $|x + y| \leq |x| + |y|$</li>
</ul>
<h3 id="distance-function"><a class="toclink" href="#distance-function">Distance Function</a></h3>
<p>$$d(x,y) = |x-y| = \sqrt{\langle x-y,x-y \rangle}$$</p>
<h2 id="orthogonality"><a class="toclink" href="#orthogonality">Orthogonality</a></h2>
<h3 id="orthogonal-vectors"><a class="toclink" href="#orthogonal-vectors">Orthogonal Vectors</a></h3>
<p>Two vectors $x,y$ are orthogonal if $\langle x,y \rangle = 0$</p>
<h3 id="orthogonal-sets"><a class="toclink" href="#orthogonal-sets">Orthogonal Sets</a></h3>
<ul>
<li>Set of vectors where each pair is orthogonal</li>
<li>If normalized, called orthonormal set</li>
<li>Orthonormal basis: orthonormal set that spans space</li>
</ul>
<h3 id="orthogonal-matrices"><a class="toclink" href="#orthogonal-matrices">Orthogonal Matrices</a></h3>
<p>Matrix $Q$ is orthogonal if $Q^TQ = QQ^T = I$</p>
<h4 id="properties_9"><a class="toclink" href="#properties_9">Properties</a></h4>
<table>
<thead>
<tr>
<th>Property</th>
<th>Formula</th>
<th>Note</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inverse</td>
<td>$Q^{-1} = Q^T$</td>
<td>Transpose equals inverse</td>
</tr>
<tr>
<td>Determinant</td>
<td>$\det(Q) = \pm 1$</td>
<td>Always unit magnitude</td>
</tr>
<tr>
<td>Column/Rows</td>
<td>$\langle q_i,q_j \rangle = \delta_{ij}$</td>
<td>Form orthonormal set</td>
</tr>
<tr>
<td>Length Preservation</td>
<td>$|Qx| = |x|$</td>
<td>Preserves distances</td>
</tr>
</tbody>
</table>
<h3 id="orthogonal-complements"><a class="toclink" href="#orthogonal-complements">Orthogonal Complements</a></h3>
<p>For subspace $W$, orthogonal complement $W^⊥$:
$$W^⊥ = {x \in V : \langle x,w \rangle = 0 \text{ for all } w \in W}$$</p>
<h3 id="orthogonal-projections"><a class="toclink" href="#orthogonal-projections">Orthogonal Projections</a></h3>
<p>Projection onto subspace $W$:
$$\text{proj}<em i="1">W(x) = \sum</em>w_i$$
where ${w_1,\ldots,w_k}$ is basis for $W$}^k \frac{\langle x,w_i \rangle}{|w_i|^2</p>
<p>For orthonormal basis:
$$\text{proj}<em i="1">W(x) = \sum</em>^k \langle x,w_i \rangle w_i$$</p>
<h1 id="special-matrices-cont-1"><a class="toclink" href="#special-matrices-cont-1">Special Matrices (cont. 1)</a></h1>
<table>
<thead>
<tr>
<th>Type</th>
<th>Definition</th>
<th>Properties</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Symmetric</td>
<td>$A = A^T$</td>
<td>- Diagonal elements can be any real number<br>- Elements symmetric across main diagonal<br>- All eigenvalues are real</td>
<td>$$\begin{bmatrix} 1 &amp; 2 &amp; 3\ 2 &amp; 4 &amp; 5\ 3 &amp; 5 &amp; 6 \end{bmatrix}$$</td>
</tr>
<tr>
<td>Skew-symmetric</td>
<td>$A = -A^T$</td>
<td>- Diagonal elements must be zero<br>- $a_{ij} = -a_{ji}$<br>- All eigenvalues are imaginary or zero</td>
<td>$$\begin{bmatrix} 0 &amp; 2 &amp; -1\ -2 &amp; 0 &amp; 3\ 1 &amp; -3 &amp; 0 \end{bmatrix}$$</td>
</tr>
<tr>
<td>Orthogonal</td>
<td>$AA^T = A^TA = I$</td>
<td>- $A^{-1} = A^T$<br>- Columns/rows form orthonormal basis<br>- $\det(A) = \pm 1$<br>- Preserves lengths and angles</td>
<td>$$\begin{bmatrix} \cos\theta &amp; -\sin\theta\ \sin\theta &amp; \cos\theta \end{bmatrix}$$</td>
</tr>
<tr>
<td>Idempotent</td>
<td>$A^2 = A$</td>
<td>- Eigenvalues are only 0 or 1<br>- Trace = rank<br>- Used in projection matrices</td>
<td>$$\begin{bmatrix} 1 &amp; 0\ 0 &amp; 0 \end{bmatrix}$$</td>
</tr>
<tr>
<td>Nilpotent</td>
<td>$A^k = 0$ for some $k$</td>
<td>- All eigenvalues = 0<br>- Trace = 0<br>- $k \leq n$ where $n$ is matrix size</td>
<td>$$\begin{bmatrix} 0 &amp; 1\ 0 &amp; 0 \end{bmatrix}$$</td>
</tr>
</tbody>
</table>
<hr />
<h3 id="additional-properties"><a class="toclink" href="#additional-properties">Additional Properties:</a></h3>
<ol>
<li>
<p><strong>Symmetric Matrices</strong>:</p>
</li>
<li>
<p>All real symmetric matrices are diagonalizable</p>
</li>
<li>
<p>$x^TAx$ is a quadratic form</p>
</li>
<li>
<p><strong>Orthogonal Matrices</strong>:</p>
</li>
<li>
<p>Every column/row has unit length</p>
</li>
<li>Any two columns/rows are perpendicular</li>
<li>
<p>Preserves inner products: $(Ax)^T(Ay) = x^Ty$</p>
</li>
<li>
<p><strong>Idempotent Matrices</strong>:</p>
</li>
<li>
<p>$I - A$ is also idempotent if $A$ is idempotent</p>
</li>
<li>
<p>Rank = Trace for idempotent matrices</p>
</li>
<li>
<p><strong>Nilpotent Matrices</strong>:</p>
</li>
<li>The minimal $k$ for which $A^k = 0$ is called the index of nilpotency</li>
<li>Characteristic polynomial is $\lambda^n$</li>
</ol>
<h1 id="change-of-basis"><a class="toclink" href="#change-of-basis">Change of Basis</a></h1>
<h2 id="transition-matrices"><a class="toclink" href="#transition-matrices">Transition Matrices</a></h2>
<p>A transition matrix $P$ transforms coordinates from one basis to another:
$$P_{B←C} = [v_1 \; v_2 \; \cdots \; v_n]$$
where $v_i$ are the basis vectors of B expressed in basis C</p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>Formula</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Change from C to B</td>
<td>$[v]<em B_C="B←C">B = P</em>[v]_C$</td>
<td>Vector v in basis B</td>
</tr>
<tr>
<td>Change from B to C</td>
<td>$[v]<em C_B="C←B">C = P</em>[v]_B$</td>
<td>Vector v in basis C</td>
</tr>
<tr>
<td>Inverse relation</td>
<td>$P_{C←B} = P_{B←C}^{-1}$</td>
<td>Matrices are inverses</td>
</tr>
</tbody>
</table>
<h2 id="similar-matrices"><a class="toclink" href="#similar-matrices">Similar Matrices</a></h2>
<p>Two matrices A and B are similar if:
$$B = P^{-1}AP$$
where P is an invertible matrix</p>
<p>Properties:</p>
<ul>
<li>Similar matrices have same eigenvalues</li>
<li>Similar matrices have same determinant</li>
<li>Similar matrices have same trace</li>
<li>Similar matrices have same rank</li>
</ul>
<h2 id="coordinate-vectors"><a class="toclink" href="#coordinate-vectors">Coordinate Vectors</a></h2>
<p>For a vector $v$ and basis $B = {b_1, b_2, ..., b_n}$:
$$[v]_B = \begin{bmatrix} c_1 \ c_2 \ \vdots \ c_n \end{bmatrix}$$
where $v = c_1b_1 + c_2b_2 + ... + c_nb_n$</p>
<h2 id="orthogonalization"><a class="toclink" href="#orthogonalization">Orthogonalization</a></h2>
<h3 id="gram-schmidt-process"><a class="toclink" href="#gram-schmidt-process">Gram-Schmidt Process</a></h3>
<p>Converting basis ${v_1, v_2, ..., v_n}$ to orthogonal basis ${u_1, u_2, ..., u_n}$:</p>
<ol>
<li>$u_1 = v_1$</li>
<li>$u_2 = v_2 - \text{proj}_{u_1}(v_2)$</li>
<li>$u_3 = v_3 - \text{proj}<em u_2="u_2">{u_1}(v_3) - \text{proj}</em>(v_3)$</li>
</ol>
<p>General formula:
$$u_k = v_k - \sum_{i=1}^{k-1} \text{proj}_{u_i}(v_k)$$
where $\text{proj}_u(v) = \frac{\langle v,u \rangle}{\langle u,u \rangle}u$</p>
<h3 id="qr-decomposition"><a class="toclink" href="#qr-decomposition">QR Decomposition</a></h3>
<p>Matrix A can be decomposed as:
$$A = QR$$
where:</p>
<ul>
<li>Q is orthogonal matrix ($Q^TQ = I$)</li>
<li>R is upper triangular matrix</li>
</ul>
<h2 id="orthonormal-basis"><a class="toclink" href="#orthonormal-basis">Orthonormal Basis</a></h2>
<h3 id="construction"><a class="toclink" href="#construction">Construction</a></h3>
<ol>
<li>Start with any basis</li>
<li>Apply Gram-Schmidt process</li>
<li>Normalize each vector: $e_i = \frac{u_i}{|u_i|}$</li>
</ol>
<h3 id="properties_10"><a class="toclink" href="#properties_10">Properties</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Orthogonality</td>
<td>$\langle e_i,e_j \rangle = 0$ for $i \neq j$</td>
</tr>
<tr>
<td>Normality</td>
<td>$|e_i| = 1$ for all i</td>
</tr>
<tr>
<td>Transition Matrix</td>
<td>P is orthogonal ($P^T = P^{-1}$)</td>
</tr>
<tr>
<td>Coordinates</td>
<td>$[v]_B = [\langle v,e_1 \rangle \; \langle v,e_2 \rangle \; \cdots \; \langle v,e_n \rangle]^T$</td>
</tr>
</tbody>
</table>
<h1 id="eigenvalues-and-eigenvectors"><a class="toclink" href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a></h1>
<h2 id="definitions"><a class="toclink" href="#definitions">Definitions</a></h2>
<ul>
<li><strong>Eigenvalue</strong> ($\lambda$): A scalar value where $A\vec{v} = \lambda\vec{v}$ for some nonzero vector $\vec{v}$</li>
<li><strong>Eigenvector</strong> ($\vec{v}$): A nonzero vector satisfying $A\vec{v} = \lambda\vec{v}$ for some scalar $\lambda$</li>
</ul>
<h2 id="characteristic-equation"><a class="toclink" href="#characteristic-equation">Characteristic Equation</a></h2>
<p>$$\det(A - \lambda I) = 0$$
where:</p>
<ul>
<li>$A$ is the square matrix</li>
<li>$\lambda$ is the eigenvalue</li>
<li>$I$ is the identity matrix</li>
</ul>
<h3 id="characteristic-polynomial"><a class="toclink" href="#characteristic-polynomial">Characteristic Polynomial</a></h3>
<p>$$p(\lambda) = \det(A - \lambda I)$$</p>
<ul>
<li>Degree equals matrix dimension</li>
<li>Roots are eigenvalues</li>
</ul>
<h2 id="properties_11"><a class="toclink" href="#properties_11">Properties</a></h2>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sum</td>
<td>$\sum \lambda_i = \text{trace}(A)$</td>
</tr>
<tr>
<td>Product</td>
<td>$\prod \lambda_i = \det(A)$</td>
</tr>
<tr>
<td>Number</td>
<td>≤ dimension of matrix</td>
</tr>
</tbody>
</table>
<h3 id="geometric-multiplicity"><a class="toclink" href="#geometric-multiplicity">Geometric Multiplicity</a></h3>
<ul>
<li>Number of linearly independent eigenvectors for a given eigenvalue</li>
<li>≤ algebraic multiplicity</li>
<li>= dimension of null space of $(A - \lambda I)$</li>
</ul>
<h3 id="algebraic-multiplicity"><a class="toclink" href="#algebraic-multiplicity">Algebraic Multiplicity</a></h3>
<ul>
<li>Multiplicity of eigenvalue as root of characteristic polynomial</li>
<li>≥ geometric multiplicity</li>
</ul>
<h3 id="eigenspace"><a class="toclink" href="#eigenspace">Eigenspace</a></h3>
<p>$$E_\lambda = \text{null}(A - \lambda I)$$</p>
<ul>
<li>Subspace spanned by eigenvectors for eigenvalue $\lambda$</li>
<li>Dimension = geometric multiplicity</li>
</ul>
<h2 id="diagonalization"><a class="toclink" href="#diagonalization">Diagonalization</a></h2>
<p>$A = PDP^{-1}$ where:</p>
<ul>
<li>$D$ is diagonal matrix of eigenvalues</li>
<li>$P$ is matrix of eigenvectors</li>
</ul>
<h3 id="diagonalizability-conditions"><a class="toclink" href="#diagonalizability-conditions">Diagonalizability Conditions</a></h3>
<p>Matrix $A$ is diagonalizable if and only if:</p>
<ol>
<li>Geometric multiplicity = algebraic multiplicity for all eigenvalues</li>
<li>Sum of dimensions of eigenspaces = matrix dimension</li>
</ol>
<h3 id="diagonalization-process"><a class="toclink" href="#diagonalization-process">Diagonalization Process</a></h3>
<ol>
<li>Find eigenvalues (solve characteristic equation)</li>
<li>Find eigenvectors for each eigenvalue</li>
<li>Form $P$ from eigenvectors as columns</li>
<li>Form $D$ with eigenvalues on diagonal</li>
<li>Verify $A = PDP^{-1}$</li>
</ol>
<h3 id="similar-matrices_1"><a class="toclink" href="#similar-matrices_1">Similar Matrices</a></h3>
<ul>
<li>$A$ and $B$ similar if $B = P^{-1}AP$ for invertible $P$</li>
<li>Similar matrices have same eigenvalues</li>
<li>Similar matrices have same characteristic polynomial</li>
</ul>
<h2 id="special-cases"><a class="toclink" href="#special-cases">Special Cases</a></h2>
<h3 id="repeated-eigenvalues"><a class="toclink" href="#repeated-eigenvalues">Repeated Eigenvalues</a></h3>
<ul>
<li>May have fewer linearly independent eigenvectors</li>
<li>Need generalized eigenvectors if geometric multiplicity &lt; algebraic multiplicity</li>
</ul>
<h3 id="complex-eigenvalues"><a class="toclink" href="#complex-eigenvalues">Complex Eigenvalues</a></h3>
<ul>
<li>Occur in conjugate pairs for real matrices</li>
<li>Complex eigenvectors also occur in conjugate pairs</li>
</ul>
<h2 id="applications"><a class="toclink" href="#applications">Applications</a></h2>
<h3 id="powers-of-matrices"><a class="toclink" href="#powers-of-matrices">Powers of Matrices</a></h3>
<p>$$A^n = PD^nP^{-1}$$</p>
<ul>
<li>Simplifies computation of matrix powers</li>
<li>Useful for recursive sequences</li>
</ul>
<h3 id="difference-equations"><a class="toclink" href="#difference-equations">Difference Equations</a></h3>
<p>For system $\vec{x}_{k+1} = A\vec{x}_k$:</p>
<ul>
<li>General solution involves eigenvalues and eigenvectors</li>
<li>Stability determined by $|\lambda| &lt; 1$</li>
</ul>
<h3 id="differential-equations"><a class="toclink" href="#differential-equations">Differential Equations</a></h3>
<p>For system $\frac{d\vec{x}}{dt} = A\vec{x}$:</p>
<ul>
<li>Solutions of form $\vec{x}(t) = ce^{\lambda t}\vec{v}$</li>
<li>Stability determined by $\text{Re}(\lambda) &lt; 0$</li>
</ul>
<hr />
<h1 id="important-theorems"><a class="toclink" href="#important-theorems">Important Theorems</a></h1>
<h2 id="fundamental-theorem-of-linear-algebra"><a class="toclink" href="#fundamental-theorem-of-linear-algebra">Fundamental Theorem of Linear Algebra</a></h2>
<p>For a linear transformation $T: V \rightarrow W$ and its matrix $A$:</p>
<ul>
<li>$\text{Null}(A) \oplus \text{Row}(A^T) = \mathbb{R}^n$</li>
<li>$\text{Col}(A) \oplus \text{Null}(A^T) = \mathbb{R}^m$</li>
<li>$\dim(\text{Null}(A)) + \dim(\text{Col}(A)) = n$</li>
<li>$\text{rank}(A) + \text{nullity}(A) = n$</li>
</ul>
<h2 id="cayley-hamilton-theorem"><a class="toclink" href="#cayley-hamilton-theorem">Cayley-Hamilton Theorem</a></h2>
<p>Every square matrix $A$ satisfies its own characteristic equation:
$$p(A) = 0 \text{ where } p(\lambda) = \det(A - \lambda I)$$</p>
<h2 id="spectral-theorem"><a class="toclink" href="#spectral-theorem">Spectral Theorem</a></h2>
<p>For symmetric matrices $A \in \mathbb{R}^{n \times n}$:</p>
<ul>
<li>All eigenvalues are real</li>
<li>Eigenvectors of distinct eigenvalues are orthogonal</li>
<li>$A = PDP^T$ where:</li>
<li>$P$ is orthogonal ($P^TP = I$)</li>
<li>$D$ is diagonal containing eigenvalues</li>
</ul>
<h2 id="triangle-inequality_1"><a class="toclink" href="#triangle-inequality_1">Triangle Inequality</a></h2>
<p>For vectors $x, y$:
$$|x + y| \leq |x| + |y|$$</p>
<h2 id="cauchy-schwarz-inequality"><a class="toclink" href="#cauchy-schwarz-inequality">Cauchy-Schwarz Inequality</a></h2>
<p>For vectors $x, y$:
$$|x^Ty| \leq |x||y|$$</p>
<ul>
<li>Equality holds if and only if vectors are linearly dependent</li>
</ul>
<h2 id="invertible-matrix-theorem"><a class="toclink" href="#invertible-matrix-theorem">Invertible Matrix Theorem</a></h2>
<p>The following statements are equivalent for a square matrix $A$:</p>
<ol>
<li>$A$ is invertible</li>
<li>$\det(A) \neq 0$</li>
<li>$\text{rank}(A) = n$</li>
<li>$\text{Null}(A) = {0}$</li>
<li>Columns are linearly independent</li>
<li>$Ax = b$ has unique solution for all $b$</li>
</ol>
<h2 id="matrix-factorization-theorems"><a class="toclink" href="#matrix-factorization-theorems">Matrix Factorization Theorems</a></h2>
<h3 id="lu-decomposition"><a class="toclink" href="#lu-decomposition">LU Decomposition</a></h3>
<p>For matrix $A$:
$$A = LU$$
where:</p>
<ul>
<li>$L$ is lower triangular</li>
<li>$U$ is upper triangular</li>
<li>Exists if all leading principal minors are nonzero</li>
</ul>
<h3 id="qr-decomposition_1"><a class="toclink" href="#qr-decomposition_1">QR Decomposition</a></h3>
<p>For matrix $A$:
$$A = QR$$
where:</p>
<ul>
<li>$Q$ is orthogonal ($Q^TQ = I$)</li>
<li>$R$ is upper triangular</li>
<li>Always exists for full-rank matrices</li>
</ul>
<h3 id="cholesky-decomposition"><a class="toclink" href="#cholesky-decomposition">Cholesky Decomposition</a></h3>
<p>For symmetric positive definite matrix $A$:
$$A = LL^T$$
where:</p>
<ul>
<li>$L$ is lower triangular</li>
<li>$L^T$ is upper triangular</li>
<li>Elements are real</li>
</ul>
<h1 id="advanced-topics"><a class="toclink" href="#advanced-topics">Advanced Topics</a></h1>
<h2 id="singular-value-decomposition-svd"><a class="toclink" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></h2>
<p>Any matrix $A \in \mathbb{R}^{m \times n}$ can be decomposed as $A = U\Sigma V^T$</p>
<h3 id="singular-values"><a class="toclink" href="#singular-values">Singular Values</a></h3>
<ul>
<li>Diagonal entries $\sigma_i$ of $\Sigma$ matrix</li>
<li>$\sigma_i = \sqrt{\lambda_i(A^TA)}$</li>
<li>Always real and non-negative</li>
<li>Ordered: $\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_n \geq 0$</li>
</ul>
<h3 id="u-and-v-matrices"><a class="toclink" href="#u-and-v-matrices">U and V Matrices</a></h3>
<ul>
<li>$U$: $m \times m$ orthogonal matrix (left singular vectors)</li>
<li>$V$: $n \times n$ orthogonal matrix (right singular vectors)</li>
<li>$\Sigma$: $m \times n$ diagonal matrix with singular values</li>
</ul>
<h3 id="applications_1"><a class="toclink" href="#applications_1">Applications</a></h3>
<table>
<thead>
<tr>
<th>Application</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low-rank approximation</td>
<td>Truncate to $k$ largest singular values</td>
</tr>
<tr>
<td>Image compression</td>
<td>Reduce dimensionality while preserving structure</td>
</tr>
<tr>
<td>Principal Component Analysis</td>
<td>Use right singular vectors as principal components</td>
</tr>
<tr>
<td>Pseudoinverse</td>
<td>$A^+ = V\Sigma^+U^T$</td>
</tr>
</tbody>
</table>
<h2 id="jordan-canonical-form"><a class="toclink" href="#jordan-canonical-form">Jordan Canonical Form</a></h2>
<p>For matrix $A$, exists invertible $P$ such that $P^{-1}AP = J$</p>
<h3 id="jordan-blocks"><a class="toclink" href="#jordan-blocks">Jordan Blocks</a></h3>
<p>$$
J_k(\lambda) = \begin{bmatrix}
\lambda &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \
0 &amp; \lambda &amp; 1 &amp; \cdots &amp; 0 \
\vdots &amp; \vdots &amp; \ddots &amp; \ddots &amp; \vdots \
0 &amp; 0 &amp; \cdots &amp; \lambda &amp; 1 \
0 &amp; 0 &amp; \cdots &amp; 0 &amp; \lambda
\end{bmatrix}
$$</p>
<h3 id="generalized-eigenvectors"><a class="toclink" href="#generalized-eigenvectors">Generalized Eigenvectors</a></h3>
<ul>
<li>Chain: $(A-\lambda I)^kv_k = 0$</li>
<li>$v_1$ is regular eigenvector</li>
<li>$(A-\lambda I)v_{i+1} = v_i$</li>
</ul>
<h2 id="positive-definite-matrices"><a class="toclink" href="#positive-definite-matrices">Positive Definite Matrices</a></h2>
<p>Symmetric matrix $A$ where $x^TAx &gt; 0$ for all nonzero $x$</p>
<h3 id="tests-for-positive-definiteness"><a class="toclink" href="#tests-for-positive-definiteness">Tests for Positive Definiteness</a></h3>
<table>
<thead>
<tr>
<th>Test</th>
<th>Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Eigenvalue</td>
<td>All eigenvalues &gt; 0</td>
</tr>
<tr>
<td>Leading Principals</td>
<td>All leading principal minors &gt; 0</td>
</tr>
<tr>
<td>Cholesky</td>
<td>Exists unique lower triangular $L$ with $A = LL^T$</td>
</tr>
<tr>
<td>Quadratic Form</td>
<td>$x^TAx &gt; 0$ for all nonzero $x$</td>
</tr>
</tbody>
</table>
<h3 id="applications_2"><a class="toclink" href="#applications_2">Applications</a></h3>
<ul>
<li>Optimization problems</li>
<li>Covariance matrices</li>
<li>Least squares solutions</li>
<li>Energy functions</li>
</ul>
<h2 id="linear-operators"><a class="toclink" href="#linear-operators">Linear Operators</a></h2>
<p>Maps between vector spaces preserving linear structure</p>
<h3 id="adjoint-operators"><a class="toclink" href="#adjoint-operators">Adjoint Operators</a></h3>
<ul>
<li>For operator $T$, adjoint $T^<em>$ satisfies $\langle Tx,y \rangle = \langle x,T^</em>y \rangle$</li>
<li>Matrix representation: conjugate transpose</li>
<li>Properties:</li>
<li>$(T^<em>)^</em> = T$</li>
<li>$(ST)^<em> = T^</em>S^*$</li>
<li>$(\alpha T)^<em> = \overline{\alpha}T^</em>$</li>
</ul>
<h3 id="self-adjoint-operators"><a class="toclink" href="#self-adjoint-operators">Self-Adjoint Operators</a></h3>
<ul>
<li>$T = T^*$</li>
<li>Real eigenvalues</li>
<li>Orthogonal eigenvectors</li>
<li>Spectral theorem applies</li>
</ul>
<h2 id="dual-spaces"><a class="toclink" href="#dual-spaces">Dual Spaces</a></h2>
<p>Vector space $V^*$ of linear functionals on $V$</p>
<h3 id="dual-basis"><a class="toclink" href="#dual-basis">Dual Basis</a></h3>
<ul>
<li>For basis ${e_i}$ of $V$, dual basis ${e_i^<em>}$ satisfies $e_i^</em>(e_j) = \delta_{ij}$</li>
<li>Dimension equals original space</li>
<li>Natural pairing: $\langle f,v \rangle = f(v)$</li>
</ul>
<h3 id="dual-maps"><a class="toclink" href="#dual-maps">Dual Maps</a></h3>
<p>For linear map $T: V \to W$, dual map $T^<em>: W^</em> \to V^<em>$
$$\langle T^</em>f,v \rangle = \langle f,Tv \rangle$$</p>
<h2 id="tensor-products"><a class="toclink" href="#tensor-products">Tensor Products</a></h2>
<p>$V \otimes W$ is universal space for bilinear maps</p>
<h3 id="definition_4"><a class="toclink" href="#definition_4">Definition</a></h3>
<ul>
<li>Elementary tensors: $v \otimes w$</li>
<li>Bilinear in components:</li>
<li>$(av_1 + bv_2) \otimes w = av_1 \otimes w + bv_2 \otimes w$</li>
<li>$v \otimes (aw_1 + bw_2) = av \otimes w_1 + bv \otimes w_2$</li>
</ul>
<h3 id="properties_12"><a class="toclink" href="#properties_12">Properties</a></h3>
<table>
<thead>
<tr>
<th>Property</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dimension</td>
<td>$\dim(V \otimes W) = \dim(V)\dim(W)$</td>
</tr>
<tr>
<td>Associativity</td>
<td>$(U \otimes V) \otimes W \cong U \otimes (V \otimes W)$</td>
</tr>
<tr>
<td>Distributivity</td>
<td>$(U \oplus V) \otimes W \cong (U \otimes W) \oplus (V \otimes W)$</td>
</tr>
<tr>
<td>Field multiplication</td>
<td>$(\alpha v) \otimes w = v \otimes (\alpha w) = \alpha(v \otimes w)$</td>
</tr>
</tbody>
</table>
<h2 id="multilinear-algebra"><a class="toclink" href="#multilinear-algebra">Multilinear Algebra</a></h2>
<h3 id="tensors"><a class="toclink" href="#tensors">Tensors</a></h3>
<ul>
<li>Multilinear maps: $T: V_1 \times ... \times V_k \to W$</li>
<li>Type $(r,s)$: $r$ contravariant, $s$ covariant indices</li>
<li>Transformation rules under basis change</li>
</ul>
<h3 id="exterior-algebra"><a class="toclink" href="#exterior-algebra">Exterior Algebra</a></h3>
<ul>
<li>Antisymmetric tensors</li>
<li>Wedge product $\wedge$</li>
<li>Properties:</li>
<li>Anticommutativity: $v \wedge w = -w \wedge v$</li>
<li>Associativity: $(u \wedge v) \wedge w = u \wedge (v \wedge w)$</li>
<li>Distributivity over addition</li>
<li>$k$-forms and differential forms</li>
</ul>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/dual%20spaces.html">dual spaces</a>
                <a href="/tags/linear%20algebra.html">linear algebra</a>
                <a href="/tags/operators.html">operators</a>
                <a href="/tags/tensor%20products.html">tensor products</a>
                <a href="/tags/vector%20spaces.html">vector spaces</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
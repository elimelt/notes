
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPU Architecture and Programming | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Overview of GPU architecture and context behind GPUs for LLM serving systems">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/gpu-basics.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="GPU Architecture and Programming">
    <meta property="og:description" content="Overview of GPU architecture and context behind GPUs for LLM serving systems">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/gpu-basics.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="GPU Architecture and Programming">
    <meta name="twitter:description" content="Overview of GPU architecture and context behind GPUs for LLM serving systems">

    <meta name="keywords" content="gpu,architecture,programming,cuda,nvidia,pytorch,triton">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "GPU Architecture and Programming", "dateModified": "2025-05-25T16:54:03.584744", "description": "Overview of GPU architecture and context behind GPUs for LLM serving systems", "articleSection": "Machine Learning Systems", "keywords": "gpu,architecture,programming,cuda,nvidia,pytorch,triton"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a>
<a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » GPU Architecture and Programming
        </div>
    </header>
    <main role="main">
        <article>
            <h1>GPU Architecture and Programming</h1>
            <div class="meta">
                <time datetime="2025-05-25T16:54:03.584744">
                    Last modified: 2025-05-25
                </time>
                <span>Category: <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="gpu-architecture-and-introduction-to-gpu-programming">GPU Architecture and Introduction to GPU Programming</h1>
<blockquote>
<p>Disclaimer: These are notes for CSE 599K "LLM Serving Systems" at the University of Washington, Spring 2025 instructed by both Prof. Baris Kasikci and TA Kan Zhu</p>
</blockquote>
<hr />
<h3 id="training-vs-inference">Training vs Inference</h3>
<ul>
<li><strong>Training</strong>: Learning from existing data</li>
<li><strong>Inference</strong>: Applying capability to new data</li>
</ul>
<h3 id="serving">Serving</h3>
<p><strong>ML model serving</strong> is about building a system to efficiently and scalably perform inference with:
- High throughput
- Low latency
- Compliance with diverse Service Level Objectives (SLOs)</p>
<h2 id="llm-applications-and-market-context">LLM Applications and Market Context</h2>
<h3 id="applications-enabled-by-llms">Applications Enabled by LLMs</h3>
<ul>
<li><strong>AI Assistants</strong> (ChatGPT, Google Bard)</li>
<li><strong>Text-to-Image</strong> (DALLcdotE, MidJourney)</li>
<li><strong>Creative Writing</strong> (Jasper, Copy.ai)</li>
<li><strong>AI Coding Tools</strong> (GitHub Copilot, Replit)</li>
<li><strong>Text-to-Speech &amp; Audio</strong> (Descript, Synthesia)</li>
</ul>
<p><strong>Key principle</strong>: Batching user requests and aiming for optimal throughput are key</p>
<h3 id="market-demand">Market Demand</h3>
<ul>
<li>ChatGPT monthly visits grew from ~500M (Dec 2022) to ~2000M (Jan 2024)</li>
<li>Users frequently encounter "We're experiencing exceptionally high demand" messages</li>
</ul>
<h3 id="infrastructure-costs">Infrastructure Costs</h3>
<p><strong>Large-scale H100 investments in 2024:</strong>
- Meta: 300K units
- Google: 150K units
- Microsoft: 150K units
- X: 85K units</p>
<p><strong>NVIDIA H200 HGX Server specs:</strong>
- Cost: ~$250,000
- High operating cost: Up to ~10,000W
- Long lead time</p>
<h2 id="gpu-fundamentals">GPU Fundamentals</h2>
<h3 id="what-is-a-gpu">What is a GPU?</h3>
<ul>
<li><strong>Graphics Processing Unit</strong></li>
<li>Originally designed for accelerated graphics rendering</li>
<li>Now handles scientific computing and machine learning</li>
<li>Come with software stacks: CUDA (NVIDIA), ROCm (AMD)</li>
</ul>
<h3 id="cpu-vs-gpu-architecture">CPU vs GPU Architecture</h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>CPU</th>
<th>GPU</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Design Focus</strong></td>
<td>Control logic (good with branching)</td>
<td>Computation/loading</td>
</tr>
<tr>
<td><strong>Performance</strong></td>
<td>Single thread performance</td>
<td>Parallel processing</td>
</tr>
<tr>
<td><strong>Cores</strong></td>
<td>Few powerful cores</td>
<td>Many simpler cores</td>
</tr>
<tr>
<td><strong>Memory</strong></td>
<td>Large cache hierarchy</td>
<td>High bandwidth memory</td>
</tr>
</tbody>
</table>
<h3 id="example-specifications">Example Specifications</h3>
<table>
<thead>
<tr>
<th>Specification</th>
<th>AMD EPYC 9555 (CPU)</th>
<th>NVIDIA H200 (GPU)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Cores/Threads</strong></td>
<td>64 Cores / 128 Threads</td>
<td>16,896 CUDA Cores</td>
</tr>
<tr>
<td><strong>Frequency</strong></td>
<td>4.4 GHz</td>
<td>1.980 GHz</td>
</tr>
<tr>
<td><strong>TFLOPs</strong></td>
<td>~10-20 TFLOPs</td>
<td>989 TFLOPs</td>
</tr>
<tr>
<td><strong>Memory Size</strong></td>
<td>Up to 6 TB</td>
<td>144GB</td>
</tr>
<tr>
<td><strong>Memory Bandwidth</strong></td>
<td>576 GB/s</td>
<td>4800 GB/s</td>
</tr>
<tr>
<td><strong>Memory Latency</strong></td>
<td>~70ns</td>
<td>~110ns</td>
</tr>
</tbody>
</table>
<p><strong>Key difference:</strong>
- <strong>CPU DRAM</strong>: Low latency random access
- <strong>GPU HBM</strong>: Higher bandwidth, structured batch access</p>
<hr />
<h2 id="gpu-hardware-architecture">GPU Hardware Architecture</h2>
<h3 id="data-center-context">Data Center Context</h3>
<ul>
<li>GPUs are deployed in server clusters</li>
<li>Connected via high-speed networks (NVLink: 900 GB/s)</li>
<li>Network connectivity: 200 Gb/s = 25 GB/s to data center network</li>
</ul>
<h3 id="gpu-memory-hierarchy">GPU Memory Hierarchy</h3>
<ul>
<li><strong>Global Memory (HBM)</strong>: 80 GB, 3TB/s bandwidth</li>
<li><strong>L2 Cache</strong>: 50MB, ~10TB/s bandwidth</li>
<li><strong>Shared Memory ("Smem")</strong>: 228 KB per SM, ~20TB/s bandwidth</li>
<li><strong>Registers</strong>: 64K    imes 32 Bit per SM, ~600TB/s bandwidth</li>
</ul>
<h3 id="streaming-multiprocessors-sms">Streaming Multiprocessors (SMs)</h3>
<p><strong>Components:</strong>
- <strong>CUDA Cores</strong>: Scalar computation
- <strong>Tensor Cores</strong>: Matrix (dense) computation
- <strong>Shared/Constant Memory</strong>: High bandwidth temp buffer</p>
<hr />
<h2 id="gpu-programming-model">GPU Programming Model</h2>
<h3 id="hierarchy-of-execution-units">Hierarchy of Execution Units</h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>Definition</th>
<th>Architecture</th>
<th>Communication</th>
<th>Limits</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Thread</strong></td>
<td>Minimal units that execute instructions</td>
<td>Function units</td>
<td>Local</td>
<td>Up to 255 registers</td>
</tr>
<tr>
<td><strong>Warp</strong></td>
<td>Group of Threads</td>
<td>"SM tiles"</td>
<td>Register File</td>
<td>32 threads</td>
</tr>
<tr>
<td><strong>Thread Blocks</strong></td>
<td>Group of Warps</td>
<td>SM</td>
<td>Shared Memory</td>
<td>Up to 32 warps (1024 threads)</td>
</tr>
<tr>
<td><strong>Kernel</strong></td>
<td>Function on GPU</td>
<td>GPU</td>
<td>L2/Global memory</td>
<td>Up to (2epsilonz-1)epsilon Blocks</td>
</tr>
</tbody>
</table>
<h3 id="key-concepts">Key Concepts</h3>
<ul>
<li><strong>32 threads form a warp</strong></li>
<li><strong>Threads in a warp run in parallel</strong> with:<ul>
<li>Same instructions</li>
<li>Same pace</li>
<li>Different data at register level</li>
</ul>
</li>
<li><strong>4 warps run on one SM simultaneously</strong></li>
<li><strong>Scheduler swaps warps on and off SM</strong></li>
<li><strong>Blocks operate independently</strong></li>
<li><strong>Block-Block communication via L2/Global memory</strong></li>
</ul>
<hr />
<h2 id="gpu-programming-approaches">GPU Programming Approaches</h2>
<h3 id="1-pytorch-easiest">1. PyTorch (Easiest)</h3>
<pre><code class="language-python">import torch

def add_tensors(a, b):
    return a + b

if __name__ == &quot;__main__&quot;:
    num_elements = 10**9

    # Create tensors on CPU
    tensor1 = torch.rand(num_elements, device='cpu')
    tensor2 = torch.rand(num_elements, device='cpu')

    # Move to GPU
    tensor1 = tensor1.to('cuda')
    tensor2 = tensor2.to('cuda')

    # Compute addition
    for i in range(10):
        result = add_tensors(tensor1, tensor2)

    # Move back to CPU
    result = result.cpu()
    print(&quot;Result of addition:&quot;, result)
</code></pre>
<h3 id="2-triton-intermediate">2. Triton (Intermediate)</h3>
<ul>
<li><strong>Compiler framework from OpenAI</strong></li>
<li><strong>Python interface with automated thread management</strong></li>
<li><strong>Higher performance than PyTorch for complex kernels</strong></li>
<li><strong>Operates at block level</strong></li>
</ul>
<pre><code class="language-python">@triton.jit
def add_kernel(x_ptr, y_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets &lt; n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y

    tl.store(output_ptr + offsets, output, mask=mask)
</code></pre>
<h3 id="3-cuda-most-control">3. CUDA (Most Control)</h3>
<ul>
<li><strong>Bare-bone, one-to-one mapping to hardware</strong></li>
<li><strong>Highest performance</strong></li>
<li><strong>Heavy implementation burden</strong></li>
</ul>
<h4 id="cuda-memory-management">CUDA Memory Management</h4>
<pre><code class="language-cpp">// Memory allocation
cudaMalloc          // device memory allocation  
cudaMallocHost      // pinned host memory allocation
cudaFree            // free memory

// Memory operations
cudaMemcpy          // synchronous copy
cudaMemcpyAsync     // asynchronous copy
cudaMemset          // synchronous set
cudaMemsetAsync     // asynchronous set
</code></pre>
<h4 id="cuda-kernel-structure">CUDA Kernel Structure</h4>
<pre><code class="language-cpp">// Kernel declaration
__global__ void kernel_name(args...)

// Device helper function  
__device__ T helper_name(args...)

// Example addition kernel
__global__ void add(int *a, int *b, int *c, size_t num) {
    int block_start = blockIdx.x * blockDim.x;
    int thread_id = threadIdx.x;
    int index = block_start + thread_id;
    if (index &lt; num) {
        c[index] = a[index] + b[index];
    }
}
</code></pre>
<h4 id="cuda-kernel-launch">CUDA Kernel Launch</h4>
<pre><code class="language-cpp">// Define block and thread dimensions
dim3 block(x, y, z);
dim3 thread(x, y, z);

// Launch kernel
kernel_name&lt;&lt;&lt;block, thread&gt;&gt;&gt;(args);
</code></pre>
<h4 id="cuda-synchronization">CUDA Synchronization</h4>
<pre><code class="language-cpp">__syncthreads()           // Thread synchronization (device function)
cudaDeviceSynchronize()   // Device synchronization (host function)

// Error checking
cudaGetLastError()        // Get last error
cudaGetErrorString()      // Get error description
</code></pre>
<hr />
<h2 id="performance-analysis">Performance Analysis</h2>
<h3 id="timing-considerations">Timing Considerations</h3>
<ul>
<li><strong>PyTorch dispatches kernels non-blocking</strong></li>
<li>CPU continues execution before GPU finishes</li>
<li><strong>Must use CUDA events for accurate GPU timing</strong></li>
</ul>
<h3 id="profiling-tools">Profiling Tools</h3>
<ul>
<li><strong>Torch.profiler</strong>: Good at showing CPU activity, slow processing</li>
<li><strong>Nsight Systems (nsys)</strong>: High performance system-level profiling</li>
<li><strong>Nsight Compute (ncu)</strong>: Tailored for intra-kernel profiling</li>
</ul>
<h3 id="cuda-streams">CUDA Streams</h3>
<ul>
<li><strong>Multiple streams may execute in parallel</strong></li>
<li>Depends on kernels and schedulers</li>
<li><strong>Use cudaEvents to synchronize between streams</strong></li>
<li>Events act as "flags" between kernels</li>
<li><code>cudaStreamWaitEvent</code> for synchronization</li>
</ul>
<hr />
<h2 id="modern-gpu-features">Modern GPU Features</h2>
<h3 id="advanced-cuda-features">Advanced CUDA Features</h3>
<ul>
<li><strong>Unified memory address</strong> (P100+)</li>
<li><strong>NvLink</strong> (P100+)</li>
<li><strong>Clusters</strong> (H100+)</li>
<li><strong>TMA (Tensor Memory Accelerator)</strong> (H100+)</li>
<li><strong>NVSHARP</strong> (H100+)</li>
<li><strong>FP4 and FP6 precision</strong> (B100+)</li>
</ul>
<hr />
<h2 id="summary">Summary</h2>
<h3 id="key-takeaways">Key Takeaways</h3>
<ol>
<li>
<p><strong>GPU Architecture Understanding</strong></p>
<ul>
<li>Parallel processing focus</li>
<li>SMs, blocks, threads hierarchy</li>
</ul>
</li>
<li>
<p><strong>Programming Approaches</strong></p>
<ul>
<li><strong>PyTorch</strong>: Easiest, high-level</li>
<li><strong>Triton</strong>: Balance of control and ease</li>
<li><strong>CUDA</strong>: Maximum control and performance</li>
</ul>
</li>
<li>
<p><strong>Performance Analysis</strong></p>
<ul>
<li>Proper timing with CUDA events</li>
<li>Profiling tools for optimization</li>
<li>Understanding memory hierarchy impact</li>
</ul>
</li>
</ol>
<p><strong>Core Principle</strong>: Batching user requests and aiming for optimal throughput are key to effective LLM serving systems.</p>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/architecture.html">architecture</a>
                <a href="/tags/cuda.html">cuda</a>
                <a href="/tags/gpu.html">gpu</a>
                <a href="/tags/nvidia.html">nvidia</a>
                <a href="/tags/programming.html">programming</a>
                <a href="/tags/pytorch.html">pytorch</a>
                <a href="/tags/triton.html">triton</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
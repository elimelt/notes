
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to write a fast kernel</title>
    <meta name="description" content="Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/how-to-write-a-fast-kernel.html">
    <link rel="stylesheet" href="/css/styles.css">
    <script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "How to write a fast kernel", "dateModified": "2025-05-25T16:32:00.802099", "description": "Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.", "url": "https://notes.elimelt.com/llm-serving-systems/how-to-write-a-fast-kernel.html", "articleSection": "Machine Learning Systems", "keywords": "cuda,gpu,pytorch,kernel"}</script>
</head>
<body>
    <header>
        <nav><a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a></nav>
        <div class="breadcrumbs"><a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » How to write a fast kernel</div>
    </header>
    <main class="content">
        <h1>How to write a fast kernel</h1>
        <h1 id="how-to-write-a-fast-kernel"><a class="toclink" href="#how-to-write-a-fast-kernel">How to write a fast kernel?</a></h1>
<h2 id="matrix-transpose-kernel"><a class="toclink" href="#matrix-transpose-kernel">Matrix Transpose Kernel</a></h2>
<h3 id="basic-way-with-torch"><a class="toclink" href="#basic-way-with-torch">Basic way with Torch</a></h3>
<pre><code class="language-py">import torch

num_rows = num_cols = 8192
a = torch.randn(num_rows, num_cols)

res = a.t().contiguous()

start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)

start.record()

for i in range(100):
    res = a.t().contiguous()

end.record()
torch.cuda.synchronize()

elapsed_time = start.elapsed_time(end)
time_per_iter = elapsed_time / 100

print(f&quot;Elapsed time: {elapsed_time} ms&quot;)
print(f&quot;Time per iteration: {time_per_iter} ms&quot;)
</code></pre>
<h3 id="how-can-we-optimize"><a class="toclink" href="#how-can-we-optimize">How can we optimize?</a></h3>
<p>Row-based partitioning in a CUDA kernel? But arrays can be very long. We can't load all the data into the shared memory. So, we need to partition the data into smaller chunks per-thread (since we have at most 1024 threads per block).</p>
<h3 id="cuda-kernel"><a class="toclink" href="#cuda-kernel">CUDA Kernel</a></h3>
<pre><code class="language-cpp">#include &lt;torch/extension.h&gt;
#include &lt;stdio.h&gt;

__global__ void transpose(float* input, float* output, int num_rows, int num_cols) {
    int row = blockIdx.x;
    int col_start = threadIdx.x * (num_cols / blockDim.x);
    int col_end = col_start + (num_cols / blockDim.x);

    for (int col = col_start; col &lt; col_end; ++col) {
        if (col &lt; num_cols) {
            output[col * num_rows + row] = input[row * num_cols + col];
        }
    }
}

</code></pre>
<h2 id="coallesced-memory-access"><a class="toclink" href="#coallesced-memory-access">Coallesced Memory Access</a></h2>
<ul>
<li>Inside one warp, if memory accesses are coalesced, then the memory access is fast because it can be batched</li>
<li>Data can then be retrieved in a single memory transaction</li>
</ul>
    </main>
    <footer>
        <p>&copy; 2025 Notes Site</p>
    </footer>
</body>
</html>
    
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>How to write a fast kernel | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/how-to-write-a-fast-kernel.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="How to write a fast kernel">
    <meta property="og:description" content="Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/how-to-write-a-fast-kernel.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="How to write a fast kernel">
    <meta name="twitter:description" content="Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.">

    <meta name="keywords" content="cuda,gpu,pytorch,kernel">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "How to write a fast kernel", "dateModified": "2025-08-09T18:34:41.052172", "description": "Techniques to write fast CUDA kernels, including coalesced memory access and shared memory usage.", "url": "https://notes.elimelt.com/llm-serving-systems/how-to-write-a-fast-kernel.html", "articleSection": "Machine Learning Systems", "keywords": "cuda,gpu,pytorch,kernel"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\\[", right: "\\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\\n").map((line) => {
                        if (line.endsWith("\\\\")) {
                            return line + "\\\\";
                        }
                        return line;
                    }).join("\\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a>
<a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » How to write a fast kernel
        </div>
    </header>
    <main role="main">
        <article>
            <h1>How to write a fast kernel</h1>
            <div class="meta">
                <time datetime="2025-08-09T18:34:41.052172">
                    Last modified: 2025-08-09
                </time>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="how-to-write-a-fast-kernel"><a class="toclink" href="#how-to-write-a-fast-kernel">How to write a fast kernel?</a></h1>
<h2 id="matrix-transpose-kernel"><a class="toclink" href="#matrix-transpose-kernel">Matrix Transpose Kernel</a></h2>
<h3 id="basic-way-with-torch"><a class="toclink" href="#basic-way-with-torch">Basic way with Torch</a></h3>
<pre><code class="language-py">import torch

num_rows = num_cols = 8192
a = torch.randn(num_rows, num_cols)

res = a.t().contiguous()

start = torch.cuda.Event(enable_timing=True)

end = torch.cuda.Event(enable_timing=True)

start.record()

for i in range(100):
    res = a.t().contiguous()

end.record()
torch.cuda.synchronize()

elapsed_time = start.elapsed_time(end)
time_per_iter = elapsed_time / 100

print(f&quot;Elapsed time: {elapsed_time} ms&quot;)
print(f&quot;Time per iteration: {time_per_iter} ms&quot;)
</code></pre>
<h3 id="how-can-we-optimize"><a class="toclink" href="#how-can-we-optimize">How can we optimize?</a></h3>
<p>Row-based partitioning in a CUDA kernel? But arrays can be very long. We can't load all the data into the shared memory. So, we need to partition the data into smaller chunks per-thread (since we have at most 1024 threads per block).</p>
<h3 id="cuda-kernel"><a class="toclink" href="#cuda-kernel">CUDA Kernel</a></h3>
<pre><code class="language-cpp">#include &lt;torch/extension.h&gt;
#include &lt;stdio.h&gt;

__global__ void transpose(float* input, float* output, int num_rows, int num_cols) {
    int row = blockIdx.x;
    int col_start = threadIdx.x * (num_cols / blockDim.x);
    int col_end = col_start + (num_cols / blockDim.x);

    for (int col = col_start; col &lt; col_end; ++col) {
        if (col &lt; num_cols) {
            output[col * num_rows + row] = input[row * num_cols + col];
        }
    }
}

</code></pre>
<h2 id="coallesced-memory-access"><a class="toclink" href="#coallesced-memory-access">Coallesced Memory Access</a></h2>
<ul>
<li>Inside one warp, if memory accesses are coalesced, then the memory access is fast because it can be batched</li>
<li>Data can then be retrieved in a single memory transaction</li>
</ul>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/cuda.html">cuda</a>
                <a href="/tags/gpu.html">gpu</a>
                <a href="/tags/kernel.html">kernel</a>
                <a href="/tags/pytorch.html">pytorch</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
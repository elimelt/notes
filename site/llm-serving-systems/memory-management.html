
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory Management in LLM Serving Systems</title>
    <meta name="description" content="Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache metho">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/memory-management.html">
    <link rel="stylesheet" href="/css/styles.css">
    <script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "Memory Management in LLM Serving Systems", "dateModified": "2025-05-25T17:28:34.089529", "description": "Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache metho", "url": "https://notes.elimelt.com/llm-serving-systems/memory-management.html", "articleSection": "Machine Learning Systems", "keywords": "memory management,kv cache,prefix sharing,paged attention,flash attention,machine learning"}</script>
</head>
<body>
    <header>
        <nav><a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a></nav>
        <div class="breadcrumbs"><a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » Memory Management in LLM Serving Systems</div>
    </header>
    <main class="content">
        <h1>Memory Management in LLM Serving Systems</h1>
        <h1 id="memory-management-in-llm-serving-systems"><a class="toclink" href="#memory-management-in-llm-serving-systems">Memory Management in LLM Serving Systems</a></h1>
<blockquote>
<p>Disclaimer: These are notes for CSE 599K "LLM Serving Systems" at the University of Washington, Spring 2025 instructed by both Prof. Baris Kasikci and TA Kan Zhuz</p>
</blockquote>
<h2 id="kv-cache-size-calculation"><a class="toclink" href="#kv-cache-size-calculation">KV Cache Size Calculation</a></h2>
<h3 id="key-components"><a class="toclink" href="#key-components">Key Components</a></h3>
<p>The KV cache size depends on:
- <strong>Num KV heads</strong>: Number of key-value attention heads
- <strong>Head Dim</strong>: Dimension of each attention head
- <strong>2</strong>: Stores both K and V matrices
- <strong>Stype</strong>: Data type size (e.g., 2 bytes for FP16)
- <strong>Seqlen</strong>: Sequence length
- <strong>Layer</strong>: Number of transformer layers</p>
<h3 id="example-calculation-llama3-8b-on-h100"><a class="toclink" href="#example-calculation-llama3-8b-on-h100">Example Calculation - Llama3-8B on H100</a></h3>
<ul>
<li><strong>Total GPU memory</strong>: 80GB</li>
<li><strong>Model weights</strong>: 2  imes8=16GB (assuming FP16)</li>
<li><strong>Activations</strong>: Negligible during serving</li>
<li><strong>Input/Output</strong>: 1024 input tokens, 1024 output tokens</li>
</ul>
<p><strong>KV cache per request</strong>:</p>
<pre><code>(1024 + 1024/2)     imes 2  imes 2  imes 8  imes 128    imes 32 / 1024 / 1024 = 192 MB
</code></pre>
<p><strong>Maximum batch size</strong>:</p>
<pre><code>(80 - 16) / (192/1024) = 341 requests
</code></pre>
<p>Note: Batch size of 333 was needed to reach compute-bound regime on H100.</p>
<h2 id="memory-allocation-challenges"><a class="toclink" href="#memory-allocation-challenges">Memory Allocation Challenges</a></h2>
<h3 id="variable-length-requests"><a class="toclink" href="#variable-length-requests">Variable-Length Requests</a></h3>
<p>When serving requests with different output lengths:
- <strong>Min KV cache</strong>: 192 MB (1024 output tokens)
- <strong>Max KV cache</strong>: 384 MB (4096 output tokens)</p>
<p><strong>Key Question</strong>: How to efficiently allocate KV cache for requests with different lengths?</p>
<h2 id="allocation-methods"><a class="toclink" href="#allocation-methods">Allocation Methods</a></h2>
<h3 id="method-1-fixed-max-sequence-length"><a class="toclink" href="#method-1-fixed-max-sequence-length">Method 1: Fixed Max Sequence Length</a></h3>
<p><strong>Approach</strong>: Allocate KV cache using maximum sequence length the model supports</p>
<p><strong>Problems</strong>:</p>
<ul>
<li><strong>Low utilization</strong>: Significant internal fragmentation</li>
<li><strong>Wasted memory</strong>: Short requests don't use full allocated space</li>
<li><strong>Reduced batch size</strong>: Max batch size = (80-16)/(384/1024) = 170</li>
</ul>
<h3 id="method-2-stdvector-style-allocation"><a class="toclink" href="#method-2-stdvector-style-allocation">Method 2: std::vector-style Allocation</a></h3>
<p><strong>Approach</strong>: </p>
<ul>
<li>Start with small size allocation</li>
<li>Double the size when fully occupied</li>
<li>Similar to dynamic array growth</li>
</ul>
<p><strong>Characteristics</strong>:</p>
<ul>
<li><strong>Internal waste</strong>: ~75% average utilization within requests</li>
<li><strong>External fragmentation</strong>: Memory gaps between requests</li>
<li><strong>Copy overhead</strong>: Acceptable for the flexibility gained</li>
</ul>
<h3 id="method-3-pagedattention"><a class="toclink" href="#method-3-pagedattention">Method 3: PagedAttention</a></h3>
<p><strong>Core Innovation</strong>: Chunk global memory space into small pages</p>
<p><strong>Key Features</strong>:</p>
<ul>
<li><strong>Page size</strong>: 16 tokens' KV = 16 imes2   imes2   imes8   imes128 = 64 KB</li>
<li><strong>No fragmentation</strong>: Pages can be allocated non-contiguously</li>
<li><strong>Efficient bandwidth</strong>: Each page large enough for good utilization</li>
</ul>
<h2 id="pagedattention-implementation"><a class="toclink" href="#pagedattention-implementation">PagedAttention Implementation</a></h2>
<h3 id="multi-level-page-table-structure"><a class="toclink" href="#multi-level-page-table-structure">Multi-Level Page Table Structure</a></h3>
<pre><code>kv_indptr:  [0, 2, 3, 6, 10]  # NumReq + 1 elements
kv_indices: [1, 4, 8, 2, 5, 0, 6, 10, 15, 17]  # NumPage elements
kv_data:    [actual KV cache data...]  # Max Page elements
</code></pre>
<p><strong>How it works</strong>:</p>
<ul>
<li><code>kv_indptr[i]</code> to <code>kv_indptr[i+1]</code> indicates page range for request i</li>
<li><code>kv_indices[kv_indptr[i]:kv_indptr[i+1]]</code> contains page IDs for request i</li>
<li><code>kv_data[page_id]</code> stores the actual KV cache data</li>
</ul>
<h3 id="prefix-sharing"><a class="toclink" href="#prefix-sharing">Prefix Sharing</a></h3>
<p><strong>Concept</strong>: Share common prefixes across multiple requests</p>
<p><strong>Benefits</strong>:</p>
<ul>
<li><strong>Reduced prefill computation</strong>: n    imesp   o p (where p = prefix length, n = requests)</li>
<li><strong>Reduced KV cache size</strong>: n  imesp   o p</li>
<li><strong>Reduced memory bandwidth</strong>: n   imesp   o p during decoding</li>
<li><strong>Asynchronous matching</strong>: Can be performed in background</li>
</ul>
<p><strong>Drawbacks</strong>:</p>
<ul>
<li><strong>Memory overhead</strong>: Must store prefix chunks even when not reused</li>
</ul>
<h3 id="use-cases-for-prefix-sharing"><a class="toclink" href="#use-cases-for-prefix-sharing">Use Cases for Prefix Sharing</a></h3>
<h4 id="multi-round-conversations"><a class="toclink" href="#multi-round-conversations">Multi-round Conversations</a></h4>
<pre><code>1. &quot;You are a helpful assistant. User:Hello, Assistant:Hi!&quot;
2. &quot;You are a helpful assistant. User:Hello, Assistant:Hi!, User: Solve this problem...&quot;
3. &quot;You are a helpful assistant. User:What can you do?&quot;
</code></pre>
<h4 id="multi-request-scenarios"><a class="toclink" href="#multi-request-scenarios">Multi-request Scenarios</a></h4>
<ul>
<li>Same system prompts across different user queries</li>
<li>Shared conversation contexts</li>
<li>Common document prefixes</li>
</ul>
<h3 id="performance-benefits"><a class="toclink" href="#performance-benefits">Performance Benefits</a></h3>
<p>Performance gains vary with:
- <strong>Shared prefix length</strong>: Longer prefixes = greater benefits
- <strong>Batch size</strong>: Higher batch sizes see more improvement
- <strong>Unique suffix length</strong>: Shorter unique parts = better gains</p>
<p>Typical improvements: <strong>2-32x speedup</strong> depending on configuration.</p>
<h2 id="memory-management-strategies"><a class="toclink" href="#memory-management-strategies">Memory Management Strategies</a></h2>
<h3 id="eviction-policies-recomputing-vs-loadoffload"><a class="toclink" href="#eviction-policies-recomputing-vs-loadoffload">Eviction Policies: Recomputing vs Load/Offload</a></h3>
<p>When GPU memory insufficient for full radix tree:</p>
<h4 id="recomputation-approach"><a class="toclink" href="#recomputation-approach">Recomputation Approach</a></h4>
<p><strong>Time cost</strong>: 
$$T_{recompute} = \frac{2pP_{model}}{Compute}$$</p>
<h4 id="loadoffload-approach"><a class="toclink" href="#loadoffload-approach">Load/Offload Approach</a></h4>
<p><strong>Time cost</strong>:
$$T_{load} = \frac{2 \times dtype \times \frac{D_{model}}{GQA} \times L \times p}{PCIE_Bandwidth}$$</p>
<h4 id="comparison-ratio"><a class="toclink" href="#comparison-ratio">Comparison Ratio</a></h4>
<p>$$\frac{T_{recompute}}{T_{load}} = \frac{PCIE_Bandwidth \times P_{compute}}{dtype \times \frac{D_{model}}{GQA} \times L \times Compute}$$</p>
<p><strong>Example calculation for A100</strong>:</p>
<pre><code>30GB/s  imes 8  imes 10^9 / (2  imes 1024   imes 32     imes 300T) = 12
</code></pre>
<p><strong>Conclusion</strong>: Loading from CPU memory is ~12x faster than recomputation.</p>
<h2 id="flashattention-memory-efficient-attention"><a class="toclink" href="#flashattention-memory-efficient-attention">FlashAttention: Memory-Efficient Attention</a></h2>
<h3 id="problem-with-standard-attention"><a class="toclink" href="#problem-with-standard-attention">Problem with Standard Attention</a></h3>
<ul>
<li><strong>Large intermediate matrices</strong>: Seq_len  imes Seq_len attention scores</li>
<li><strong>Memory bottleneck</strong>: Quadratic memory growth with sequence length</li>
</ul>
<h3 id="softmax-numerical-stability"><a class="toclink" href="#softmax-numerical-stability">Softmax Numerical Stability</a></h3>
<p><strong>Standard softmax</strong>:
$$sm(x_i) = \frac{e^{x_i}}{\sum_{j=1}^d e^{x_j}}$$</p>
<p><strong>Numerically stable version</strong>:
$$sm(x_i) = \frac{e^{x_i-c}}{\sum_{j=1}^d e^{x_j-c}}$$</p>
<p>Where c = max(x_i) prevents overflow.</p>
<h3 id="flashattention-algorithm"><a class="toclink" href="#flashattention-algorithm">FlashAttention Algorithm</a></h3>
<p><strong>Key Innovation</strong>: Tile-based computation with online softmax</p>
<p><strong>Steps</strong>:
1. <strong>Tiling</strong>: Divide Q, K, V into blocks that fit in SRAM
2. <strong>Block-wise computation</strong>: Compute attention for each block pair
3. <strong>Online aggregation</strong>: Maintain running statistics (max, sum) across blocks
4. <strong>Rescaling</strong>: Properly combine results from different blocks</p>
<p><strong>Memory hierarchy utilization</strong>:</p>
<ul>
<li><strong>SRAM</strong>: Fast, limited capacity for active blocks</li>
<li><strong>Global memory</strong>: Slower, larger capacity for full matrices</li>
<li><strong>Registers</strong>: Fastest, smallest capacity for running statistics</li>
</ul>
<h3 id="causal-masking-in-flashattention"><a class="toclink" href="#causal-masking-in-flashattention">Causal Masking in FlashAttention</a></h3>
<ul>
<li><strong>Implementation</strong>: Set masked positions to -infty before softmax</li>
<li><strong>Effect</strong>: Masked positions contribute 0 to attention weights</li>
<li><strong>Integration</strong>: Seamlessly handled within block-wise computation</li>
</ul>
<h3 id="grouped-query-attention-gqa"><a class="toclink" href="#grouped-query-attention-gqa">Grouped Query Attention (GQA)</a></h3>
<ul>
<li><strong>Shared KV heads</strong>: Multiple query heads share same key-value heads</li>
<li><strong>Memory savings</strong>: Reduces KV cache size significantly</li>
<li><strong>Implementation</strong>: Process multiple query blocks with same KV block</li>
</ul>
<h2 id="key-takeaways"><a class="toclink" href="#key-takeaways">Key Takeaways</a></h2>
<ol>
<li><strong>KV cache dominates memory usage</strong> in LLM serving, limiting batch sizes</li>
<li><strong>PagedAttention eliminates fragmentation</strong> through page-based allocation</li>
<li><strong>Prefix sharing provides significant speedups</strong> for common use cases</li>
<li><strong>Loading is much faster than recomputation</strong> for evicted cache entries</li>
<li><strong>FlashAttention enables long sequences</strong> through memory-efficient tiled computation</li>
<li><strong>Proper memory management is crucial</strong> for maximizing GPU utilization in LLM serving</li>
</ol>
    </main>
    <footer>
        <p>&copy; 2025 Notes Site</p>
    </footer>
</body>
</html>
    
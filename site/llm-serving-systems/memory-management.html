
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Memory Management in LLM Serving Systems | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache methods.">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/memory-management.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Memory Management in LLM Serving Systems">
    <meta property="og:description" content="Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache methods.">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/memory-management.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Memory Management in LLM Serving Systems">
    <meta name="twitter:description" content="Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache methods.">

    <meta name="keywords" content="memory management,kv cache,prefix sharing,paged attention,flash attention,machine learning">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "Memory Management in LLM Serving Systems", "dateModified": "2025-05-25T16:47:09.382408", "description": "Overview of memory management techniques in LLM serving systems, performance optimization strategies for serving serving systems, particularly on KV cache methods.", "articleSection": "Machine Learning Systems", "keywords": "memory management,kv cache,prefix sharing,paged attention,flash attention,machine learning"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a>
<a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » Memory Management in LLM Serving Systems
        </div>
    </header>
    <main role="main">
        <article>
            <h1>Memory Management in LLM Serving Systems</h1>
            <div class="meta">
                <time datetime="2025-05-25T16:47:09.382408">
                    Last modified: 2025-05-25
                </time>
                <span>Category: <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="memory-management-in-llm-serving-systems">Memory Management in LLM Serving Systems</h1>
<h2 id="kv-cache-size-calculation">KV Cache Size Calculation</h2>
<h3 id="key-components">Key Components</h3>
<p>The KV cache size depends on:
- <strong>Num KV heads</strong>: Number of key-value attention heads
- <strong>Head Dim</strong>: Dimension of each attention head
- <strong>2</strong>: Stores both K and V matrices
- <strong>Stype</strong>: Data type size (e.g., 2 bytes for FP16)
- <strong>Seqlen</strong>: Sequence length
- <strong>Layer</strong>: Number of transformer layers</p>
<h3 id="example-calculation-llama3-8b-on-h100">Example Calculation - Llama3-8B on H100</h3>
<ul>
<li><strong>Total GPU memory</strong>: 80GB</li>
<li><strong>Model weights</strong>: 2  imes8=16GB (assuming FP16)</li>
<li><strong>Activations</strong>: Negligible during serving</li>
<li><strong>Input/Output</strong>: 1024 input tokens, 1024 output tokens</li>
</ul>
<p><strong>KV cache per request</strong>:</p>
<pre><code>(1024 + 1024/2)     imes 2  imes 2  imes 8  imes 128    imes 32 / 1024 / 1024 = 192 MB
</code></pre>
<p><strong>Maximum batch size</strong>:</p>
<pre><code>(80 - 16) / (192/1024) = 341 requests
</code></pre>
<p>Note: Batch size of 333 was needed to reach compute-bound regime on H100.</p>
<h2 id="memory-allocation-challenges">Memory Allocation Challenges</h2>
<h3 id="variable-length-requests">Variable-Length Requests</h3>
<p>When serving requests with different output lengths:
- <strong>Min KV cache</strong>: 192 MB (1024 output tokens)
- <strong>Max KV cache</strong>: 384 MB (4096 output tokens)</p>
<p><strong>Key Question</strong>: How to efficiently allocate KV cache for requests with different lengths?</p>
<h2 id="allocation-methods">Allocation Methods</h2>
<h3 id="method-1-fixed-max-sequence-length">Method 1: Fixed Max Sequence Length</h3>
<p><strong>Approach</strong>: Allocate KV cache using maximum sequence length the model supports</p>
<p><strong>Problems</strong>:
- <strong>Low utilization</strong>: Significant internal fragmentation
- <strong>Wasted memory</strong>: Short requests don't use full allocated space
- <strong>Reduced batch size</strong>: Max batch size = (80-16)/(384/1024) = 170</p>
<h3 id="method-2-stdvector-style-allocation">Method 2: std::vector-style Allocation</h3>
<p><strong>Approach</strong>: 
- Start with small size allocation
- Double the size when fully occupied
- Similar to dynamic array growth</p>
<p><strong>Characteristics</strong>:
- <strong>Internal waste</strong>: ~75% average utilization within requests
- <strong>External fragmentation</strong>: Memory gaps between requests
- <strong>Copy overhead</strong>: Acceptable for the flexibility gained</p>
<h3 id="method-3-pagedattention">Method 3: PagedAttention</h3>
<p><strong>Core Innovation</strong>: Chunk global memory space into small pages</p>
<p><strong>Key Features</strong>:
- <strong>Page size</strong>: 16 tokens' KV = 16 imes2   imes2   imes8   imes128 = 64 KB
- <strong>No fragmentation</strong>: Pages can be allocated non-contiguously
- <strong>Efficient bandwidth</strong>: Each page large enough for good utilization</p>
<h2 id="pagedattention-implementation">PagedAttention Implementation</h2>
<h3 id="multi-level-page-table-structure">Multi-Level Page Table Structure</h3>
<pre><code>kv_indptr:  [0, 2, 3, 6, 10]  # NumReq + 1 elements
kv_indices: [1, 4, 8, 2, 5, 0, 6, 10, 15, 17]  # NumPage elements
kv_data:    [actual KV cache data...]  # Max Page elements
</code></pre>
<p><strong>How it works</strong>:
- <code>kv_indptr[i]</code> to <code>kv_indptr[i+1]</code> indicates page range for request i
- <code>kv_indices[kv_indptr[i]:kv_indptr[i+1]]</code> contains page IDs for request i
- <code>kv_data[page_id]</code> stores the actual KV cache data</p>
<h3 id="prefix-sharing">Prefix Sharing</h3>
<p><strong>Concept</strong>: Share common prefixes across multiple requests</p>
<p><strong>Benefits</strong>:
- <strong>Reduced prefill computation</strong>: n    imesp   o p (where p = prefix length, n = requests)
- <strong>Reduced KV cache size</strong>: n  imesp   o p
- <strong>Reduced memory bandwidth</strong>: n   imesp   o p during decoding
- <strong>Asynchronous matching</strong>: Can be performed in background</p>
<p><strong>Drawbacks</strong>:
- <strong>Memory overhead</strong>: Must store prefix chunks even when not reused</p>
<h3 id="use-cases-for-prefix-sharing">Use Cases for Prefix Sharing</h3>
<h4 id="multi-round-conversations">Multi-round Conversations</h4>
<pre><code>1. &quot;You are a helpful assistant. User:Hello, Assistant:Hi!&quot;
2. &quot;You are a helpful assistant. User:Hello, Assistant:Hi!, User: Solve this problem...&quot;
3. &quot;You are a helpful assistant. User:What can you do?&quot;
</code></pre>
<h4 id="multi-request-scenarios">Multi-request Scenarios</h4>
<ul>
<li>Same system prompts across different user queries</li>
<li>Shared conversation contexts</li>
<li>Common document prefixes</li>
</ul>
<h3 id="performance-benefits">Performance Benefits</h3>
<p>Performance gains vary with:
- <strong>Shared prefix length</strong>: Longer prefixes = greater benefits
- <strong>Batch size</strong>: Higher batch sizes see more improvement
- <strong>Unique suffix length</strong>: Shorter unique parts = better gains</p>
<p>Typical improvements: <strong>2-32x speedup</strong> depending on configuration.</p>
<h2 id="memory-management-strategies">Memory Management Strategies</h2>
<h3 id="eviction-policies-recomputing-vs-loadoffload">Eviction Policies: Recomputing vs Load/Offload</h3>
<p>When GPU memory insufficient for full radix tree:</p>
<h4 id="recomputation-approach">Recomputation Approach</h4>
<p><strong>Time cost</strong>: 
$$T_{recompute} = \frac{2pP_{model}}{Compute}$$</p>
<h4 id="loadoffload-approach">Load/Offload Approach</h4>
<p><strong>Time cost</strong>:
$$T_{load} = \frac{2 \times dtype \times \frac{D_{model}}{GQA} \times L \times p}{PCIE_Bandwidth}$$</p>
<h4 id="comparison-ratio">Comparison Ratio</h4>
<p>$$\frac{T_{recompute}}{T_{load}} = \frac{PCIE_Bandwidth \times P_{compute}}{dtype \times \frac{D_{model}}{GQA} \times L \times Compute}$$</p>
<p><strong>Example calculation for A100</strong>:</p>
<pre><code>30GB/s  imes 8  imes 10^9 / (2  imes 1024   imes 32     imes 300T) = 12
</code></pre>
<p><strong>Conclusion</strong>: Loading from CPU memory is ~12x faster than recomputation.</p>
<h2 id="flashattention-memory-efficient-attention">FlashAttention: Memory-Efficient Attention</h2>
<h3 id="problem-with-standard-attention">Problem with Standard Attention</h3>
<ul>
<li><strong>Large intermediate matrices</strong>: Seq_len  imes Seq_len attention scores</li>
<li><strong>Memory bottleneck</strong>: Quadratic memory growth with sequence length</li>
</ul>
<h3 id="softmax-numerical-stability">Softmax Numerical Stability</h3>
<p><strong>Standard softmax</strong>:
$$sm(x_i) = \frac{e^{x_i}}{\sum_{j=1}^d e^{x_j}}$$</p>
<p><strong>Numerically stable version</strong>:
$$sm(x_i) = \frac{e^{x_i-c}}{\sum_{j=1}^d e^{x_j-c}}$$</p>
<p>Where c = max(x_i) prevents overflow.</p>
<h3 id="flashattention-algorithm">FlashAttention Algorithm</h3>
<p><strong>Key Innovation</strong>: Tile-based computation with online softmax</p>
<p><strong>Steps</strong>:
1. <strong>Tiling</strong>: Divide Q, K, V into blocks that fit in SRAM
2. <strong>Block-wise computation</strong>: Compute attention for each block pair
3. <strong>Online aggregation</strong>: Maintain running statistics (max, sum) across blocks
4. <strong>Rescaling</strong>: Properly combine results from different blocks</p>
<p><strong>Memory hierarchy utilization</strong>:
- <strong>SRAM</strong>: Fast, limited capacity for active blocks
- <strong>Global memory</strong>: Slower, larger capacity for full matrices
- <strong>Registers</strong>: Fastest, smallest capacity for running statistics</p>
<h3 id="causal-masking-in-flashattention">Causal Masking in FlashAttention</h3>
<ul>
<li><strong>Implementation</strong>: Set masked positions to -infty before softmax</li>
<li><strong>Effect</strong>: Masked positions contribute 0 to attention weights</li>
<li><strong>Integration</strong>: Seamlessly handled within block-wise computation</li>
</ul>
<h3 id="grouped-query-attention-gqa">Grouped Query Attention (GQA)</h3>
<ul>
<li><strong>Shared KV heads</strong>: Multiple query heads share same key-value heads</li>
<li><strong>Memory savings</strong>: Reduces KV cache size significantly</li>
<li><strong>Implementation</strong>: Process multiple query blocks with same KV block</li>
</ul>
<h2 id="key-takeaways">Key Takeaways</h2>
<ol>
<li><strong>KV cache dominates memory usage</strong> in LLM serving, limiting batch sizes</li>
<li><strong>PagedAttention eliminates fragmentation</strong> through page-based allocation</li>
<li><strong>Prefix sharing provides significant speedups</strong> for common use cases</li>
<li><strong>Loading is much faster than recomputation</strong> for evicted cache entries</li>
<li><strong>FlashAttention enables long sequences</strong> through memory-efficient tiled computation</li>
<li><strong>Proper memory management is crucial</strong> for maximizing GPU utilization in LLM serving</li>
</ol>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/flash%20attention.html">flash attention</a>
                <a href="/tags/kv%20cache.html">kv cache</a>
                <a href="/tags/machine%20learning.html">machine learning</a>
                <a href="/tags/memory%20management.html">memory management</a>
                <a href="/tags/paged%20attention.html">paged attention</a>
                <a href="/tags/prefix%20sharing.html">prefix sharing</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
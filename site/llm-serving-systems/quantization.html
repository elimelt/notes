
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quantization in LLM Serving Systems | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Overview of quantization techniques for LLM serving systems, focusing on theoretical foundations and practical applications.">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/quantization.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Quantization in LLM Serving Systems">
    <meta property="og:description" content="Overview of quantization techniques for LLM serving systems, focusing on theoretical foundations and practical applications.">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/quantization.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Quantization in LLM Serving Systems">
    <meta name="twitter:description" content="Overview of quantization techniques for LLM serving systems, focusing on theoretical foundations and practical applications.">

    <meta name="keywords" content="quantization,low precision,performance,memory efficiency">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "Quantization in LLM Serving Systems", "dateModified": "2025-05-25T16:34:20.797394", "description": "Overview of quantization techniques for LLM serving systems, focusing on theoretical foundations and practical applications.", "articleSection": "Machine Learning Systems", "keywords": "quantization,low precision,performance,memory efficiency"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » Quantization in LLM Serving Systems
        </div>
    </header>
    <main role="main">
        <article>
            <h1>Quantization in LLM Serving Systems</h1>
            <div class="meta">
                <time datetime="2025-05-25T16:34:20.797394">
                    Last modified: 2025-05-25
                </time>
                <span>Category: <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="quantization">Quantization</h1>
<h2 id="fundamentals">Fundamentals</h2>
<h3 id="what-is-quantization">What is Quantization?</h3>
<p><strong>Quantization</strong> is the process of reducing the precision of numerical representations to achieve:
- <strong>Reduced memory footprint</strong>
- <strong>Lower latency</strong>
- <strong>Decreased energy consumption</strong>
- <strong>Compact representation</strong></p>
<h3 id="key-idea">Key Idea</h3>
<p>Convert high-precision floating-point numbers (FP32) to lower-precision representations (INT4, INT8) while maintaining acceptable accuracy.</p>
<h3 id="energy-motivation">Energy Motivation</h3>
<ul>
<li><strong>Energy scales quadratically</strong> with bit-width</li>
<li><strong>4x bit width</strong>  o <strong>~16x energy consumption</strong></li>
<li><strong>Memory access costs</strong> (DRAM: 640pJ vs 8b Add: 0.03pJ)</li>
</ul>
<h3 id="why-quantization-works">Why Quantization Works</h3>
<ul>
<li><strong>Activation ranges are well-defined</strong> in neural networks</li>
<li><strong>Even distributions</strong> lead to better training outcomes</li>
<li><strong>Proper initialization</strong> prevents activation divergence and gradient instability</li>
</ul>
<hr />
<h2 id="floating-point-representations">Floating Point Representations</h2>
<h3 id="fp32-full-precision">FP32 (Full Precision)</h3>
<ul>
<li><strong>Sign</strong>: 1 bit</li>
<li><strong>Exponent</strong>: 8 bits</li>
<li><strong>Significand/Mantissa</strong>: 23 bits</li>
<li><strong>Total</strong>: 32 bits</li>
</ul>
<h3 id="fp16-half-precision">FP16 (Half Precision)</h3>
<ul>
<li><strong>Sign</strong>: 1 bit</li>
<li><strong>Exponent</strong>: 5 bits</li>
<li><strong>Significand/Mantissa</strong>: 10 bits</li>
<li><strong>Total</strong>: 16 bits</li>
</ul>
<h3 id="dynamic-range-vs-precision-trade-offs">Dynamic Range vs Precision Trade-offs</h3>
<ul>
<li><strong>Dynamic Range</strong>: Range of representable numbers (better for training)</li>
<li><strong>Precision</strong>: Distance between neighboring values (better for inference)</li>
<li><strong>INT8</strong>: Limited dynamic range (-127 to 127) but consistent precision</li>
</ul>
<hr />
<h2 id="linear-quantization">Linear Quantization</h2>
<h3 id="mathematical-formulation">Mathematical Formulation</h3>
<p><strong>Quantization</strong>: $q = \text{clip}(\text{round}(r/S + Z), -2^{b-1}, 2^{b-1})$</p>
<p><strong>Dequantization</strong>: $r = S(q - Z)$</p>
<p>Where:
- $S = \frac{r_{max} - r_{min}}{q_{max} - q_{min}}$ (scaling factor)
- $Z$ = zero point
- $b$ = bit width</p>
<h3 id="sources-of-error">Sources of Error</h3>
<ol>
<li><strong>Rounding Error</strong>: Bounded by [-S/2, S/2]</li>
<li><strong>Clipping Error</strong>: Values outside representable range</li>
</ol>
<h3 id="symmetric-vs-asymmetric-quantization">Symmetric vs Asymmetric Quantization</h3>
<ul>
<li><strong>Symmetric</strong>: Zero point Z = 0 (simpler computation)</li>
<li><strong>Asymmetric</strong>: Non-zero Z (more flexible, better for skewed distributions like ReLU outputs)</li>
</ul>
<h3 id="matrix-multiplication-with-quantization">Matrix Multiplication with Quantization</h3>
<p>$$Y = S_W S_X [q_W q_X - q_W Z_X - q_X Z_W + Z_W Z_X]$$</p>
<p><strong>Symmetric quantization</strong> eliminates the overhead terms when $Z_W = Z_X = 0$.</p>
<hr />
<h2 id="non-linear-quantization">Non-Linear Quantization</h2>
<h3 id="clustering-based-approach">Clustering-Based Approach</h3>
<ul>
<li><strong>Use case</strong>: Skewed weight distributions</li>
<li><strong>Method</strong>: K-means clustering to determine quantization levels</li>
<li><strong>Storage</strong>:<ul>
<li>Indices: $\log_2(N)$ bits per weight</li>
<li>Codebook: N centroids in original precision</li>
<li><strong>Example</strong>: 3.2x compression for 4-bit quantization</li>
</ul>
</li>
</ul>
<h3 id="granularity-options">Granularity Options</h3>
<ol>
<li><strong>Per-Tensor</strong>: Single scale/zero-point for entire tensor</li>
<li><strong>Per-Channel/Vector</strong>: Scale per channel (more precise)</li>
<li><strong>Per-Group</strong>: Intermediate granularity</li>
</ol>
<p><strong>Trade-off</strong>: Finer granularity    o Higher precision but increased overhead</p>
<hr />
<h2 id="training-workflows">Training Workflows</h2>
<h3 id="post-training-quantization-ptq">Post-Training Quantization (PTQ)</h3>
<ul>
<li><strong>Training</strong>: Uses full precision (FP32/BF16)</li>
<li><strong>Inference</strong>: Applies quantization to weights and/or activations</li>
<li><strong>Simplest approach</strong> but may have accuracy degradation</li>
</ul>
<h3 id="quantization-aware-training-qat">Quantization-Aware Training (QAT)</h3>
<ul>
<li><strong>Training</strong>: Simulates quantization effects during training</li>
<li><strong>Method</strong>: Quantize  o Dequantize in forward pass</li>
<li><strong>Backpropagation</strong>: Uses Straight-Through Estimator (STE) for differentiability</li>
<li><strong>Results</strong>: Generally outperforms PTQ</li>
</ul>
<h3 id="straight-through-estimator-ste">Straight-Through Estimator (STE)</h3>
<ul>
<li><strong>Problem</strong>: Step function derivative is 0 or infty</li>
<li><strong>Solution</strong>: Use identity function gradient: $\frac{\partial}{\partial x}\text{quantize}(x) \approx 1$</li>
</ul>
<hr />
<h2 id="llm-specific-quantization-challenges">LLM-Specific Quantization Challenges</h2>
<h3 id="outlier-problem">Outlier Problem</h3>
<ul>
<li><strong>Observation</strong>: Quantization accuracy drops significantly for large models (&gt;6.7B parameters)</li>
<li><strong>Cause</strong>: Emergence of outlier features in activations</li>
<li><strong>Impact</strong>: Sharp accuracy degradation with standard quantization</li>
</ul>
<h3 id="modern-llm-context">Modern LLM Context</h3>
<p><strong>Example - DeepSeek V3</strong>:
- <strong>FP8 quantization</strong>: 671B parameters     imes 1 byte = 671 GB (fits ~5 H200s)
- <strong>BF16 weights</strong>: 671B parameters     imes 2 bytes = 1.3 TB</p>
<hr />
<h2 id="advanced-llm-quantization-methods">Advanced LLM Quantization Methods</h2>
<h3 id="llmint8">LLM.int8()</h3>
<p><strong>Key Ideas</strong>:
1. <strong>Vector-wise quantization</strong> for better outlier handling
2. <strong>Mixed precision</strong>: Keep outliers in FP16, quantize regular values to INT8
3. <strong>Decomposition</strong>: Separate outlier and regular computations</p>
<h3 id="smoothquant-w8a8">SmoothQuant (W8A8)</h3>
<p><strong>Motivation</strong>: Outliers typically in activations, not weights</p>
<p><strong>Core Concept</strong>: Migrate quantization difficulty
- <strong>Formula</strong>: $s_j = \frac{\max(|X_j|)^\alpha}{\max(|W_j|)^{1-\alpha}}$
- <strong>Process</strong>: $WX \rightarrow Q(W \cdot s)(s^{-1} \cdot X)$
- <strong>Optimal alpha</strong>: Empirically found to be 0.5</p>
<p><strong>Benefits</strong>: Balances quantization difficulty between weights and activations</p>
<h3 id="awq-activation-aware-weight-only-quantization">AWQ (Activation-Aware Weight-Only Quantization)</h3>
<p><strong>Target</strong>: Low-batch scenarios where activation quantization is prohibitive</p>
<p><strong>Key Insights</strong>:
1. <strong>Weight-only quantization</strong> (W4) for memory efficiency
2. <strong>Salient weight identification</strong> using activation magnitudes
3. <strong>Per-channel scaling</strong> to protect important weights</p>
<p><strong>Method</strong>: $WX \rightarrow Q(W \cdot s)(s^{-1} \cdot X)$
- Scale important channels up before quantization
- Fuse inverse scaling with previous operations (e.g., LayerNorm)</p>
<hr />
<h2 id="quantization-performance-impact">Quantization Performance Impact</h2>
<h3 id="accuracy-vs-bit-width">Accuracy vs Bit-width</h3>
<ul>
<li><strong>High variance</strong> across models and quantization levels</li>
<li><strong>INT8</strong>: Generally acceptable accuracy loss</li>
<li><strong>INT4</strong>: Requires careful techniques (AWQ, etc.)</li>
<li><strong>INT3/INT2</strong>: Significant accuracy challenges</li>
</ul>
<h3 id="rounding-schemes-impact">Rounding Schemes Impact</h3>
<table>
<thead>
<tr>
<th>Scheme</th>
<th>Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>Nearest</td>
<td>52.29%</td>
</tr>
<tr>
<td>Stochastic</td>
<td>52.06plusequal5.52%</td>
</tr>
<tr>
<td>Stochastic (best)</td>
<td>63.06%</td>
</tr>
<tr>
<td>Ceil/Floor</td>
<td>0.10%</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="summary">Summary</h2>
<p><strong>Quantization</strong> is essential for efficient LLM serving, providing:
- <strong>Memory reduction</strong> (2-8x compression)
- <strong>Energy savings</strong> (quadratic with bit-width)
- <strong>Inference speedup</strong> through specialized hardware</p>
<p><strong>Key techniques for LLMs</strong>:
- <strong>Handle outliers</strong> through mixed precision or smoothing
- <strong>Choose appropriate granularity</strong> (per-tensor vs per-channel)
- <strong>Balance accuracy vs efficiency</strong> based on deployment requirements</p>
<p><strong>Modern approaches</strong> (LLM.int8(), SmoothQuant, AWQ) address LLM-specific challenges while maintaining practical deployability.</p>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/low%20precision.html">low precision</a>
                <a href="/tags/memory%20efficiency.html">memory efficiency</a>
                <a href="/tags/performance.html">performance</a>
                <a href="/tags/quantization.html">quantization</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
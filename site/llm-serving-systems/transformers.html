
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Architecture and Implementation | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Overview of transformer architecture (specifically Llama) and its implementation details for LLM serving systems">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/transformers.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="Transformer Architecture and Implementation">
    <meta property="og:description" content="Overview of transformer architecture (specifically Llama) and its implementation details for LLM serving systems">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/transformers.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Transformer Architecture and Implementation">
    <meta name="twitter:description" content="Overview of transformer architecture (specifically Llama) and its implementation details for LLM serving systems">

    <meta name="keywords" content="transformers,architecture,implementation,attention,prefill,decode,feedforward,normalization">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "Transformer Architecture and Implementation", "dateModified": "2025-05-25T16:32:00.770325", "description": "Overview of transformer architecture (specifically Llama) and its implementation details for LLM serving systems", "articleSection": "Machine Learning Systems", "keywords": "transformers,architecture,implementation,attention,prefill,decode,feedforward,normalization"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="/index.html">Home</a>
<a href="/categories/index.html">Categories</a>
<a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » Transformer Architecture and Implementation
        </div>
    </header>
    <main role="main">
        <article>
            <h1>Transformer Architecture and Implementation</h1>
            <div class="meta">
                <time datetime="2025-05-25T16:32:00.770325">
                    Last modified: 2025-05-25
                </time>
                <span>Category: <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h2 id="transformer-architecture-overview">Transformer Architecture Overview</h2>
<h3 id="prefill-vs-decode-phases">Prefill vs. Decode Phases</h3>
<p><strong>Prefill Phase:</strong>
- Processes entire input prompt at once
- All tokens processed in parallel
- Compute-bound operation</p>
<p><strong>Decode Phase:</strong>
- Generates one token at a time
- Sequential generation process
- Memory-bound operation (utilizes KV cache)</p>
<h3 id="transformer-layers-and-iterations">Transformer Layers and Iterations</h3>
<ul>
<li><strong>Inference Iteration:</strong> Complete forward pass through all layers to generate one output token</li>
<li><strong>Inference Layer:</strong> Single transformer layer containing attention and FFN components</li>
<li><strong>Activations:</strong> Intermediate representations passed between layers</li>
</ul>
<hr />
<h2 id="core-transformer-components">Core Transformer Components</h2>
<h3 id="1-embedding-layer">1. Embedding Layer</h3>
<p><strong>Purpose:</strong> Convert token IDs to dense vector representations</p>
<pre><code class="language-python">input_ids: [The, University, ...]
embeddings: {
    The: [0, 1, 0, 1, ...],  # 4096 elements in Llama
    University: [0, 0, 0, ...]
}
</code></pre>
<h3 id="2-attention-mechanism">2. Attention Mechanism</h3>
<h4 id="self-attention-formula">Self-Attention Formula</h4>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$</p>
<p>Where:
- <strong>Q (Query):</strong> What the transformer is looking for
- <strong>K (Key):</strong> What's available in the sequence
- <strong>V (Value):</strong> What needs to be updated to assimilate context
- <strong>$d_k$:</strong> Head dimension for scaling</p>
<h4 id="causal-self-attention">Causal Self-Attention</h4>
<ul>
<li><strong>Causal Mask:</strong> Prevents tokens from attending to future tokens</li>
<li>Uses $-\infty$ values in attention matrix for future positions</li>
<li>When $x_i \to -\infty$, $\text{softmax}(x_i) \to 0$</li>
</ul>
<h4 id="grouped-query-attention-gqa">Grouped Query Attention (GQA)</h4>
<ul>
<li><strong>Purpose:</strong> Reduce KV cache memory usage</li>
<li><strong>Mechanism:</strong> Multiple query heads share the same key and value heads</li>
<li><strong>Group Size:</strong> Number of query heads per key/value head (e.g., group size = 4)</li>
<li><strong>Benefit:</strong> Allows increasing batch size by factor of group size</li>
</ul>
<h3 id="3-multi-head-attention">3. Multi-Head Attention</h3>
<h4 id="head-separation-process">Head Separation Process</h4>
<pre><code class="language-python"># Original query tensor
q = [[1, 2, 3, 4, 5, 6],    # Token 1
     [7, 8, 9, 10, 11, 12]] # Token 2
# Shape: (seq_len, hidden_dim) = (2, 6)

# Separated into heads
sub_q = [[[1, 2, 3],   # Head 1 for Token 1
          [4, 5, 6]],  # Head 2 for Token 1
         [[7, 8, 9],   # Head 1 for Token 2
          [10, 11, 12]]] # Head 2 for Token 2
# Shape: (seq_len, num_heads, head_dim) = (2, 2, 3)
</code></pre>
<h3 id="4-feed-forward-network-ffn">4. Feed Forward Network (FFN)</h3>
<p><strong>Architecture:</strong> Two linear transformations with activation function
- <strong>Up Projection:</strong> Expands hidden dimension
- <strong>Gate Projection:</strong> Controls information flow
- <strong>Activation Function:</strong> SwiGLU (Swish-Gated Linear Unit)
- <strong>Down Projection:</strong> Returns to original dimension</p>
<p><strong>Mathematical representation:</strong>
$$\text{FFN}(x) = \text{Down}(\text{SwiGLU}(\text{Up}(x)) \odot \text{Gate}(x))$$</p>
<h3 id="5-normalization">5. Normalization</h3>
<h4 id="rmsnorm-root-mean-square-normalization">RMSNorm (Root Mean Square Normalization)</h4>
<p>$$\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2 + \epsilon}} \odot g$$</p>
<p>Where:
- <strong>x:</strong> Input vector of size n
- <strong>epsilon:</strong> Small constant for numerical stability (e.g., $10^{-8}$)
- <strong>g:</strong> Learned scaling parameter (element-wise multiplication)</p>
<h3 id="6-residual-connections">6. Residual Connections</h3>
<ul>
<li><strong>Purpose:</strong> Enable gradient flow in deep networks</li>
<li><strong>Implementation:</strong> Add input to output of each major component</li>
<li><strong>Formula:</strong> $\text{output} = \text{input} + \text{component}(\text{input})$</li>
</ul>
<hr />
<h2 id="multi-gpu-implementation">Multi-GPU Implementation</h2>
<h3 id="tensor-parallelism">Tensor Parallelism</h3>
<ul>
<li><strong>Weight Distribution:</strong> Split weight matrices across GPUs</li>
<li><strong>Query/Key/Value:</strong> Distributed across different GPUs</li>
<li><strong>Computation:</strong> Parallel matrix multiplications</li>
</ul>
<h3 id="communication-operations">Communication Operations</h3>
<h4 id="allgather">AllGather</h4>
<ul>
<li><strong>Purpose:</strong> Collect partial results from all GPUs</li>
<li><strong>Usage:</strong> After attention computation to gather all head outputs</li>
<li><strong>Operation Type:</strong> Network-bound</li>
</ul>
<h4 id="allreduce">AllReduce</h4>
<ul>
<li><strong>Purpose:</strong> Sum partial results across GPUs</li>
<li><strong>Composition:</strong> ReduceScatter + AllGather</li>
<li><strong>Usage:</strong> After FFN down projection</li>
<li><strong>Operation Type:</strong> Network-bound</li>
</ul>
<hr />
<h2 id="resource-utilization-patterns">Resource Utilization Patterns</h2>
<h3 id="compute-bound-operations">Compute-Bound Operations</h3>
<ul>
<li>Query, Key, Value projections</li>
<li>Up and Gate projections in FFN</li>
<li>Output projections</li>
<li>Prefill attention computation</li>
</ul>
<h3 id="memory-bound-operations">Memory-Bound Operations</h3>
<ul>
<li>Decode attention (KV cache access)</li>
<li>Reading cached key-value pairs</li>
</ul>
<h3 id="network-bound-operations">Network-Bound Operations</h3>
<ul>
<li>AllGather communications</li>
<li>AllReduce communications</li>
</ul>
<hr />
<h2 id="key-implementation-details">Key Implementation Details</h2>
<h3 id="kv-cache-management">KV Cache Management</h3>
<ul>
<li><strong>Storage:</strong> Unique per batch, shared across layers</li>
<li><strong>Purpose:</strong> Avoid recomputing key-value pairs during decode</li>
<li><strong>Memory Impact:</strong> Major contributor to GPU memory usage</li>
</ul>
<h3 id="rotary-positional-encoding">Rotary Positional Encoding</h3>
<ul>
<li><strong>Application:</strong> Applied to query and key vectors</li>
<li><strong>Purpose:</strong> Encode relative position information</li>
<li><strong>Advantage:</strong> Better handling of variable sequence lengths</li>
</ul>
<h3 id="softmax-computation">Softmax Computation</h3>
<p>$$\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$$
- <strong>Numerical Stability:</strong> Often implemented with temperature scaling
- <strong>Attention Weights:</strong> Output represents attention distribution</p>
<hr />
<h2 id="architecture-comparison">Architecture Comparison</h2>
<h3 id="original-transformer-vs-llama-architecture">Original Transformer vs. Llama Architecture</h3>
<ul>
<li><strong>Original:</strong> Encoder-decoder with cross-attention</li>
<li><strong>Llama:</strong> Decoder-only architecture</li>
<li><strong>Normalization:</strong> LayerNorm  o RMSNorm</li>
<li><strong>Activation:</strong> ReLU  o SwiGLU</li>
<li><strong>Position Encoding:</strong> Absolute   o Rotary (RoPE)</li>
<li><strong>Attention:</strong> Multi-head     o Grouped Query Attention</li>
</ul>
<p>This architecture forms the foundation for modern large language models and understanding these components is crucial for optimizing LLM serving systems.</p>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/architecture.html">architecture</a>
                <a href="/tags/attention.html">attention</a>
                <a href="/tags/decode.html">decode</a>
                <a href="/tags/feedforward.html">feedforward</a>
                <a href="/tags/implementation.html">implementation</a>
                <a href="/tags/normalization.html">normalization</a>
                <a href="/tags/prefill.html">prefill</a>
                <a href="/tags/transformers.html">transformers</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPU Kernel Programming with Triton and CUDA | Elijah's Notes</title>

    <!-- SEO Meta Tags -->
    <meta name="description" content="Overview of programming GPU kernels with Triton and CUDA">
    <meta name="author" content="Elijah Melton">
    <meta name="robots" content="index, follow">
    <meta name="generator" content="Custom Static Site Generator">
    <link rel="canonical" href="https://notes.elimelt.com/llm-serving-systems/triton.html">

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="GPU Kernel Programming with Triton and CUDA">
    <meta property="og:description" content="Overview of programming GPU kernels with Triton and CUDA">
    <meta property="og:url" content="https://notes.elimelt.com/llm-serving-systems/triton.html">

    <!-- Twitter -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="GPU Kernel Programming with Triton and CUDA">
    <meta name="twitter:description" content="Overview of programming GPU kernels with Triton and CUDA">

    <meta name="keywords" content="gpu,triton,cuda">

    <!-- Schema.org JSON-LD -->
    <script type="application/ld+json">
    {"@context": "https://schema.org", "@type": "Article", "headline": "GPU Kernel Programming with Triton and CUDA", "dateModified": "2025-05-25T16:32:00.765660", "description": "Overview of programming GPU kernels with Triton and CUDA", "url": "https://notes.elimelt.com/llm-serving-systems/triton.html", "articleSection": "Machine Learning Systems", "keywords": "gpu,triton,cuda"}
    </script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <!-- and it's easy to individually load additional languages -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/verilog.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/java.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/cpp.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/c.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <link rel="stylesheet" href=/css/styles.css>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.css">

    <!-- Configure KaTeX auto-render -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "$$", right: "$$", display: true},
                    {left: "\[", right: "\]", display: true},
                    {left: "$", right: "$", display: false},
                ],
                preProcess: (math) => {
                    console.log("Pre-processing: " + math);
                    math = math.split("\n").map((line) => {
                        if (line.endsWith("\\")) {
                            return line + "\\";
                        }
                        return line;
                    }).join("\n");
                    return math;
                },
                throwOnError: false
            });
        });
    </script>
    <!-- Configure Highlight.js -->
    <script>hljs.highlightAll();</script>
</head>
<body>
    <header>
        <nav role="navigation" aria-label="Main navigation">
            <a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a>
        </nav>
        <div class="breadcrumbs" role="navigation" aria-label="Breadcrumb">
            <a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » GPU Kernel Programming with Triton and CUDA
        </div>
    </header>
    <main role="main">
        <article>
            <h1>GPU Kernel Programming with Triton and CUDA</h1>
            <div class="meta">
                <time datetime="2025-05-25T16:32:00.765660">
                    Last modified: 2025-05-25
                </time>
                <span>Category: <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a></span>
                <span><a id="parent-link" href="index.html">..</a></span>
            </div>
            <div class=content>
                <h1 id="triton"><a class="toclink" href="#triton">Triton</a></h1>
<p>Avoids allocating and deallocating lots of memory, and instead uses a single memory pool for all allocations and uses register file for temporary storage.</p>
<pre><code class="language-py">import torch
import triton
import triton.language as tl

@triton.jit # JIT compile the function
def add_with_triton(a, b, output, n_elements, BLOCK_SIZE: tl.constexpr):
    DIM = 0
    block_id = yl.program_id(DIM)
    start = block_id * BLOCK_SIZE

    offset = tl.arrange(DIM, BLOCK_SIZE) + start
    mask = offset &lt; n_elements

    a_seg = tl.load(a + offset, mask=mask)
    b_seg = tl.load(b + offset, mask=mask)
    output_seg = a_seg + b_seg

    tl.store(output + offset, output_seg, mask=mask)

@triton.jit
def add_with_triton_grid(a, b, output, n_elements, BLOCK_SIZE: tl.constexpr):
    output = torch.empty_like(a)


num = 100000000
a = torch.rand(num, device='cuda')
b = torch.rand(num, device='cuda')

output_torch = a + b
output_triton = add_with_triton(a, b, num, BLOCK_SIZE=1024)

assert torch.allclose(output_torch, output_triton)

start = torch.cuda.Event(enable_timing=True)
end = torch.cuda.Event(enable_timing=True)
start.record()
for i in range(100):
    output_triton = add_with_triton(a, b, num, BLOCK_SIZE=1024)
end.record()

torch.cuda.synchronize()
time_total = start.elapsed_time(end) / 1000
print(&quot;with triton&quot;)
print(f'bandwidth: {num * 4 * 3 / time_total / 1000)} MB/s')
print(f'total time: {time_total} s')
</code></pre>
<h1 id="cuda"><a class="toclink" href="#cuda">CUDA</a></h1>
<ul>
<li>Memory allocation and deallocation is done explicitly<ul>
<li>cudaMalloc, cudaFree, cudaMallocHost</li>
</ul>
</li>
<li>Memory movement and setting<ul>
<li>cudaMemcpy, cudaMemcpyAsync, cudaMemsset, cudaMemsetAsync</li>
</ul>
</li>
</ul>
<h2 id="cuda-kernels"><a class="toclink" href="#cuda-kernels">CUDA Kernels</a></h2>
<ul>
<li>Declare a kernel: <code>__global__ void kernel_name(args...)</code></li>
<li>Declare device "helper" function: <code>__device__ void helper_name(args...)</code></li>
<li>Args are on host, pointers to device memory also on host</li>
</ul>
<h2 id="launching-kernels"><a class="toclink" href="#launching-kernels">Launching Kernels</a></h2>
<ul>
<li>Define block shapes, e.g. <code>dim3 block(dim_x, dim_y, dim_z)</code></li>
<li>Define thread shapes, e.g. <code>dim3 thread(thread_x, thread_y, thread_z)</code></li>
<li>Launch kernel: <code>kernel_name&lt;&lt;&lt;block, thread&gt;&gt;&gt;(args...)</code></li>
</ul>
<h2 id="synchronization-and-error-handling"><a class="toclink" href="#synchronization-and-error-handling">Synchronization and Error Handling</a></h2>
<ul>
<li>Thread synchronization: <code>__syncthreads()</code> -&gt; device function</li>
<li>Block synchronization: usually not feasible, except for "cooperative launch"</li>
<li>Device synchronization: <code>cudaDeviceSynchronize()</code> -&gt; host function</li>
<li>Error handling: <code>cudaGetLastError()</code>, <code>cudaGetErrorString()</code></li>
</ul>
<h2 id="cuda-addition-kernel"><a class="toclink" href="#cuda-addition-kernel">CUDA Addition Kernel</a></h2>
<pre><code class="language-python">#include &lt;cuda_runtime.h&gt;
#include &lt;stdio.h&gt;

#define BLOCK_SIZE 256

// copied num_blocks * num_threads from Triton example
__global__ void addition(int *a, int *b, int *c, int num) {
    int start = blockIdx.x * blockDim.x + threadIdx.x;
    if (start &lt; num) {
        c[start] = a[start] + b[start];
    }
}

int main() {
    int num = 100000000;
    int *a, *b, *c;
    int *d_a, *d_b, *d_c;

    int size = num * sizeof(int);

    a = (int *)malloc(size);
    b = (int *)malloc(size);
    c = (int *)malloc(size);

    cudaMalloc(&amp;d_a, size);
    cudaMalloc(&amp;d_b, size);
    cudaMalloc(&amp;d_c, size);

    for (int i = 0; i &lt; num; i++) {
        a[i] = i;
        b[i] = i;
    }

    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);

    // Launch kernel
    dim3 grid((num + BLOCK_SIZE - 1) / BLOCK_SIZE);
    dim3 block(BLOCK_SIZE);
    cudaDeviceSynchronize();

    // check for errors
    cudaError_t error = cudaGetLastError();
    if (error != cudaSuccess) {
        fprintf(stderr, &quot;ERROR: %s\n&quot;, cudaGetErrorString(error));
        return 1;
    }

    // Copy output
    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);

    // validate output
    for (int i = 0; i &lt; num; i++) {
        if (c[i] != a[i] + b[i]) {
            printf(&quot;Error at %d\n&quot;, i);
            break;
        }
    }
}
</code></pre>
<h3 id="compile"><a class="toclink" href="#compile">Compile</a></h3>
<p>use <code>nvcc</code> to compile the CUDA code</p>
<pre><code class="language-sh"># compile
nvcc addition.cu -o addition

# run
./addition
</code></pre>
<h2 id="tuning"><a class="toclink" href="#tuning">tuning</a></h2>
<p>Some things you can try to tune your CUDA code:
- Increase block size
- Increase number of blocks
- Load more things per thread</p>
<h2 id="additional-cuda-features-on-modern-gpus"><a class="toclink" href="#additional-cuda-features-on-modern-gpus">Additional CUDA features on modern GPUs</a></h2>
<ul>
<li>Unified memory addressing (P100+)</li>
<li>NvLink (P100+)</li>
<li>Clusters (H100+)</li>
<li>TMA (H100+)</li>
<li>NVSHART (H100+)</li>
<li>FP4 &amp; FP6 (B100+)</li>
</ul>
            </div>
            <div class="tags">
                Tags:
                <a href="/tags/cuda.html">cuda</a>
                <a href="/tags/gpu.html">gpu</a>
                <a href="/tags/triton.html">triton</a>
            </div>
        </article>
    </main>
    <footer role="contentinfo">
        <p>2025, authored by Elijah Melton.</p>
    </footer>
</body>
</html>
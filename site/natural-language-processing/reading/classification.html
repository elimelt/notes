
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Classification with Multinomial Naive Bayes</title>
    <meta name="description" content="Overview of classification with Multinomial Naive Bayes, including the Naive Bayes assumption, training, and evaluation.">
    <link rel="canonical" href="https://notes.elimelt.com/natural-language-processing/reading/classification.html">
    <link rel="stylesheet" href="/css/styles.css">
    <script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "Classification with Multinomial Naive Bayes", "dateModified": "2025-02-12T15:23:20.176602", "description": "Overview of classification with Multinomial Naive Bayes, including the Naive Bayes assumption, training, and evaluation.", "url": "https://notes.elimelt.com/natural-language-processing/reading/classification.html", "articleSection": "Natural Language Processing", "keywords": "classification,naive bayes,multinomial naive bayes,text classification,bag of words,laplace smoothing"}</script>
</head>
<body>
    <header>
        <nav><a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a></nav>
        <div class="breadcrumbs"><a href="/index.html">Home</a> » <a href="/categories/natural%20language%20processing.html">Natural Language Processing</a> » Classification with Multinomial Naive Bayes</div>
    </header>
    <main class="content">
        <h1>Classification with Multinomial Naive Bayes</h1>
        <h1 id="classification"><a class="toclink" href="#classification">Classification</a></h1>
<p>Take an input $x$ and a fixed set of output classes $Y = {y_1, y_2, \ldots, y_M}$ and return a predicted class $y \in Y$.</p>
<p>Text classification sometimes uses $c$ for class instead of $y$ as output, and $d$ for document instead of $x$ as input.</p>
<p>In the supervised situation we have a training set of $N$ documents that have each been hand labeled with a class: ${(d_1, c_1),....,(d_N, c_N)}$. Our goal is to learn a classifier that is capable of mapping from a new document $d$ to its correct class $c \in C$, where $C$ is some set of useful document classes.</p>
<h2 id="multinomial-naive-bayes"><a class="toclink" href="#multinomial-naive-bayes">Multinomial Naive Bayes</a></h2>
<p>Represent text as <strong>bag of words</strong>. For a document $d$ out of all all classes $c \in C$, outputs $\hat{c}$ that maximizes $P(c|d)$.</p>
<p>$$
\begin{aligned}
\hat{c} &amp;= \arg \max_{c \in C} P(c|d) \
  &amp;= \arg \max_{c \in C} \frac{P(d|c)P(c)}{P(d)}
\end{aligned}
$$</p>
<p>Where $P(d | c)$ is the likelihood, and $P(c)$ is the prior. Or with features $f_1, f_2, \ldots, f_n$:</p>
<p>$$
\hat{c} = \arg \max_{c \in C} P(f_1, f_2, \ldots, f_n | c)P(c)
$$</p>
<h3 id="naive-bayes-assumption"><a class="toclink" href="#naive-bayes-assumption">Naive Bayes Assumption</a></h3>
<p>$$
P(f_1, f_2, \ldots, f_n | c) = \prod_{i=1}^n P(f_i | c)
$$</p>
<p>$$
\begin{aligned}
C_{\text{NB}} &amp;= \arg \max_{c \in C} P(c) \prod_{i=1}^n P(w_i | c) \
&amp;= \arg \max_{c \in C} \log P(c) + \sum_{i=1}^n \log P(w_i | c)
\end{aligned}
$$</p>
<p>$$
A = B \
\Rightarrow \log A = \log B
$$</p>
<h3 id="training"><a class="toclink" href="#training">Training</a></h3>
<p>Estimate $P(c)$: the prior probability of each class.</p>
<p>$$
\hat{P}(w_i | c) = \frac{count(w_i, c) + 1}{\sum_{w \in V} count(w, c) + |V|}
$$</p>
<p>Or with Laplace smoothing:</p>
<p>$$
\hat{P}(w_i | c) = \frac{count(w_i, c) + 1}{\sum_{w \in V} count(w, c) + |V|}
$$</p>
<pre><code>function TRAIN_NAIVE_BAYES(D, C) returns V, log_P(c), log_P(w|c)
  for each class c ∈ C do
    Ndoc = number of documents in D
    Nc = number of documents from D in class c
    logprior[c] = log(Nc / Ndoc)
    V = vocab of D
    bigdoc[c] = concat(d ∈ D where d.class = c)
    for each word w ∈ V do
      count[w, c] = number of times w appears in bigdoc[c]
      loglikelihood[w, c] = log [(count[w, c] + 1) / (sum_{w' ∈ V} count[w', c] + |V|)]

  return logprior, loglikelihood, V

function TEST_NAIVE_BAYES(testdoc, logprior, loglikelihood, C, V) returns best_c
  for each class c ∈ C do
    sum[c] = logprior[c]
    for each position i in testdoc do
      word = testdoc[i]
      if word ∈ V then
        sum[c] = sum[c] + loglikelihood[word, c]

  return argmax_c sum[c]
</code></pre>
<h3 id="evaluation"><a class="toclink" href="#evaluation">Evaluation</a></h3>
<p>Use a confusion matrix, precision, recall, and F1 score.</p>
    </main>
    <footer>
        <p>&copy; 2025 Notes Site</p>
    </footer>
</body>
</html>
    
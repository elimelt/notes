
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Recommender Systems</title>
    <meta name="description" content="A brief overview of recommender systems, including their challenges, approaches, and applications.">
    <link rel="canonical" href="https://notes.elimelt.com/recc-sys/reccomender-systems.html">
    <link rel="stylesheet" href="/css/styles.css">
    <script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "headline": "Recommender Systems", "dateModified": "2025-05-10T15:55:06.961479", "description": "A brief overview of recommender systems, including their challenges, approaches, and applications.", "url": "https://notes.elimelt.com/recc-sys/reccomender-systems.html", "articleSection": "Machine Learning Systems", "keywords": "recommender systems,collaborative filtering,matrix factorization,matrix completion,personalization,cold-start problem"}</script>
</head>
<body>
    <header>
        <nav><a href="https://github.com/elimelt/notes" style="font-size:24px; color: white;" class="fa">&#xf09b;</a> | <a href="/index.html">Home</a> | <a href="/categories/index.html">Categories</a> | <a href="/tags/index.html">Tags</a></nav>
        <div class="breadcrumbs"><a href="/index.html">Home</a> » <a href="/categories/machine%20learning%20systems.html">Machine Learning Systems</a> » Recommender Systems</div>
    </header>
    <main class="content">
        <h1>Recommender Systems</h1>
        <h2 id="recommender-systems"><a class="toclink" href="#recommender-systems">Recommender Systems</a></h2>
<p><strong>Personalization and Data Sparsity</strong>
- Personalization leverages user data (preferences, activities) to recommend items users might like.
- Challenge: User-item interaction data is sparse-most users rate only a few items.
- Collaborative filtering: Users are likely to enjoy items liked by similar users.</p>
<p><strong>Types of Feedback</strong>
- Explicit feedback: Ratings, purchase history, rankings.
- Implicit feedback: Browsing history, time spent, clicks-requires preprocessing.</p>
<p><strong>Major Challenges</strong>
- Sparsity of data.
- Cold-start problem: Hard to recommend for new users/items with no history.
- Changing interests: User preferences and item popularity shift over time.
- Scalability: Need algorithms that efficiently handle millions of users/items.</p>
<hr />
<h2 id="approaches-to-recommendation"><a class="toclink" href="#approaches-to-recommendation">Approaches to Recommendation</a></h2>
<p><strong>Popularity-Based</strong>
- Recommend most popular items (no personalization).</p>
<p><strong>Classifier-Based</strong>
- Treat as a classification problem:<br />
  Input: $ x =$ (user features, item features), Output: $ y = +1$ (like) or $ -1$ (dislike).
- Pros: Personalized, can include extra features.
- Cons: Feature engineering is hard, often underperforms collaborative filtering.</p>
<p><strong>Co-Occurrence-Based</strong>
- Use normalized co-occurrence matrix $ C$:
  $$
  C_{ij} = \frac{\text{users who bought both } i \text{ and } j}{\text{users who bought } i \text{ or } j}
  $$
- For a user who bought items $ A$ and $ B$, score for item $ X$:
  $$
  \text{Score}(X) = \frac{C_{XA} + C_{XB}}{2}
  $$</p>
<hr />
<h2 id="matrix-factorization-and-completion"><a class="toclink" href="#matrix-factorization-and-completion">Matrix Factorization and Completion</a></h2>
<p><strong>Matrix Factorization Model</strong>
- Represent user $ u$ and item $ v$ with feature vectors $ L_u$ and $ R_v$.
- Predicted rating:
  $$
  \text{Rating}(u, v) = L_u^T R_v
  $$
- The ratings matrix $ M$ is approximated by $ M \approx L R^T$.</p>
<p><strong>Matrix Completion Problem</strong>
- Given observed ratings, estimate missing entries by finding $ L$ and $ R$ that minimize:
  $$
  \min_{L, R} \sum_{(u,v): r_{uv} \text{ observed}} (L_u^T R_v - r_{uv})^2
  $$</p>
<p><strong>Degrees of Freedom</strong>
- For $ m$ movies, $ n$ users, and $ k$ topics:
  $$
  \text{Degrees of freedom} = k(m + n)
  $$</p>
<p><strong>Coordinate Descent Algorithm</strong>
- Alternately fix $ R$ and optimize $ L$, then fix $ L$ and optimize $ R$.
- Each step reduces to multiple independent linear regression problems.</p>
<p><strong>Regularization to Prevent Overfitting</strong>
- Add $ \ell_2$ regularization:
  $$
  \min_{L, R} \sum_{(u,v): r_{uv} \text{ observed}} (L_u^T R_v - r_{uv})^2 + \lambda (|L_u|^2 + |R_v|^2)
  $$</p>
<hr />
<h2 id="extensions-cold-start-solutions"><a class="toclink" href="#extensions-cold-start-solutions">Extensions &amp; Cold-Start Solutions</a></h2>
<p><strong>Feature-Based Linear Models</strong>
- Represent items by feature vector $ \phi(v)$, learn global weights $ w$:
  $$
  r_{uv} \approx w \cdot \phi(v)
  $$
- Minimize:
  $$
  \min_w \sum_{(u,v): r_{uv} \text{ observed}} (w \cdot \phi(v) - r_{uv})^2 + \lambda |w|^2
  $$</p>
<p><strong>Personalization via User-Specific Deviations</strong>
- Add user-specific weights $ w_u$:
  $$
  r_{uv} \approx (w + w_u) \cdot \phi(v)
  $$
- For new users, $ w_u = 0$ (use global weights); as more data accumulates, $ w_u$ adapts.</p>
<p><strong>Featurized Matrix Factorization (Unified Model)</strong>
- Combine matrix factorization and feature-based approaches:
  $$
  r_{uv} \approx L_u \cdot R_v + (w + w_u) \cdot \phi(u, v)
  $$
- Can be optimized with coordinate descent or gradient descent.</p>
<hr />
<h2 id="applications"><a class="toclink" href="#applications">Applications</a></h2>
<ul>
<li><strong>Localization</strong>: Matrix completion can infer missing entries in distance matrices, exploiting low-rank structure due to spatial constraints.</li>
<li><strong>Text Data</strong>: Matrix factorization can uncover latent topics in document-word matrices, similar to topic modeling.</li>
</ul>
<hr />
<h2 id="summary-table-approaches-comparison"><a class="toclink" href="#summary-table-approaches-comparison">Summary Table: Approaches Comparison</a></h2>
<table>
<thead>
<tr>
<th>Approach</th>
<th>Personalization</th>
<th>Handles Cold-Start</th>
<th>Uses Features</th>
<th>Handles Sparsity</th>
<th>Scalability</th>
</tr>
</thead>
<tbody>
<tr>
<td>Popularity</td>
<td>No</td>
<td>Yes</td>
<td>No</td>
<td>Yes</td>
<td>High</td>
</tr>
<tr>
<td>Classifier</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Limited</td>
<td>Moderate</td>
</tr>
<tr>
<td>Co-Occurrence</td>
<td>Limited</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>High</td>
</tr>
<tr>
<td>Matrix Factorization</td>
<td>Yes</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>High</td>
</tr>
<tr>
<td>Feature-Based Linear</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>High</td>
</tr>
<tr>
<td>Featurized Matrix Fact.</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Moderate</td>
</tr>
</tbody>
</table>
<hr />
<p><strong>Best Practices</strong>
- Use collaborative filtering (matrix factorization) for personalization when sufficient data exists.
- Use feature-based models to address cold-start and incorporate context.
- Combine both for robust, scalable, and adaptive recommender systems.</p>
    </main>
    <footer>
        <p>&copy; 2025 Notes Site</p>
    </footer>
</body>
</html>
    
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ba00b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (2.2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from scikit-learn) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from nltk) (1.5.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /Users/emm12/repos/notes/venv/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install scikit-learn\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af4a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import subprocess\n",
    "import markdown\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Any, Tuple, Set, Callable\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import quote\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c92c7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Page:\n",
    "    title: str\n",
    "    path: Path\n",
    "    content: str\n",
    "    modified_date: datetime\n",
    "    category: Optional[str]\n",
    "    tags: List[str]\n",
    "    description: Optional[str]\n",
    "    is_index: bool = False\n",
    "    css_classes: List[str] = None\n",
    "\n",
    "\n",
    "class MetadataIndex:\n",
    "\n",
    "    MARKDOWN_EXTENSIONS = [\n",
    "        \"meta\",\n",
    "        \"toc\",\n",
    "        \"fenced_code\",\n",
    "        \"tables\",\n",
    "        \"attr_list\",\n",
    "        \"footnotes\",\n",
    "        \"def_list\",\n",
    "        \"admonition\",\n",
    "        \"mdx_truly_sane_lists\",\n",
    "    ]\n",
    "    SUPPORTED_CONTENT = {\".md\", \".markdown\"}\n",
    "    IGNORED_DIRECTORIES = {\n",
    "        \".git\",\n",
    "        \"__pycache__\",\n",
    "        \"node_modules\",\n",
    "        \".github\",\n",
    "        \"nlp.venv\",\n",
    "        \"site\",\n",
    "        \"venv\",\n",
    "        \".venv\",\n",
    "    }\n",
    "    DEFAULT_CSS_CLASSES = [\"markdown-content\", \"content\"]\n",
    "    DEFAULT_PERMISSIONS = {\"directory\": 0o755, \"file\": 0o644}\n",
    "\n",
    "    def __init__(self, input_dir: str):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.pages: Dict[Path, Page] = {}\n",
    "        self.categories: Dict[str, List[Page]] = defaultdict(list)\n",
    "        self.tags: Dict[str, List[Page]] = defaultdict(list)\n",
    "        self.markdown_converter = markdown.Markdown(\n",
    "            extensions=self.MARKDOWN_EXTENSIONS,\n",
    "            output_format=\"html5\",\n",
    "            tab_length=4,\n",
    "        )\n",
    "\n",
    "    def _walk_directory(self, directory: Path) -> List[Path]:\n",
    "        return [\n",
    "            item\n",
    "            for item in directory.rglob(\"*\")\n",
    "            if not any(ignored in item.parts for ignored in self.IGNORED_DIRECTORIES)\n",
    "        ]\n",
    "\n",
    "    def _extract_metadata(self, file_path: Path, content: str) -> dict:\n",
    "\n",
    "        md = markdown.Markdown(extensions=self.MARKDOWN_EXTENSIONS)\n",
    "        md.convert(content)\n",
    "\n",
    "        default_title = file_path.stem.replace(\"-\", \" \").title()\n",
    "\n",
    "        if not hasattr(md, \"Meta\"):\n",
    "            return {\n",
    "                \"title\": default_title,\n",
    "                \"category\": None,\n",
    "                \"tags\": [],\n",
    "                \"description\": None,\n",
    "            }\n",
    "\n",
    "        metadata = {\n",
    "            \"title\": md.Meta.get(\"title\", [default_title])[0],\n",
    "            \"category\": md.Meta.get(\"category\", [None])[0],\n",
    "            \"tags\": [],\n",
    "            \"description\": md.Meta.get(\"description\", [None])[0],\n",
    "        }\n",
    "\n",
    "        if \"tags\" in md.Meta and md.Meta[\"tags\"][0]:\n",
    "            metadata[\"tags\"] = [\n",
    "                tag.strip() for tag in md.Meta[\"tags\"][0].split(\",\") if tag.strip()\n",
    "            ]\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def _process_markdown(self, file_path: Path) -> None:\n",
    "\n",
    "        try:\n",
    "            content = file_path.read_text(encoding=\"utf-8\")\n",
    "            metadata = self._extract_metadata(file_path, content)\n",
    "\n",
    "            relative_path = file_path.relative_to(self.input_dir)\n",
    "            is_index = file_path.stem.lower() == \"index\"\n",
    "            tags = [tag.strip().lower() for tag in metadata[\"tags\"] if tag.strip()]\n",
    "\n",
    "            page = Page(\n",
    "                title=metadata[\"title\"],\n",
    "                path=relative_path,\n",
    "                content=content,\n",
    "                modified_date=datetime.fromtimestamp(file_path.stat().st_mtime),\n",
    "                category=metadata[\"category\"],\n",
    "                tags=tags,\n",
    "                description=metadata[\"description\"],\n",
    "                is_index=is_index,\n",
    "                css_classes=self.DEFAULT_CSS_CLASSES,\n",
    "            )\n",
    "\n",
    "            self.pages[file_path] = page\n",
    "            self.categories[page.category].append(page)\n",
    "            self.tags[page.title] = page\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process {file_path}: {str(e)}\")\n",
    "\n",
    "    def _index_all_markdown_files(self) -> None:\n",
    "        \"\"\"\n",
    "        Walk through the input directory and process all markdown files\n",
    "        \"\"\"\n",
    "        print(f\"Indexing markdown files in {self.input_dir.resolve()}...\")\n",
    "        for file_path in self._walk_directory(self.input_dir):\n",
    "\n",
    "            if file_path.suffix in self.SUPPORTED_CONTENT:\n",
    "                self._process_markdown(file_path)\n",
    "\n",
    "    def _get_all_tags(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Get all unique tags from the indexed pages\n",
    "        \"\"\"\n",
    "        all_tags = set()\n",
    "        for page in self.pages.values():\n",
    "            all_tags.update(page.tags)\n",
    "        return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ac1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/emm12/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/emm12/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.replace(\"title\", \"\").replace(\"category\", \"\").replace(\"tags\", \"\").replace(\"description\", \"\")\n",
    "    tokens = text.split()\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01b5ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing markdown files in /Users/emm12/repos/notes...\n",
      "Indexed 171 markdown files.\n"
     ]
    }
   ],
   "source": [
    "page_idx = MetadataIndex(\"..\")\n",
    "page_idx._index_all_markdown_files()\n",
    "print(f\"Indexed {len(page_idx.pages)} markdown files.\")\n",
    "documents = [\n",
    "    page.content for page in page_idx.pages.values() if page.content\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d547d303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 interesting words appearing in multiple documents\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processed_docs = [\n",
    "    preprocess_text(doc) for doc in documents\n",
    "]  \n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, min_df=2, max_df=0.02)\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "doc_word_counts = (tfidf_matrix > 0).sum(axis=0)\n",
    "multi_doc_words = [\n",
    "    (feature_names[i], doc_word_counts[0, i])\n",
    "    for i in range(len(feature_names))\n",
    "    if doc_word_counts[0, i] > 1\n",
    "]  \n",
    "\n",
    "multi_doc_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_links = {}\n",
    "for word, count in multi_doc_words[:100]:  \n",
    "\n",
    "    docs_with_word = [i for i, doc in enumerate(processed_docs) if word in doc.split()]\n",
    "    word_links[word] = {\"count\": int(count), \"documents\": docs_with_word}\n",
    "\n",
    "print(f\"Found {len(word_links)} interesting words appearing in multiple documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281216aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9044190",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
